{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "  Cloning https://github.com/awslabs/fast-differential-privacy.git to /tmp/pip-req-build-8k8krvee\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy.git /tmp/pip-req-build-8k8krvee\n",
      "  Resolved https://github.com/awslabs/fast-differential-privacy.git to commit af783b348e82516f7565802cf1144a8be95c69a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 35, in __init__\n",
      "    parsed = _parse_requirement(requirement_string)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 64, in parse_requirement\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 82, in _parse_requirement\n",
      "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 126, in _parse_requirement_details\n",
      "    marker = _parse_requirement_marker(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 147, in _parse_requirement_marker\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/bin/pip\", line 15, in <module>\n",
      "    snapshot_util.add_python_dependency(sys.argv[2:])\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2579, in add_python_dependency\n",
      "    new_packages = _parse_install_command(install_args)\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2527, in _parse_install_command\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2527, in <listcomp>\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 37, in __init__\n",
      "    raise InvalidRequirement(str(e)) from e\n",
      "packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/awslabs/fast-differential-privacy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ray/anaconda3/lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in /home/ray/anaconda3/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2023.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ray/anaconda3/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_ktp4s9kggijipdihj3cy2lc46q-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ray/anaconda3/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_ktp4s9kggijipdihj3cy2lc46q-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastDP import PrivacyEngine\n",
    "import transformers, torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp_temp0.6_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1_topk50</th>\n",
       "      <th>synthetic_sentence_eps8_temp1_topk100</th>\n",
       "      <th>synthetic_sentence_eps3_temp1_topk100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>(primarily obtained from boyfriend): This 31 y...</td>\n",
       "      <td>43</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 55-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 38-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 42-year-ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>The patient is a 50-year-old white male with h...</td>\n",
       "      <td>72</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a yo...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>43</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient was brought to th...</td>\n",
       "      <td>[BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...</td>\n",
       "      <td>[BOS]Urology[SEP]This is a 38-year-old with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>DESCRIPTION:, The patient was placed in the su...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>44</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 64-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient with a history of...</td>\n",
       "      <td>[BOS]Urology[SEP]Ophytology[SEPURE] The patien...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is referred to a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>63</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 1-year-old f...</td>\n",
       "      <td>[BOS]Urology[SEP]Neurology[SEPUE]Neuroptic[SEP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          medical_specialty  \\\n",
       "0                 Neurology   \n",
       "1  Cardiovascular/Pulmonary   \n",
       "2                   Urology   \n",
       "3                   Urology   \n",
       "4                   Urology   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1  PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:,  The patient was placed in the s...   \n",
       "4  PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "\n",
       "                                age_related_sentence  \\\n",
       "0  HX , (primarily obtained from boyfriend): This...   \n",
       "1  INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:, The patient was placed in the su...   \n",
       "4  INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "\n",
       "                                      extracted_text  word_count  \\\n",
       "0  (primarily obtained from boyfriend): This 31 y...          43   \n",
       "1  The patient is a 50-year-old white male with h...          72   \n",
       "2  Elective male sterilization via bilateral vase...          43   \n",
       "3  The patient was placed in the supine position ...          44   \n",
       "4  A gentleman who is here today requesting volun...          63   \n",
       "\n",
       "                           med_masked_transcriptions  \\\n",
       "0  (primarily obtained from boyfriend): This[AGE]...   \n",
       "1  The patient is a[AGE] white[SEX] with history ...   \n",
       "2  Elective male sterilization via bilateral vase...   \n",
       "3  The patient was placed in the supine position ...   \n",
       "4  A gentleman who is here today requesting volun...   \n",
       "\n",
       "                           pii_masked_transcriptions  \\\n",
       "0  (primarily obtained from boyfriend): This[AGE]...   \n",
       "1  The patient is a[AGE] white[SEX] with history ...   \n",
       "2  Elective male sterilization via bilateral vase...   \n",
       "3  The patient was placed in the supine position ...   \n",
       "4  A gentleman who is here today requesting volun...   \n",
       "\n",
       "              synthetic_sentence_nodp_temp0.6_topk50  \\\n",
       "0  [BOS]Neurology[SEP]The patient is a 55-year-ol...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]The patient was brought to th...   \n",
       "3  [BOS]Urology[SEP]The patient is a 64-year-old ...   \n",
       "4  [BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...   \n",
       "\n",
       "               synthetic_sentence_eps16_temp1_topk50  \\\n",
       "0  [BOS]Neurology[SEP]The patient is a 38-year-ol...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...   \n",
       "3  [BOS]Urology[SEP]The patient with a history of...   \n",
       "4  [BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...   \n",
       "\n",
       "               synthetic_sentence_eps8_temp1_topk100  \\\n",
       "0  [BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]This is a yo...   \n",
       "2  [BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...   \n",
       "3  [BOS]Urology[SEP]Ophytology[SEPURE] The patien...   \n",
       "4  [BOS]Urology[SEP]The patient is a 1-year-old f...   \n",
       "\n",
       "               synthetic_sentence_eps3_temp1_topk100  \n",
       "0  [BOS]Neurology[SEP]The patient is a 42-year-ol...  \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2  [BOS]Urology[SEP]This is a 38-year-old with a ...  \n",
       "3  [BOS]Urology[SEP]The patient is referred to a ...  \n",
       "4  [BOS]Urology[SEP]Neurology[SEPUE]Neuroptic[SEP...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/home/ray/default/synthetic_sentence_eps3_temp1_topk50.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_before_sep(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes everything before '[SEP]' in the specified column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - column_name: The name of the column to process.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with the processed column.\n",
    "    \"\"\"\n",
    "    # Define the regular expression pattern to match everything before '[SEP]'\n",
    "    pattern = re.compile(r'.*?\\[SEP\\]')\n",
    "\n",
    "    # Apply the regular expression to the specified column\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything before '[SEP]' in the 'synthetic_sentence2' column\n",
    "df = remove_before_sep(df, 'synthetic_sentence_dp_eps16')\n",
    "df = remove_before_sep(df, 'synthetic_sentence_dp_eps8_v2')\n",
    "df = remove_before_sep(df, 'synthetic_sentence_dp_eps3')\n",
    "df = remove_before_sep(df, 'synthetic_sentence_nodp_temp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AWS FastDP Privacy Engine Runs with ChatGPT 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This gives a cuda out of memore error on the heavy GPUs \n",
    "* It runs succesfully on 32 vCPU, 128 GiB memory, takes around 5-10 min to run \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6444dd499f4f7e839ef9ae5e6ca989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989c34268c69438e98bd3230dbfa6269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43daab59bedb46ffb56fa273c5e92dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from transformers import GPT2LMHeadModel\n",
    "from fastDP import PrivacyEngine\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and move it to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=50000,\n",
    "    epochs=3,\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Example training pipeline\n",
    "# Assuming `batch` and `labels` are your input tensors and target labels respectively\n",
    "# For this example, we'll create dummy data\n",
    "batch = torch.randint(0, model.config.vocab_size, (32, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "labels = torch.randint(0, model.config.vocab_size, (32, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "\n",
    "# Standard training loop\n",
    "outputs = model(batch)\n",
    "loss = F.cross_entropy(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Training step completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilgpt 2 trained on medical dataset without FastDP Privacy Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e9b1586afe44ba9e1e4c4c34e1699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6487f410454ce98fd6ce72806bfef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6f9605442b42e6aa92edfded9ff6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f559d81ec5b4973891d25f9ae41939f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/rss@vtti.com/Kyra_Test (1) (1).ipynb Cell 26\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, labels\u001b[39m=\u001b[39mtarget_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''''import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"Specialty: {self.labels[idx]} \"\n",
    "        text = specialty_label + self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Increased epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95  # Top-p sampling (nucleus sampling)\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences\n",
    "synthetic_sentences = []\n",
    "for label in [\"Orthopedic\", \"Urology\", \"Cardiovascular / Pulmonary\", \"Neurology\", \"Gastroenterology\",\n",
    "              \"Obstetrics / Gynecology\", \"Hematology - Oncology\", \"ENT - Otolaryngology\", \"Radiology\",\n",
    "              \"Pediatrics - Neonatal\", \"Ophthalmology\"]:\n",
    "    sentence = generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50)\n",
    "    synthetic_sentences.append((sentence, label))\n",
    "\n",
    "print(\"Synthetic sentences generated:\", synthetic_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "'''# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95  # Top-p sampling (nucleus sampling)\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt=extracted_text, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is a 73-year-old pleasant Caucasian male who is known to me from his previous hospitalization. He has also been seen by me in the clinic in the last few weeks.\n",
      "Specialty: Cardiovascular / Pulmonary The patient is a 73-year-old pleasant Caucasian male who is known to me from his previous hospitalization. He has also been seen by me in the clinic in the last few weeks. The diagnosis for these symptoms is very complicated, and he was a very well-developedly, white male with a light complexion and a short weight. At the time of his appointment, he underwent an extensive evaluation for heart failure. This is the diagnosis\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['extracted_text'])\n",
    "print(df.iloc[0]['synthetic_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "Specialty: Cardiovascular / Pulmonary This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. I have had a very small heart rate, pulmonary hypertension or pulmonary bronchoscopy. My chest is low at this time and a slight loss in volume of air mass and breath rate. When breathing is normal, my upper body is at the moment\n",
      "history present illness 91yearold male previous history working coalmine significant exposure silica resultant pneumoconiosis fibrosis lung patient also positive history smoking past present time admitted continued management respiratory depression medical complication patient treated multiple problem jefferson hospital prior coming including abdominal discomfort due ureteral stone resultant hydronephrosis hydroureter addition also developed cardiac complication including atrial fibrillation patient evaluated cardiologist well pulmonary service urology cystoscopy performed left ureteral stone removed well insertion left ureteral stent 07 23 2008 subsequently underwent cardiac arrest resuscitated time intubated placed mechanical ventilatory support subsequent weaning unsuccessful tracheostomy placed current medication 1 albuterol 2 pacerone 3 theophylline 4 lovenox 5 atrovent 6 insulin 7 lantus 8 zestril 9 magnesium oxide 10 lopressor 11 zegerid 12 tylenol needed allergy penicillin past medical history 1 history coal miner disease 2 history copd 3 history atrial fibrillation 4 history coronary artery disease 5 history coronary artery stent placement 6 history gastric obstruction 7 history prostate cancer 8 history chronic diarrhea 9 history pernicious anemia 10 history radiation proctitis 11 history anxiety 12 history ureteral stone 13 history hydronephrosis social history patient previously smoker could obtained tracheostomy presently family history noncontributory present condition review previous chart system review patient currently agitated rapidly moving upper extremity history regarding system could elicited patient physical exam general patient currently agitated level distress rapid respiratory rate responsive verbal command looking eye vital sign per monitor stable extremity inspection upper extremity reveals extreme xerosis skin multiple area ecchymosis skin tear level stage ii especially dorsum hand forearm area also edema forearm extending mid upper arm area palpation upper extremity reveals fibrosis prominent right forearm area maximum edema elbow area ulnar aspect also scabbing possibly earlier skin tear upper side forearm area impression 1 ulceration bilateral upper extremity 2 cellulitis upper extremity 3 lymphedema upper extremity 4 noninfectious disorder lymphatic channel 5 ventilatorydependent respiratory failure\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['transcription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try from 18062024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac386ea6086548d08f755e02fdfebcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65452fc1643e40a1a8dd8c220f4c351f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299cd8fdbd1f4d74a399862f645a6942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cf2ed69a9a48a58d1a04ffd0d6cdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_synthetic_sentence() missing 1 required positional argument: 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y225sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y225sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m     specialty \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mmedical_specialty\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y225sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     synthetic_sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, max_length\u001b[39m=\u001b[39;49m\u001b[39m125\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y225sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y225sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msynthetic_sentence_nodp_temp0.6_topk50\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m synthetic_sentences\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_synthetic_sentence() missing 1 required positional argument: 'prompt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=125)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp_temp0.6_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('2016_synthetic_sentence_nodp_temp0.6_topk50.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_nodp_2016\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_nodp_2016\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=125)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp_temp0.6_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_bossep_2016.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nodpcheck.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18062024 16 Eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=16,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_dpeps16_2016\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_dpeps16_2016\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_eps16_temp1_topk50.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp_temp0.6_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1_topk50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>(primarily obtained from boyfriend): This 31 y...</td>\n",
       "      <td>43</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 55-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 38-year-ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>The patient is a 50-year-old white male with h...</td>\n",
       "      <td>72</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>43</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient was brought to th...</td>\n",
       "      <td>[BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>DESCRIPTION:, The patient was placed in the su...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>44</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 64-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient with a history of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>63</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>41</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>46</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>62</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a 5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>119</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>45</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a 35...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             medical_specialty  \\\n",
       "0                    Neurology   \n",
       "1     Cardiovascular/Pulmonary   \n",
       "2                      Urology   \n",
       "3                      Urology   \n",
       "4                      Urology   \n",
       "...                        ...   \n",
       "2011  Cardiovascular/Pulmonary   \n",
       "2012  Cardiovascular/Pulmonary   \n",
       "2013  Cardiovascular/Pulmonary   \n",
       "2014  Cardiovascular/Pulmonary   \n",
       "2015  Cardiovascular/Pulmonary   \n",
       "\n",
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2011  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2012  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2015  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:, The patient was placed in the su...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2011  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2012  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                                         extracted_text  word_count  \\\n",
       "0     (primarily obtained from boyfriend): This 31 y...          43   \n",
       "1     The patient is a 50-year-old white male with h...          72   \n",
       "2     Elective male sterilization via bilateral vase...          43   \n",
       "3     The patient was placed in the supine position ...          44   \n",
       "4     A gentleman who is here today requesting volun...          63   \n",
       "...                                                 ...         ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...          41   \n",
       "2012  The patient is a 40-year-old white male who pr...          46   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...          62   \n",
       "2014  ABC is a 60-year-old gentleman who had a marke...         119   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...          45   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     (primarily obtained from boyfriend): This[AGE]...   \n",
       "1     The patient is a[AGE] white[SEX] with history ...   \n",
       "2     Elective male sterilization via bilateral vase...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...   \n",
       "2012  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     (primarily obtained from boyfriend): This[AGE]...   \n",
       "1     The patient is a[AGE] white[SEX] with history ...   \n",
       "2     Elective male sterilization via bilateral vase...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...   \n",
       "2012  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                 synthetic_sentence_nodp_temp0.6_topk50  \\\n",
       "0     [BOS]Neurology[SEP]The patient is a 55-year-ol...   \n",
       "1     [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2     [BOS]Urology[SEP]The patient was brought to th...   \n",
       "3     [BOS]Urology[SEP]The patient is a 64-year-old ...   \n",
       "4     [BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...   \n",
       "...                                                 ...   \n",
       "2011  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2012  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2013  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2014  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2015  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "\n",
       "                  synthetic_sentence_eps16_temp1_topk50  \n",
       "0     [BOS]Neurology[SEP]The patient is a 38-year-ol...  \n",
       "1     [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2     [BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...  \n",
       "3     [BOS]Urology[SEP]The patient with a history of...  \n",
       "4     [BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...  \n",
       "...                                                 ...  \n",
       "2011  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2012  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2013  [BOS]Cardiovascular/Pulmonary[SEP]This is a 5-...  \n",
       "2014  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2015  [BOS]Cardiovascular/Pulmonary[SEP]This is a 35...  \n",
       "\n",
       "[2016 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1806 Eps 8 including privacy accountant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "This file is adapted from the privacy accounting procedure in Opacus', which in turn is adapted from tf-privacy.\n",
    "Below is the original documentation in Opacus.\n",
    "\n",
    "*Based on Google's TF Privacy:* https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/analysis\n",
    "/rdp_accountant.py.\n",
    "*Here, we update this code to Python 3, and optimize dependencies.*\n",
    "\n",
    "Functionality for computing Renyi Differential Privacy (RDP) of an additive\n",
    "Sampled Gaussian Mechanism (SGM).\n",
    "\n",
    "Example:\n",
    "    Suppose that we have run an SGM applied to a function with L2-sensitivity of 1.\n",
    "\n",
    "    Its parameters are given as a list of tuples\n",
    "    ``[(q_1, sigma_1, steps_1), ..., (q_k, sigma_k, steps_k)],``\n",
    "    and we wish to compute epsilon for a given target delta.\n",
    "\n",
    "    The example code would be:\n",
    "\n",
    "    >>> max_order = 32\n",
    "    >>> orders = range(2, max_order + 1)\n",
    "    >>> rdp = np.zeros_like(orders, dtype=float)\n",
    "    >>> for q, sigma, steps in parameters:\n",
    "    >>>     rdp += privacy_analysis.compute_rdp(q, sigma, steps, orders)\n",
    "    >>> epsilon, opt_order = privacy_analysis.get_privacy_spent(orders, rdp, delta)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import List, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "########################\n",
    "# LOG-SPACE ARITHMETIC #\n",
    "########################\n",
    "\n",
    "\n",
    "def _log_add(logx: float, logy: float) -> float:\n",
    "    r\"\"\"Adds two numbers in the log space.\n",
    "\n",
    "    Args:\n",
    "        logx: First term in log space.\n",
    "        logy: Second term in log space.\n",
    "\n",
    "    Returns:\n",
    "        Sum of numbers in log space.\n",
    "    \"\"\"\n",
    "    a, b = min(logx, logy), max(logx, logy)\n",
    "    if a == -np.inf:  # adding 0\n",
    "        return b\n",
    "    # Use exp(a) + exp(b) = (exp(a - b) + 1) * exp(b)\n",
    "    return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)\n",
    "\n",
    "\n",
    "def _log_sub(logx: float, logy: float) -> float:\n",
    "    r\"\"\"Subtracts two numbers in the log space.\n",
    "\n",
    "    Args:\n",
    "        logx: First term in log space. Expected to be greater than the second term.\n",
    "        logy: First term in log space. Expected to be less than the first term.\n",
    "\n",
    "    Returns:\n",
    "        Difference of numbers in log space.\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "            If the result is negative.\n",
    "    \"\"\"\n",
    "    if logx < logy:\n",
    "        raise ValueError(\"The result of subtraction must be non-negative.\")\n",
    "    if logy == -np.inf:  # subtracting 0\n",
    "        return logx\n",
    "    if logx == logy:\n",
    "        return -np.inf  # 0 is represented as -np.inf in the log space.\n",
    "\n",
    "    try:\n",
    "        # Use exp(x) - exp(y) = (exp(x - y) - 1) * exp(y).\n",
    "        return math.log(math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1\n",
    "    except OverflowError:\n",
    "        return logx\n",
    "\n",
    "\n",
    "def _compute_log_a_for_int_alpha(q: float, sigma: float, alpha: int) -> float:\n",
    "    r\"\"\"Computes :math:`log(A_\\alpha)` for integer ``alpha``.\n",
    "\n",
    "    Notes:\n",
    "        Note that\n",
    "        :math:`A_\\alpha` is real valued function of ``alpha`` and ``q``,\n",
    "        and that 0 < ``q`` < 1.\n",
    "\n",
    "        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf for details.\n",
    "\n",
    "    Args:\n",
    "        q: Sampling rate of SGM.\n",
    "        sigma: The standard deviation of the additive Gaussian noise.\n",
    "        alpha: The order at which RDP is computed.\n",
    "\n",
    "    Returns:\n",
    "        :math:`log(A_\\alpha)` as defined in Section 3.3 of\n",
    "        https://arxiv.org/pdf/1908.10530.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize with 0 in the log space.\n",
    "    log_a = -np.inf\n",
    "\n",
    "    for i in range(alpha + 1):\n",
    "        log_coef_i = (\n",
    "            math.log(special.binom(alpha, i))\n",
    "            + i * math.log(q)\n",
    "            + (alpha - i) * math.log(1 - q)\n",
    "        )\n",
    "\n",
    "        s = log_coef_i + (i * i - i) / (2 * (sigma ** 2))\n",
    "        log_a = _log_add(log_a, s)\n",
    "\n",
    "    return float(log_a)\n",
    "\n",
    "\n",
    "def _compute_log_a_for_frac_alpha(q: float, sigma: float, alpha: float) -> float:\n",
    "    r\"\"\"Computes :math:`log(A_\\alpha)` for fractional ``alpha``.\n",
    "\n",
    "    Notes:\n",
    "        Note that\n",
    "        :math:`A_\\alpha` is real valued function of ``alpha`` and ``q``,\n",
    "        and that 0 < ``q`` < 1.\n",
    "\n",
    "        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf for details.\n",
    "\n",
    "    Args:\n",
    "        q: Sampling rate of SGM.\n",
    "        sigma: The standard deviation of the additive Gaussian noise.\n",
    "        alpha: The order at which RDP is computed.\n",
    "\n",
    "    Returns:\n",
    "        :math:`log(A_\\alpha)` as defined in Section 3.3 of\n",
    "        https://arxiv.org/pdf/1908.10530.pdf.\n",
    "    \"\"\"\n",
    "    # The two parts of A_alpha, integrals over (-inf,z0] and [z0, +inf), are\n",
    "    # initialized to 0 in the log space:\n",
    "    log_a0, log_a1 = -np.inf, -np.inf\n",
    "    i = 0\n",
    "\n",
    "    z0 = sigma ** 2 * math.log(1 / q - 1) + 0.5\n",
    "\n",
    "    while True:  # do ... until loop\n",
    "        coef = special.binom(alpha, i)\n",
    "        log_coef = math.log(abs(coef))\n",
    "        j = alpha - i\n",
    "\n",
    "        log_t0 = log_coef + i * math.log(q) + j * math.log(1 - q)\n",
    "        log_t1 = log_coef + j * math.log(q) + i * math.log(1 - q)\n",
    "\n",
    "        log_e0 = math.log(0.5) + _log_erfc((i - z0) / (math.sqrt(2) * sigma))\n",
    "        log_e1 = math.log(0.5) + _log_erfc((z0 - j) / (math.sqrt(2) * sigma))\n",
    "\n",
    "        log_s0 = log_t0 + (i * i - i) / (2 * (sigma ** 2)) + log_e0\n",
    "        log_s1 = log_t1 + (j * j - j) / (2 * (sigma ** 2)) + log_e1\n",
    "\n",
    "        if coef > 0:\n",
    "            log_a0 = _log_add(log_a0, log_s0)\n",
    "            log_a1 = _log_add(log_a1, log_s1)\n",
    "        else:\n",
    "            log_a0 = _log_sub(log_a0, log_s0)\n",
    "            log_a1 = _log_sub(log_a1, log_s1)\n",
    "\n",
    "        i += 1\n",
    "        if max(log_s0, log_s1) < -30:\n",
    "            break\n",
    "\n",
    "    return _log_add(log_a0, log_a1)\n",
    "\n",
    "\n",
    "def _compute_log_a(q: float, sigma: float, alpha: float) -> float:\n",
    "    r\"\"\"Computes :math:`log(A_\\alpha)` for any positive finite ``alpha``.\n",
    "\n",
    "    Notes:\n",
    "        Note that\n",
    "        :math:`A_\\alpha` is real valued function of ``alpha`` and ``q``,\n",
    "        and that 0 < ``q`` < 1.\n",
    "\n",
    "        Refer to Section 3.3 of https://arxiv.org/pdf/1908.10530.pdf\n",
    "        for details.\n",
    "\n",
    "    Args:\n",
    "        q: Sampling rate of SGM.\n",
    "        sigma: The standard deviation of the additive Gaussian noise.\n",
    "        alpha: The order at which RDP is computed.\n",
    "\n",
    "    Returns:\n",
    "        :math:`log(A_\\alpha)` as defined in the paper mentioned above.\n",
    "    \"\"\"\n",
    "    if float(alpha).is_integer():\n",
    "        return _compute_log_a_for_int_alpha(q, sigma, int(alpha))\n",
    "    else:\n",
    "        return _compute_log_a_for_frac_alpha(q, sigma, alpha)\n",
    "\n",
    "\n",
    "def _log_erfc(x: float) -> float:\n",
    "    r\"\"\"Computes :math:`log(erfc(x))` with high accuracy for large ``x``.\n",
    "\n",
    "    Helper function used in computation of :math:`log(A_\\alpha)`\n",
    "    for a fractional alpha.\n",
    "\n",
    "    Args:\n",
    "        x: The input to the function\n",
    "\n",
    "    Returns:\n",
    "        :math=`log(erfc(x))`\n",
    "    \"\"\"\n",
    "    return math.log(2) + special.log_ndtr(-x * 2 ** 0.5)\n",
    "\n",
    "\n",
    "def _compute_rdp(q: float, sigma: float, alpha: float) -> float:\n",
    "    r\"\"\"Computes RDP of the Sampled Gaussian Mechanism at order ``alpha``.\n",
    "\n",
    "    Args:\n",
    "        q: Sampling rate of SGM.\n",
    "        sigma: The standard deviation of the additive Gaussian noise.\n",
    "        alpha: The order at which RDP is computed.\n",
    "\n",
    "    Returns:\n",
    "        RDP at order ``alpha``; can be np.inf.\n",
    "    \"\"\"\n",
    "    if q == 0:\n",
    "        return 0\n",
    "\n",
    "    # no privacy\n",
    "    if sigma == 0:\n",
    "        return np.inf\n",
    "\n",
    "    if q == 1.0:\n",
    "        return alpha / (2 * sigma ** 2)\n",
    "\n",
    "    if np.isinf(alpha):\n",
    "        return np.inf\n",
    "\n",
    "    return _compute_log_a(q, sigma, alpha) / (alpha - 1)\n",
    "\n",
    "\n",
    "def compute_rdp(\n",
    "    q: float, noise_multiplier: float, steps: int, orders: Union[Sequence[float], float]\n",
    ") -> Union[List[float], float]:\n",
    "    r\"\"\"Computes Renyi Differential Privacy (RDP) guarantees of the\n",
    "    Sampled Gaussian Mechanism (SGM) iterated ``steps`` times.\n",
    "\n",
    "    Args:\n",
    "        q: Sampling rate of SGM.\n",
    "        noise_multiplier: The ratio of the standard deviation of the\n",
    "            additive Gaussian noise to the L2-sensitivity of the function\n",
    "            to which it is added. Note that this is same as the standard\n",
    "            deviation of the additive Gaussian noise when the L2-sensitivity\n",
    "            of the function is 1.\n",
    "        steps: The number of iterations of the mechanism.\n",
    "        orders: An array (or a scalar) of RDP orders.\n",
    "\n",
    "    Returns:\n",
    "        The RDP guarantees at all orders; can be ``np.inf``.\n",
    "    \"\"\"\n",
    "    if isinstance(orders, float):\n",
    "        rdp = _compute_rdp(q, noise_multiplier, orders)\n",
    "    else:\n",
    "        rdp = np.array([_compute_rdp(q, noise_multiplier, order) for order in orders])\n",
    "\n",
    "    return rdp * steps\n",
    "\n",
    "\n",
    "# Based on\n",
    "#   https://github.com/tensorflow/privacy/blob/5f07198b66b3617b22609db983926e3ba97cd905/tensorflow_privacy/privacy/analysis/rdp_accountant.py#L237\n",
    "def get_privacy_spent(orders, rdp, delta):\n",
    "    \"\"\"Compute epsilon given a list of RDP values and target delta.\n",
    "    Args:\n",
    "        orders: An array (or a scalar) of orders.\n",
    "        rdp: A list (or a scalar) of RDP guarantees.\n",
    "        delta: The target delta.\n",
    "    Returns:\n",
    "        Pair of (eps, optimal_order).\n",
    "    Raises:\n",
    "        ValueError: If input is malformed.\n",
    "    \"\"\"\n",
    "    orders_vec = np.atleast_1d(orders)\n",
    "    rdp_vec = np.atleast_1d(rdp)\n",
    "\n",
    "    if delta <= 0:\n",
    "        raise ValueError(\"Privacy failure probability bound delta must be >0.\")\n",
    "    if len(orders_vec) != len(rdp_vec):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    # Basic bound (see https://arxiv.org/abs/1702.07476 Proposition 3 in v3):\n",
    "    #   eps = min( rdp_vec - math.log(delta) / (orders_vec - 1) )\n",
    "\n",
    "    # Improved bound from https://arxiv.org/abs/2004.00010 Proposition 12 (in v4).\n",
    "    # Also appears in https://arxiv.org/abs/2001.05990 Equation 20 (in v1).\n",
    "    eps_vec = []\n",
    "    for (a, r) in zip(orders_vec, rdp_vec):\n",
    "        if a < 1:\n",
    "            raise ValueError(\"Renyi divergence order must be >=1.\")\n",
    "        if r < 0:\n",
    "            raise ValueError(\"Renyi divergence must be >=0.\")\n",
    "\n",
    "        if delta ** 2 + math.expm1(-r) >= 0:\n",
    "            # In this case, we can simply bound via KL divergence:\n",
    "            # delta <= sqrt(1-exp(-KL)).\n",
    "            eps = 0  # No need to try further computation if we have eps = 0.\n",
    "        elif a > 1.01:\n",
    "            # This bound is not numerically stable as alpha->1.\n",
    "            # Thus we have a min value of alpha.\n",
    "            # The bound is also not useful for small alpha, so doesn't matter.\n",
    "            eps = r + math.log1p(-1 / a) - math.log(delta * a) / (a - 1)\n",
    "        else:\n",
    "            # In this case we can't do anything. E.g., asking for delta = 0.\n",
    "            eps = np.inf\n",
    "        eps_vec.append(eps)\n",
    "\n",
    "    idx_opt = np.argmin(eps_vec)\n",
    "    return max(0, eps_vec[idx_opt]), orders_vec[idx_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n",
      "Epoch 1 completed. Privacy budget: ε = 3.77, α = 4\n",
      "Epoch 2 completed. Privacy budget: ε = 4.44, α = 4\n",
      "Epoch 3 completed. Privacy budget: ε = 4.98, α = 3\n",
      "Epoch 4 completed. Privacy budget: ε = 5.03, α = 3\n",
      "Epoch 5 completed. Privacy budget: ε = 5.09, α = 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP.privacy_engine import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Now you can use the compute_rdp and get_privacy_spent functions directly\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=8,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    "    accounting_mode='rdp'  # Specify the privacy accounting mode\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Define the orders for RDP accounting and delta\n",
    "orders = range(2, 33)\n",
    "delta = 1e-5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate and print the privacy budget used so far\n",
    "    rdp = compute_rdp(privacy_engine.sample_rate, privacy_engine.noise_multiplier, epoch + 1, orders)\n",
    "    epsilon, best_alpha = get_privacy_spent(orders, rdp, delta)\n",
    "    print(f\"Epoch {epoch + 1} completed. Privacy budget: ε = {epsilon:.2f}, α = {best_alpha}\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_dpeps8_2016\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_dpeps8_2016\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1906 Eps 3 including Privacy Accountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfe79c4f3542099a8ca6a08b6f3ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff1bbc2cc4b4b9cbb53923645a20c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08255a60052f4d629dcd4521f40fd701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5273bfba98c240d5b239e0f793177ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4630cf39b044bb9e1997b2fbd8d748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038e9ddf09e94a4c954400902a294b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf198a71eba4f7b988f4b920401c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Privacy budget: ε = 1.72, α = 7\n",
      "Epoch 2 completed. Privacy budget: ε = 1.83, α = 6\n",
      "Epoch 3 completed. Privacy budget: ε = 1.87, α = 6\n",
      "Epoch 4 completed. Privacy budget: ε = 1.91, α = 6\n",
      "Epoch 5 completed. Privacy budget: ε = 1.94, α = 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP.privacy_engine import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Now you can use the compute_rdp and get_privacy_spent functions directly\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=3,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    "    accounting_mode='rdp'  # Specify the privacy accounting mode\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Define the orders for RDP accounting and delta\n",
    "orders = range(2, 33)\n",
    "delta = 1e-5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate and print the privacy budget used so far\n",
    "    rdp = compute_rdp(privacy_engine.sample_rate, privacy_engine.noise_multiplier, epoch + 1, orders)\n",
    "    epsilon, best_alpha = get_privacy_spent(orders, rdp, delta)\n",
    "    print(f\"Epoch {epoch + 1} completed. Privacy budget: ε = {epsilon:.2f}, α = {best_alpha}\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_dpeps3_2016\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_dpeps3_2016\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1906 Model Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from /home/ray/default/trained_model_with_classifier_distilgpt2_dpeps16_2016\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define the directory where the model and tokenizer are saved\n",
    "save_directory = \"/home/ray/default/trained_model_with_classifier_distilgpt2_dpeps16_2016\"\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# If necessary, move the model to the specified device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded from\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_eps16_temp1_topk100.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from /home/ray/default/trained_model_with_classifier_distilgpt2_nodp_2016\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define the directory where the model and tokenizer are saved\n",
    "save_directory = \"/home/ray/default/trained_model_with_classifier_distilgpt2_nodp_2016\"\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# If necessary, move the model to the specified device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded from\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions', 'synthetic_sentence_nodp_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp1_topk50',\n",
       "       'synthetic_sentence_eps8_temp1_topk100',\n",
       "       'synthetic_sentence_eps3_temp1_topk100'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: codecarbon in /home/ray/anaconda3/lib/python3.9/site-packages (2.4.2)\n",
      "Requirement already satisfied: arrow in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (1.3.0)\n",
      "Requirement already satisfied: click in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (8.1.7)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (1.5.3)\n",
      "Requirement already satisfied: prometheus-client in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (0.19.0)\n",
      "Requirement already satisfied: psutil in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (9.0.0)\n",
      "Requirement already satisfied: pynvml in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (11.5.0)\n",
      "Requirement already satisfied: rapidfuzz in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (3.9.3)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from arrow->codecarbon) (2.8.2)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ray/anaconda3/lib/python3.9/site-packages (from arrow->codecarbon) (2.9.0.20240316)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->codecarbon) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->codecarbon) (1.24.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->codecarbon) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->codecarbon) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->codecarbon) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_ktp4s9kggijipdihj3cy2lc46q-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions', 'synthetic_sentence_nodp_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp1_topk50',\n",
       "       'synthetic_sentence_eps8_temp1_topk100',\n",
       "       'synthetic_sentence_eps3_temp1_topk100'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 11:33:29] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 11:33:29] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 11:33:29] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 11:33:29] No GPU found.\n",
      "[codecarbon INFO @ 11:33:29] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 11:33:29] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 11:33:31] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 11:33:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 11:33:31]   Platform system: Linux-6.5.0-1020-aws-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 11:33:31]   Python version: 3.9.19\n",
      "[codecarbon INFO @ 11:33:31]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 11:33:31]   Available RAM : 124.434 GB\n",
      "[codecarbon INFO @ 11:33:31]   CPU count: 32\n",
      "[codecarbon INFO @ 11:33:31]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 11:33:31]   GPU count: None\n",
      "[codecarbon INFO @ 11:33:31]   GPU model: None\n",
      "[codecarbon INFO @ 11:33:46] Energy consumed for RAM : 0.000194 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:33:46] Energy consumed for all CPUs : 0.000438 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:33:46] 0.000632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:01] Energy consumed for RAM : 0.000389 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:01] Energy consumed for all CPUs : 0.000875 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:01] 0.001264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:16] Energy consumed for RAM : 0.000583 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:16] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:16] 0.001896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:31] Energy consumed for RAM : 0.000778 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:31] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:31] 0.002528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:46] Energy consumed for RAM : 0.000972 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:46] Energy consumed for all CPUs : 0.002187 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:46] 0.003159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:01] Energy consumed for RAM : 0.001166 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:01] Energy consumed for all CPUs : 0.002625 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:01] 0.003791 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:16] Energy consumed for RAM : 0.001361 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:16] Energy consumed for all CPUs : 0.003062 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:16] 0.004423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:31] Energy consumed for RAM : 0.001555 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:31] Energy consumed for all CPUs : 0.003500 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:31] 0.005055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:46] Energy consumed for RAM : 0.001750 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:46] Energy consumed for all CPUs : 0.003937 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:46] 0.005687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:01] Energy consumed for RAM : 0.001944 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:01] Energy consumed for all CPUs : 0.004375 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:01] 0.006319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:16] Energy consumed for RAM : 0.002138 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:16] Energy consumed for all CPUs : 0.004812 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:16] 0.006951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:31] Energy consumed for RAM : 0.002333 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:31] Energy consumed for all CPUs : 0.005250 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:31] 0.007582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:46] Energy consumed for RAM : 0.002527 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:46] Energy consumed for all CPUs : 0.005687 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:46] 0.008214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:01] Energy consumed for RAM : 0.002721 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:01] Energy consumed for all CPUs : 0.006125 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:01] 0.008846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:16] Energy consumed for RAM : 0.002916 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:16] Energy consumed for all CPUs : 0.006562 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:16] 0.009478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:31] Energy consumed for RAM : 0.003110 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:31] Energy consumed for all CPUs : 0.007000 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:31] 0.010110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:46] Energy consumed for RAM : 0.003305 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:46] Energy consumed for all CPUs : 0.007437 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:46] 0.010742 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:01] Energy consumed for RAM : 0.003499 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:01] Energy consumed for all CPUs : 0.007875 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:01] 0.011374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:16] Energy consumed for RAM : 0.003693 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:16] Energy consumed for all CPUs : 0.008312 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:16] 0.012005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:31] Energy consumed for RAM : 0.003888 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:31] Energy consumed for all CPUs : 0.008750 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:31] 0.012637 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:46] Energy consumed for RAM : 0.004082 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:46] Energy consumed for all CPUs : 0.009187 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:46] 0.013269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:01] Energy consumed for RAM : 0.004277 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:01] Energy consumed for all CPUs : 0.009624 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:01] 0.013901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:16] Energy consumed for RAM : 0.004471 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:16] Energy consumed for all CPUs : 0.010062 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:16] 0.014533 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:31] Energy consumed for RAM : 0.004665 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:31] Energy consumed for all CPUs : 0.010499 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:31] 0.015165 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:46] Energy consumed for RAM : 0.004860 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:46] Energy consumed for all CPUs : 0.010937 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:46] 0.015797 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:01] Energy consumed for RAM : 0.005054 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:01] Energy consumed for all CPUs : 0.011374 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:01] 0.016428 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:16] Energy consumed for RAM : 0.005249 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:16] Energy consumed for all CPUs : 0.011812 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:16] 0.017060 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:31] Energy consumed for RAM : 0.005443 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:31] Energy consumed for all CPUs : 0.012249 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:31] 0.017692 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:46] Energy consumed for RAM : 0.005637 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:46] Energy consumed for all CPUs : 0.012687 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:46] 0.018324 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:01] Energy consumed for RAM : 0.005832 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:01] Energy consumed for all CPUs : 0.013124 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:01] 0.018956 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:16] Energy consumed for RAM : 0.006026 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:16] Energy consumed for all CPUs : 0.013562 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:16] 0.019588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:24] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 11:41:24] Energy consumed for RAM : 0.006126 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:24] Energy consumed for all CPUs : 0.013787 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:24] 0.019913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:24] Done!\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSynthetic sentences generated and saved to \u001b[39m\u001b[39m{\u001b[39;00moutput_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     generate_and_save_synthetic_sentences(df, model, tokenizer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/codecarbon/emissions_tracker.py:1040\u001b[0m, in \u001b[0;36mtrack_emissions.<locals>._decorate.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m tracker\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     fn_result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1041\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1043\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGraceful stopping: collecting and writing information.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1044\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPlease wait a few seconds...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m     )\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     specialty \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mmedical_specialty\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     synthetic_sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, max_length\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     synthetic_sentences\u001b[39m.\u001b[39mappend(synthetic_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m specialty_label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[BOS]\u001b[39m\u001b[39m{\u001b[39;00mspecialty_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,  \u001b[39m# Avoid repeating n-grams\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Enable sampling\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,  \u001b[39m# Top-k sampling\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,  \u001b[39m# Top-p sampling (nucleus sampling)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m generated_sentence\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1759\u001b[0m         input_ids,\n\u001b[1;32m   1760\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1761\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1762\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1763\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1764\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1765\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config) \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2398\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2399\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2400\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2401\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2402\u001b[0m )\n\u001b[1;32m   2404\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1302\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1302\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1303\u001b[0m     input_ids,\n\u001b[1;32m   1304\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1305\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1306\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1307\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1308\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1309\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1310\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1311\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1312\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1313\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1314\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1315\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1317\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1319\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1116\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1105\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1106\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         output_attentions,\n\u001b[1;32m   1114\u001b[0m     )\n\u001b[1;32m   1115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m   1117\u001b[0m         hidden_states,\n\u001b[1;32m   1118\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m   1119\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1120\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m   1121\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1122\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1123\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1124\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1125\u001b[0m     )\n\u001b[1;32m   1127\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 614\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    615\u001b[0m     hidden_states,\n\u001b[1;32m    616\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    617\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    618\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    619\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    620\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:325\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    323\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    327\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    328\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/pytorch_utils.py:104\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    103\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 104\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "@track_emissions(project_name=\"synthetic_sentence_generation\")\n",
    "def generate_and_save_synthetic_sentences(df, model, tokenizer, output_file='synthetic_sentence_nodp_temp0.8_topk50.csv'):\n",
    "    # Generate synthetic sentences for each row in the dataframe\n",
    "    synthetic_sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        specialty = row['medical_specialty']\n",
    "        synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "        synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "    # Append the synthetic sentences to the dataframe\n",
    "    df['synthetic_sentence_nodp_temp0.8_topk50'] = synthetic_sentences\n",
    "\n",
    "    # Save the dataframe with the synthetic sentences\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Synthetic sentences generated and saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_and_save_synthetic_sentences(df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions', 'synthetic_sentence_nodp_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp1_topk50',\n",
       "       'synthetic_sentence_eps8_temp1_topk100',\n",
       "       'synthetic_sentence_eps3_temp1_topk100',\n",
       "       'synthetic_sentence_eps16_temp1_topk100',\n",
       "       'synthetic_sentence_nodp_temp0.8_topk50',\n",
       "       'synthetic_sentence_nodp_temp1.0_topk50',\n",
       "       'synthetic_sentence_nodp_temp1.2_topk50'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp_temp0.6_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_nodp_temp0.6_topk100.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions', 'synthetic_sentence_nodp_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp1_topk50',\n",
       "       'synthetic_sentence_eps8_temp1_topk100',\n",
       "       'synthetic_sentence_eps3_temp1_topk100',\n",
       "       'synthetic_sentence_nodp_temp0.8_topk50',\n",
       "       'synthetic_sentence_nodp_temp1.0_topk50',\n",
       "       'synthetic_sentence_nodp_temp1.2_topk50',\n",
       "       'synthetic_sentence_nodp_temp1.2_topk100',\n",
       "       'synthetic_sentence_nodp_temp1.0_topk100',\n",
       "       'synthetic_sentence_nodp_temp0.8_topk100',\n",
       "       'synthetic_sentence_nodp_temp0.6_topk100'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp_temp0.6_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1_topk50</th>\n",
       "      <th>synthetic_sentence_eps8_temp1_topk100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>(primarily obtained from boyfriend): This 31 y...</td>\n",
       "      <td>43</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 55-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 38-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>The patient is a 50-year-old white male with h...</td>\n",
       "      <td>72</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>43</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient was brought to th...</td>\n",
       "      <td>[BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>DESCRIPTION:, The patient was placed in the su...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>44</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 64-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient with a history of...</td>\n",
       "      <td>[BOS]Urology[SEP]Ophytology[SEPURE] The patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>63</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 1-year-old f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>41</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]Pulmonary/Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>46</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]After the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>62</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a 5-...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>119</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]Overmed. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>45</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: ,The patient is an...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a 35...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]FF.UHOSSEP]T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             medical_specialty  \\\n",
       "0                    Neurology   \n",
       "1     Cardiovascular/Pulmonary   \n",
       "2                      Urology   \n",
       "3                      Urology   \n",
       "4                      Urology   \n",
       "...                        ...   \n",
       "2011  Cardiovascular/Pulmonary   \n",
       "2012  Cardiovascular/Pulmonary   \n",
       "2013  Cardiovascular/Pulmonary   \n",
       "2014  Cardiovascular/Pulmonary   \n",
       "2015  Cardiovascular/Pulmonary   \n",
       "\n",
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2011  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2012  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2015  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:, The patient was placed in the su...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2011  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2012  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                                         extracted_text  word_count  \\\n",
       "0     (primarily obtained from boyfriend): This 31 y...          43   \n",
       "1     The patient is a 50-year-old white male with h...          72   \n",
       "2     Elective male sterilization via bilateral vase...          43   \n",
       "3     The patient was placed in the supine position ...          44   \n",
       "4     A gentleman who is here today requesting volun...          63   \n",
       "...                                                 ...         ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...          41   \n",
       "2012  The patient is a 40-year-old white male who pr...          46   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...          62   \n",
       "2014  ABC is a 60-year-old gentleman who had a marke...         119   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...          45   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     (primarily obtained from boyfriend): This[AGE]...   \n",
       "1     The patient is a[AGE] white[SEX] with history ...   \n",
       "2     Elective male sterilization via bilateral vase...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...   \n",
       "2012  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     (primarily obtained from boyfriend): This[AGE]...   \n",
       "1     The patient is a[AGE] white[SEX] with history ...   \n",
       "2     Elective male sterilization via bilateral vase...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2011  Adenosine with nuclear scan as the patient una...   \n",
       "2012  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2013  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2014  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2015  HISTORY OF PRESENT ILLNESS: ,The patient is an...   \n",
       "\n",
       "                 synthetic_sentence_nodp_temp0.6_topk50  \\\n",
       "0     [BOS]Neurology[SEP]The patient is a 55-year-ol...   \n",
       "1     [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2     [BOS]Urology[SEP]The patient was brought to th...   \n",
       "3     [BOS]Urology[SEP]The patient is a 64-year-old ...   \n",
       "4     [BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...   \n",
       "...                                                 ...   \n",
       "2011  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2012  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2013  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2014  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2015  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "\n",
       "                  synthetic_sentence_eps16_temp1_topk50  \\\n",
       "0     [BOS]Neurology[SEP]The patient is a 38-year-ol...   \n",
       "1     [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2     [BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...   \n",
       "3     [BOS]Urology[SEP]The patient with a history of...   \n",
       "4     [BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...   \n",
       "...                                                 ...   \n",
       "2011  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2012  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2013  [BOS]Cardiovascular/Pulmonary[SEP]This is a 5-...   \n",
       "2014  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2015  [BOS]Cardiovascular/Pulmonary[SEP]This is a 35...   \n",
       "\n",
       "                  synthetic_sentence_eps8_temp1_topk100  \n",
       "0     [BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...  \n",
       "1     [BOS]Cardiovascular/Pulmonary[SEP]This is a yo...  \n",
       "2     [BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...  \n",
       "3     [BOS]Urology[SEP]Ophytology[SEPURE] The patien...  \n",
       "4     [BOS]Urology[SEP]The patient is a 1-year-old f...  \n",
       "...                                                 ...  \n",
       "2011  [BOS]Cardiovascular/Pulmonary[SEP]Pulmonary/Ho...  \n",
       "2012  [BOS]Cardiovascular/Pulmonary[SEP]After the pr...  \n",
       "2013  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2014  [BOS]Cardiovascular/Pulmonary[SEP]Overmed. The...  \n",
       "2015  [BOS]Cardiovascular/Pulmonary[SEP]FF.UHOSSEP]T...  \n",
       "\n",
       "[2016 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code below is without privacy accountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 29\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     input_ids, attention_mask, target_ids \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mtarget_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1302\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1302\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1303\u001b[0m     input_ids,\n\u001b[1;32m   1304\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1305\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1306\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1307\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1308\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1309\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1310\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1311\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1312\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1313\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1314\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1315\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1317\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1319\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/fastDP/transformers_support.py:283\u001b[0m, in \u001b[0;36mswap_gpt2_model_forward.<locals>.new_forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    273\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    274\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    275\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    284\u001b[0m         hidden_states,\n\u001b[1;32m    285\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    286\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    287\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    288\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    289\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    290\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    291\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    294\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 614\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    615\u001b[0m     hidden_states,\n\u001b[1;32m    616\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    617\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    618\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    619\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    620\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:344\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    346\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    347\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attn\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 196\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    199\u001b[0m         attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mfull(\n\u001b[1;32m    200\u001b[0m             [], value\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    201\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=8,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_eps16'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_incleps16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv') \n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.0.5)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Increased epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=125)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_2016.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilbert no DP and BOS and SEP Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transcription', 'medical_specialty', 'age_related_sentence',\n",
       "       'extracted_text', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffd0950ac07433bb0b8ab021f126190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cce2931bd54ebc9354cd85a6b89a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6a9d67ae3e453683792930417c92e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15e2fbd34d4469806b0b598d4d9ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv') \n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Increased epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95  # Top-p sampling (nucleus sampling)\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_bossep_2014.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HX , (primarily obtained from boyfriend): This 31 y/o RHF experienced a \"flu-like illness 6-8 weeks prior to presentation. 3-4 weeks prior to presentation, she was found \"passed out\" in bed, and when awoken appeared confused, and lethargic. She apparently recovered within 24 hours.\n",
      "[BOS]Neurology[SEP]The patient is an approximately eight year-old boy who is well-stressed with dysphasia and also had a severe headache and felt nauseous for days for 2 hours. She was\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['extracted_text'])\n",
    "print(df.iloc[0]['synthetic_sentence_nodp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is on my schedule today to explore treatment of the above complaints. She has a two-year history of small cell lung cancer, which she says has spread to metastasis in both femurs, her lower lumbar spine, and her pelvis.\n",
      "[BOS]Orthopedic[SEP]This patient is undergoing a comprehensive scan of both bilateral fibrocord fracture and the lateral fibromatoma. He was prepped with a 2,000 x 8,500 cm diameter\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[500]['extracted_text'])\n",
    "print(df.iloc[500]['synthetic_sentence_nodp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another try with more extensive prompt and different temperature\n",
    "* Trial to eliminate initial overlap in original text and synthesized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1).ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m input_ids, attention_mask, target_ids\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Load your dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39myour_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Extract texts and labels from the dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m texts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mextracted_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"Specialty: {self.labels[idx]} \"\n",
    "        text = specialty_label + self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Increase epochs if needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = f\"Patient Case: {specialty}. Details: \" + extracted_text[:max(0, len(extracted_text)//2)]  # Use only part of the prompt\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate this code with fastDP Privacy Engine \n",
    "* Takes 50 min to finetune and the synthesize with the finetuned model every sentence in the dataframe, initial results seems as if the output text is almost usable, not fully,  with high levels of epsilon, need to experiment if the quality improves with more training epochs and very low privacy levels (high epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e5fe4f72684c8fa5cca2b2351899f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62759309ba042daafc97a29533782e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb6a746c52644c3b5521620fc636798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd73ed444855451fb7c3d68fa37e56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b64a7f475949ed922d13a899913db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d107046402d34b29bc305635ca18b8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13443c286074361b150f5e20a999bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"Specialty: {self.labels[idx]} \"\n",
    "        text = specialty_label + self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=3,  # Increased epochs\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Increase epochs if needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences2 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence2 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences2.append(synthetic_sentence2)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_nobossep'] = synthetic_sentences2\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_nodp+dp_2014.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "      <th>synthetic_sentence_dp_nobossep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "      <td>Specialty: Neurology  presentation, she was fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ressive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "      <td>Specialty: Urology  the need for procedure wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "      <td>Specialty: Urology ft vas was grasped in betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "      <td>Specialty: Urology ent was brought to the oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ATION:, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary f corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary pain aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary divertic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \\\n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...   \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN   \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...   \n",
       "\n",
       "                         synthetic_sentence_dp_nobossep  \n",
       "0     Specialty: Neurology  presentation, she was fo...  \n",
       "1     Specialty: Cardiovascular / Pulmonary ressive ...  \n",
       "2     Specialty: Urology  the need for procedure wit...  \n",
       "3     Specialty: Urology ft vas was grasped in betwe...  \n",
       "4     Specialty: Urology ent was brought to the oper...  \n",
       "...                                                 ...  \n",
       "2009  Specialty: Cardiovascular / Pulmonary ATION:, ...  \n",
       "2010  Specialty: Cardiovascular / Pulmonary f corona...  \n",
       "2011  Specialty: Cardiovascular / Pulmonary pain aft...  \n",
       "2012  Specialty: Cardiovascular / Pulmonary patient ...  \n",
       "2013  Specialty: Cardiovascular / Pulmonary divertic...  \n",
       "\n",
       "[2014 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDICATIONS FOR SURGERY,The patient is a 50-year-old white male with history of progressive tracheomalacia treated in the National Tennessee, and several years ago he had a tracheal metallic stent placed with some temporary improvement. However developed progressive problems and he had two additional stents placed with some initial improvement. Subsequently, he developed progressive airway obstruction and came into the ABC Hospital critical airway service for further evaluation and was admitted on Month DD, YYYY.\n",
      "[BOS]Cardiovascular / Pulmonary[SEP] heart cardiac cardiovascular vascular coronary blood arter artery athe cholesterol [cardiovascular]card (cardiologic) card cardi\n",
      "Specialty: Cardiovascular / Pulmonary ressive problems and he had two additional stents placed with some initial improvement. Subsequently, he developed progressive airway obstruction and came into the ABC Hospital critical airway service for further evaluation and was admitted on Month DD, YYYY.ty waster, andters weretingtss. Hettedthers and changed theerersing.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "#print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence2'])\n",
    "print(df.iloc[1]['synthetic_sentence_dp_nobossep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "Specialty: Cardiovascular / Pulmonary This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. I have had a very small heart rate, pulmonary hypertension or pulmonary bronchoscopy. My chest is low at this time and a slight loss in volume of air mass and breath rate. When breathing is normal, my upper body is at the moment\n",
      "Specialty: Cardiovascular / Pulmonary h resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. lung lungs pulmonary respiratory bron air and heart asthma.\n",
      " breathing- oxygen chest throat airflow breath, is or living body with breathe\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is a 78-year-old female with the problem of essential hypertension. She has symptoms that suggested intracranial pathology, but so far work-up has been negative. Sample Doctor4 and was told that she had a massive heart attack in the past. I have not had the opportunity to review any investigative data like chest x-ray, echocardiogram, EKG, etc. So, I advised her to have a chest x-ray and an EKG done before her next appointment, and we will try to get hold of the echocardiogram on her from the office of Dr. Sample Doctor4.\n",
      "Specialty: Cardiovascular / Pulmonary The patient is a 78-year-old female with the problem of essential hypertension. She has symptoms that suggested intracranial pathology, but so far work-up has been negative. Sample Doctor4 and was told that she had a massive heart attack in the past. I have not had the opportunity to review any investigative data like chest x-ray, echocardiogram, EKG, etc. So, I advised her to have a chest x-ray and an EKG done before her next appointment, and we will try to get hold of the echocardiogram on her from the office of Dr. Sample Doctor4. Her current status is on the patient. The EAKG was placed in a large vein at the heart. Since her current medical record is the EKAG in her chest, she is scheduled to be discharged for a final evaluation. This is an\n",
      "Specialty: Cardiovascular / Pulmonary  opportunity to review any investigative data like chest x-ray, echocardiogram, EKG, etc. So, I advised her to have a chest x-ray and an EKG done before her next appointment, and we will try to get hold of the echocardiogram on her from the office of Dr. Sample Doctor4..\n",
      ", and is to could had would in or with that- if has were who needed for without of () she\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[4]['extracted_text'])\n",
    "print(df.iloc[4]['synthetic_sentence'])\n",
    "print(df.iloc[4]['synthetic_sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 69-year-old man status post trauma, slightly prolonged respiratory failure status post tracheostomy, requires another bronchoscopy for further evaluation of refractory pneumonitis. ,PROCEDURE: , The patient was sedated with 5 mg of Versed that was placed on the endotracheal tube.\n",
      "Specialty: Cardiovascular / Pulmonary A 69-year-old man status post trauma, slightly prolonged respiratory failure status post tracheostomy, requires another bronchoscopy for further evaluation of refractory pneumonitis.,PROCEDURE:, The patient was sedated with 5 mg of Versed that was placed on the endotracheal tube. The respiratory tube had 2xCLB with a large 2-inch diameter, a 2mm long x3.5mm thick diameter at the elbow and 1/2mm to the right. This patient had a normal colonoscopia, but\n",
      "Specialty: Cardiovascular / Pulmonary her evaluation of refractory pneumonitis.,PROCEDURE:, The patient was sedated with 5 mg of Versed that was placed on the endotracheal tube. were. The was, and control are () positive non- animals animal dogs dog box containing boxes or mice pets cats mouse\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[10]['extracted_text'])\n",
    "print(df.iloc[10]['synthetic_sentence'])\n",
    "print(df.iloc[10]['synthetic_sentence2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 epoch model from above tried with different parameter settings \n",
    "* no repeat n gram=3\n",
    "* top k 30\n",
    "* top p 0.95 \n",
    "* temperature 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=3,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=30,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.3  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences3 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence3 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences3.append(synthetic_sentence3)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence3'] = synthetic_sentences2\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data3.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output of the synthetic text on the same finetuned model with different parameters is identical, how come ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is a 70-year-old Caucasian female with prior history of lung cancer, status post upper lobectomy. She was recently diagnosed with recurrent pneumonia and does have a cancer on the CAT scan, lung cancer with metastasis.\n",
      "Specialty: Cardiovascular / Pulmonary The patient is a 70-year-old Caucasian female with prior history of lung cancer, status post upper lobectomy. She was recently diagnosed with recurrent pneumonia and does have a cancer on the CAT scan, lung cancer with metastasis. The medical history for her is now over a year ago. This patient has had her lung transplantation in her head which was performed on her CT scan and was able to get her lower back after treatment with radiation at least 7 days ago, and she\n",
      "Specialty: Cardiovascular / Pulmonary as recently diagnosed with recurrent pneumonia and does have a cancer on the CAT scan, lung cancer with metastasis.-. We and are, is that or has space small with can box ()'s in on the board side - a\n",
      "Specialty: Cardiovascular / Pulmonary as recently diagnosed with recurrent pneumonia and does have a cancer on the CAT scan, lung cancer with metastasis.-. We and are, is that or has space small with can box ()'s in on the board side - a\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[8]['extracted_text'])\n",
    "print(df.iloc[8]['synthetic_sentence'])\n",
    "print(df.iloc[8]['synthetic_sentence2'])\n",
    "print(df.iloc[8]['synthetic_sentence3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep_dp.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=3,  # Increased epochs\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Increase epochs if needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences2 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence2 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences2.append(synthetic_sentence2)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence2'] = synthetic_sentences2\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_bossep_dp_2014.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep_dp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...  \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...  \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...  \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN  \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...  \n",
       "...                                                 ...  \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...  \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...  \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...  \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...  \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...  \n",
       "\n",
       "[2014 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient was taken to the Operating Room and placed in the supine position, prepped with Betadine solution and draped in the usual sterile fashion.\n",
      "[BOS]Urology[SEP]WW]Rotation[SSEEUS[SI]Sse-Ssel[1]Y-Y]Bos[3]X-X\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[60]['extracted_text'])\n",
    "#print(df.iloc[8]['synthetic_sentence'])\n",
    "print(df.iloc[60]['synthetic_sentence2'])\n",
    "#print(df.iloc[8]['synthetic_sentence3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_bossep_dp\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_bossep_dp\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilGPT 2 3 epochs, different prompt \"This patient has\" and Epsilon level of 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=8,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=125)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_eps8_v2'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_incleps8_v2.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                          [BOS]Neurology[SEP]This patient has  epidural arterial enlargement - small esphyration of left anterior lumbarotomy. There was not a surgical taping method with a CT or an optical field between the site and the anterior cervical area.,This patient is suffering with a small esphyncetion after his tumor was healed with a 1/4 cm incision. He has found an epureus in pain from her colonum.The patient is very slow.,On the condition of presentation of the patient when this procedure was made. The patient was brought to\n",
      "2                            [BOS]Urology[SEP]This patient has peritoneopileptic, pulmonary edema, oral surgeon, patient on-discussion. She has received anesthesia. She also developed a good working condition. She was brought to his office after hearing hypotosis with the sudden fall of anesthesia. Aplastia, normal upper hypotaxis of the preoperative may be an associated with metastatic and metastatic esence. She admitted operating for the morning with anesthesia. She had an additional pulmonary wound. She was complaining for some time and aneurocopy, the patient was sedation. She told\n",
      "4    [BOS]Urology[SEP]This patient has ienced and experienced acute acute visual altradience and contains a visual quality right where there is currently a visual acibrome that has not been documented throughout the patient with visual acuity. This patient has been experiencing visual visual acuity. It is an an undulating side effects that can be observed and present in the visual preocubabrate. He has been seeing visual acuity between several different visual acuity units. The patient can be presented to a device and a device or an device. While they are presented by the device in the\n",
      "Name: synthetic_sentence_dp_eps8_v2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set display option to show the full string\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Select specific rows from column 'A'\n",
    "rows = df['synthetic_sentence_dp_eps8_v2'].iloc[[0, 2, 4]]\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_eps8_v2\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_eps8_v2\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "      <th>synthetic_sentence_dp_nobossep</th>\n",
       "      <th>synthetic_sentence_dp_eps8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "      <td>Specialty: Neurology  presentation, she was fo...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has  [SV]Sv]Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ressive ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "      <td>Specialty: Urology  the need for procedure wit...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has сстьят]The p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "      <td>Specialty: Urology ft vas was grasped in betwe...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is )This person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "      <td>Specialty: Urology ent was brought to the oper...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has _______H[SI]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ATION:, ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary f corona...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary pain aft...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary patient ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary divertic...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \\\n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...   \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN   \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...   \n",
       "\n",
       "                         synthetic_sentence_dp_nobossep  \\\n",
       "0     Specialty: Neurology  presentation, she was fo...   \n",
       "1     Specialty: Cardiovascular / Pulmonary ressive ...   \n",
       "2     Specialty: Urology  the need for procedure wit...   \n",
       "3     Specialty: Urology ft vas was grasped in betwe...   \n",
       "4     Specialty: Urology ent was brought to the oper...   \n",
       "...                                                 ...   \n",
       "2009  Specialty: Cardiovascular / Pulmonary ATION:, ...   \n",
       "2010  Specialty: Cardiovascular / Pulmonary f corona...   \n",
       "2011  Specialty: Cardiovascular / Pulmonary pain aft...   \n",
       "2012  Specialty: Cardiovascular / Pulmonary patient ...   \n",
       "2013  Specialty: Cardiovascular / Pulmonary divertic...   \n",
       "\n",
       "                             synthetic_sentence_dp_eps8  \n",
       "0     [BOS]Neurology[SEP]This patient has  [SV]Sv]Se...  \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2     [BOS]Urology[SEP]This patient has сстьят]The p...  \n",
       "3     [BOS]Urology[SEP]This patient is )This person ...  \n",
       "4     [BOS]Urology[SEP]This patient has _______H[SI]...  \n",
       "...                                                 ...  \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "\n",
       "[2014 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient was placed in the supine position and sterilely prepped and draped in the usual fashion. After 2% lidocaine was instilled, the anterior urethra is normal. The prostatic urethra reveals mild lateral lobe obstruction.\n",
      "[BOS]Urology[SEP]This patient has ____[SSE]Sse.Ssel.Mul.Nol.Pil.Bos.DilS S s t s\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[100]['extracted_text'])\n",
    "#print(df.iloc[8]['synthetic_sentence'])\n",
    "#print(df.iloc[8]['synthetic_sentence2'])\n",
    "print(df.iloc[100]['synthetic_sentence_dp_eps8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilGPT2 Epoch 3 Eps 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7297e00e8745bf9371c379a326bc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f69d68754294c66bb5bca1f39a39a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab74b31cbbf1436c8489f15fc00f477b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029194adf0ac4618b3574fa35fcb365b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77c3f23fc9644e7ad0194a9faa6037c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ad8d7011df4044a28fad8fe0dd8148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7a3efcf8334e9199e9cef313233d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=16,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_eps16'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_incleps16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_eps16\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_eps16\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient was placed in the supine position and sterilely prepped and draped in the usual fashion. After 2% lidocaine was instilled, the anterior urethra is normal. The prostatic urethra reveals mild lateral lobe obstruction.\n",
      "[BOS]Urology[SEP]This patient has ural electrical insomatrynia.This is a 58-year-old female male whose male had the right anterior otral bulb tube to his left-limapillary vein\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[100]['extracted_text'])\n",
    "#print(df.iloc[8]['synthetic_sentence'])\n",
    "#print(df.iloc[8]['synthetic_sentence2'])\n",
    "print(df.iloc[100]['synthetic_sentence_dp_eps16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "      <th>synthetic_sentence_dp_nobossep</th>\n",
       "      <th>synthetic_sentence_dp_eps16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "      <td>Specialty: Neurology  presentation, she was fo...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has uctal dysf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ressive ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "      <td>Specialty: Urology  the need for procedure wit...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has opedic compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "      <td>Specialty: Urology ft vas was grasped in betwe...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is iphgym who ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "      <td>Specialty: Urology ent was brought to the oper...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has SEP demonstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ATION:, ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary f corona...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary pain aft...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary patient ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary divertic...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \\\n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...   \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN   \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...   \n",
       "\n",
       "                         synthetic_sentence_dp_nobossep  \\\n",
       "0     Specialty: Neurology  presentation, she was fo...   \n",
       "1     Specialty: Cardiovascular / Pulmonary ressive ...   \n",
       "2     Specialty: Urology  the need for procedure wit...   \n",
       "3     Specialty: Urology ft vas was grasped in betwe...   \n",
       "4     Specialty: Urology ent was brought to the oper...   \n",
       "...                                                 ...   \n",
       "2009  Specialty: Cardiovascular / Pulmonary ATION:, ...   \n",
       "2010  Specialty: Cardiovascular / Pulmonary f corona...   \n",
       "2011  Specialty: Cardiovascular / Pulmonary pain aft...   \n",
       "2012  Specialty: Cardiovascular / Pulmonary patient ...   \n",
       "2013  Specialty: Cardiovascular / Pulmonary divertic...   \n",
       "\n",
       "                            synthetic_sentence_dp_eps16  \n",
       "0     [BOS]Neurology[SEP]This patient has uctal dysf...  \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2     [BOS]Urology[SEP]This patient has opedic compl...  \n",
       "3     [BOS]Urology[SEP]This patient is iphgym who ha...  \n",
       "4     [BOS]Urology[SEP]This patient has SEP demonstr...  \n",
       "...                                                 ...  \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "\n",
       "[2014 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilGPT2 Eps 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cc0273b287468a80e256942aef29e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92046886f0a9440d968d2636e4dea9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e0b6f2c6614fffb900e47cd378c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afca0ffb16b7486ebba9f04da123a48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3396a0893b084d5b8235ab5dd0ab3a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b9702b02a744f0923fdc46c48f5cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87431896ab2442e1b33c888eea1582e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=3,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_eps3'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_incleps3.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_eps3\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_eps3\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP no Eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp_temp1'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_epsinfin.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "      <th>synthetic_sentence_dp_nobossep</th>\n",
       "      <th>synthetic_sentence_dp_eps16</th>\n",
       "      <th>synthetic_sentence_dp_eps8_v2</th>\n",
       "      <th>synthetic_sentence_dp_eps3</th>\n",
       "      <th>synthetic_sentence_nodp_temp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "      <td>Specialty: Neurology  presentation, she was fo...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has uctal dysf...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has  epidural ...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has SEPRA, a p...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has This is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ressive ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "      <td>Specialty: Urology  the need for procedure wit...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has opedic compl...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has peritoneopil...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has ergosteretri...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has  The patient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "      <td>Specialty: Urology ft vas was grasped in betwe...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is iphgym who ha...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is ialectonic pr...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is SEP. The pati...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is ,Patient pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "      <td>Specialty: Urology ent was brought to the oper...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has SEP demonstr...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has ienced and e...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has bripped up t...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has  He had a ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ATION:, ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary f corona...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary pain aft...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary patient ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary divertic...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \\\n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...   \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN   \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...   \n",
       "\n",
       "                         synthetic_sentence_dp_nobossep  \\\n",
       "0     Specialty: Neurology  presentation, she was fo...   \n",
       "1     Specialty: Cardiovascular / Pulmonary ressive ...   \n",
       "2     Specialty: Urology  the need for procedure wit...   \n",
       "3     Specialty: Urology ft vas was grasped in betwe...   \n",
       "4     Specialty: Urology ent was brought to the oper...   \n",
       "...                                                 ...   \n",
       "2009  Specialty: Cardiovascular / Pulmonary ATION:, ...   \n",
       "2010  Specialty: Cardiovascular / Pulmonary f corona...   \n",
       "2011  Specialty: Cardiovascular / Pulmonary pain aft...   \n",
       "2012  Specialty: Cardiovascular / Pulmonary patient ...   \n",
       "2013  Specialty: Cardiovascular / Pulmonary divertic...   \n",
       "\n",
       "                            synthetic_sentence_dp_eps16  \\\n",
       "0     [BOS]Neurology[SEP]This patient has uctal dysf...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has opedic compl...   \n",
       "3     [BOS]Urology[SEP]This patient is iphgym who ha...   \n",
       "4     [BOS]Urology[SEP]This patient has SEP demonstr...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                          synthetic_sentence_dp_eps8_v2  \\\n",
       "0     [BOS]Neurology[SEP]This patient has  epidural ...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has peritoneopil...   \n",
       "3     [BOS]Urology[SEP]This patient is ialectonic pr...   \n",
       "4     [BOS]Urology[SEP]This patient has ienced and e...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                             synthetic_sentence_dp_eps3  \\\n",
       "0     [BOS]Neurology[SEP]This patient has SEPRA, a p...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has ergosteretri...   \n",
       "3     [BOS]Urology[SEP]This patient is SEP. The pati...   \n",
       "4     [BOS]Urology[SEP]This patient has bripped up t...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                          synthetic_sentence_nodp_temp1  \n",
       "0     [BOS]Neurology[SEP]This patient has This is a ...  \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2     [BOS]Urology[SEP]This patient has  The patient...  \n",
       "3     [BOS]Urology[SEP]This patient is ,Patient pres...  \n",
       "4     [BOS]Urology[SEP]This patient has  He had a ni...  \n",
       "...                                                 ...  \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "\n",
       "[2014 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_epsinfin\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_epsinfin\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transcription', 'medical_specialty', 'age_related_sentence',\n",
       "       'extracted_text', 'med_masked_transcriptions',\n",
       "       'pii_masked_transcriptions', 'synthetic_sentence_nodp',\n",
       "       'synthetic_sentence2', 'synthetic_sentence_dp_nobossep',\n",
       "       'synthetic_sentence_dp_eps16', 'synthetic_sentence_dp_eps8_v2',\n",
       "       'synthetic_sentence_dp_eps3', 'synthetic_sentence_nodp_temp1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDICATIONS FOR SURGERY,The patient is a 50-year-old white male with history of progressive tracheomalacia treated in the National Tennessee, and several years ago he had a tracheal metallic stent placed with some temporary improvement. However developed progressive problems and he had two additional stents placed with some initial improvement. Subsequently, he developed progressive airway obstruction and came into the ABC Hospital critical airway service for further evaluation and was admitted on Month DD, YYYY.\n",
      "[BOS]Cardiovascular / Pulmonary[SEP]This patient is iphone-developed with two right-sided cramps on the stomach, with both right back, right of the right upper thighs. iphonebral organia\n",
      "[BOS]Cardiovascular / Pulmonary[SEP]This patient is   40-year-old male on 2005-old male of Dr. he had a nice female who was had doing back pain for a relatively pleasant history of low\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "#print(df.iloc[8]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence_dp_eps3'])\n",
    "print(df.iloc[1]['synthetic_sentence_nodp_temp1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve synthetic Text with NoDP finetuned model, different temperature and top k levels, max length 125 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from /home/ray/default/trained_model_with_classifier_distilgpt2_epsinfin\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define the directory where the model and tokenizer are saved\n",
    "save_directory = \"/home/ray/default/trained_model_with_classifier_distilgpt2_epsinfin\"\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# If necessary, move the model to the specified device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded from\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_med_nodp_temp0.6_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_med_nodp_temp0.6_topk50', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_med_nodp_temp0.8_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_med_nodp_temp0.8_topk50', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=1.2\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_med_nodp_temp1.2_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_med_nodp_temp1.2_topk50', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1).ipynb Cell 62\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis patient has \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m index \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThis patient is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# Generate synthetic sentence\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     synthetic_sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length\u001b[39m=\u001b[39;49m\u001b[39m125\u001b[39;49m, top_k\u001b[39m=\u001b[39;49mtop_k, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m, temperature\u001b[39m=\u001b[39;49mtemperature)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     synthetic_sentences\u001b[39m.\u001b[39mappend(synthetic_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1).ipynb Cell 62\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m specialty_label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[BOS]\u001b[39m\u001b[39m{\u001b[39;00mspecialty_label\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m generated_sentence\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1759\u001b[0m         input_ids,\n\u001b[1;32m   1760\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1761\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1762\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1763\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1764\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1765\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config) \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2449\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m streamer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2448\u001b[0m     streamer\u001b[39m.\u001b[39mput(next_tokens\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m-> 2449\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_model_kwargs_for_generation(\n\u001b[1;32m   2450\u001b[0m     outputs,\n\u001b[1;32m   2451\u001b[0m     model_kwargs,\n\u001b[1;32m   2452\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mis_encoder_decoder,\n\u001b[1;32m   2453\u001b[0m )\n\u001b[1;32m   2455\u001b[0m unfinished_sequences \u001b[39m=\u001b[39m unfinished_sequences \u001b[39m&\u001b[39m \u001b[39m~\u001b[39mstopping_criteria(input_ids, scores)\n\u001b[1;32m   2456\u001b[0m this_peer_finished \u001b[39m=\u001b[39m unfinished_sequences\u001b[39m.\u001b[39mmax() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:669\u001b[0m, in \u001b[0;36mGenerationMixin._update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, standardize_cache_format, num_new_tokens)\u001b[0m\n\u001b[1;32m    659\u001b[0m         model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdecoder_attention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    660\u001b[0m             [decoder_attention_mask, decoder_attention_mask\u001b[39m.\u001b[39mnew_ones((decoder_attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))],\n\u001b[1;32m    661\u001b[0m             dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    662\u001b[0m         )\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    665\u001b[0m     model_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    666\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcache_position\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model_kwargs\n\u001b[1;32m    667\u001b[0m     \u001b[39mand\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcache_position\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    668\u001b[0m ):\n\u001b[0;32m--> 669\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcache_position\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mcache_position\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m:] \u001b[39m+\u001b[39;49m num_new_tokens\n\u001b[1;32m    671\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125, top_k=100, top_p=0.95, temperature=1.0):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Different settings for top_k and temperature\n",
    "top_k_values = [50] #100, 1000]\n",
    "temperatures = [0.6, 0.8] #, 1.0, 1.2\n",
    "\n",
    "# Iterate over the settings and generate synthetic sentences for each combination\n",
    "for temperature in temperatures:\n",
    "    for top_k in top_k_values:\n",
    "        synthetic_sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            specialty = row['medical_specialty']\n",
    "            \n",
    "            # Alternate between \"This patient has\" and \"This patient is\"\n",
    "            prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "            \n",
    "            # Generate synthetic sentence\n",
    "            synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=125, top_k=top_k, top_p=0.95, temperature=temperature)\n",
    "            synthetic_sentences.append(synthetic_sentence)\n",
    "        \n",
    "        # Append the synthetic sentences to the dataframe\n",
    "        column_name = f'synthetic_sentence_temp_{temperature}_topk_{top_k}'\n",
    "        df[column_name] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_with_varied_settings.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_with_varied_settings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "      <th>synthetic_sentence_dp_nobossep</th>\n",
       "      <th>synthetic_sentence_dp_eps16</th>\n",
       "      <th>synthetic_sentence_dp_eps8_v2</th>\n",
       "      <th>synthetic_sentence_dp_eps3</th>\n",
       "      <th>synthetic_sentence_nodp_temp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is an approxima...</td>\n",
       "      <td>[BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...</td>\n",
       "      <td>Specialty: Neurology  presentation, she was fo...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has uctal dysf...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has  epidural ...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has SEPRA, a p...</td>\n",
       "      <td>[BOS]Neurology[SEP]This patient has This is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a[AGE] ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ressive ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS:  ,This 34-year-old gentleman has ...</td>\n",
       "      <td>This 34-year-old gentleman has come to the off...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>This[AGE][SEX] has come to the office requesti...</td>\n",
       "      <td>[BOS]Urology[SEP]This is an 18-year-old male, ...</td>\n",
       "      <td>[BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...</td>\n",
       "      <td>Specialty: Urology  the need for procedure wit...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has opedic compl...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has peritoneopil...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has ergosteretri...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has  The patient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]After the procedure was over,...</td>\n",
       "      <td>[BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN</td>\n",
       "      <td>Specialty: Urology ft vas was grasped in betwe...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is iphgym who ha...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is ialectonic pr...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is SEP. The pati...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is ,Patient pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 26-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...</td>\n",
       "      <td>Specialty: Urology ent was brought to the oper...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has SEP demonstr...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has ienced and e...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has bripped up t...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient has  He had a ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>TYPE OF TEST: , Adenosine with nuclear scan as...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>Adenosine with nuclear scan as the patient una...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] heart car...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary ATION:, ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 40-year-old white male who pr...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] who presents ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] blood pre...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary f corona...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old woman whom I have...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>The patient is a[AGE][SEX] whom I have been fo...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac h...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary pain aft...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a 60-year-old gentleman who had a marke...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] card type...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary patient ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>REASON FOR CONSULTATION:  ,Abnormal echocardio...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:  ,The patient is a...</td>\n",
       "      <td>The patient is an 86-year-old female admitted ...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>The patient is an[AGE][SEX] admitted for evalu...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]The patien...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "      <td>Specialty: Cardiovascular / Pulmonary divertic...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP]This patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1     PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2     PROCEDURE: , Elective male sterilization via b...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "...                                                 ...   \n",
       "2009  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "2010  CHIEF COMPLAINT: , Chest pain.,HISTORY OF PRES...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "2013  REASON FOR CONSULTATION:  ,Abnormal echocardio...   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0                      Neurology   \n",
       "1     Cardiovascular / Pulmonary   \n",
       "2                        Urology   \n",
       "3                        Urology   \n",
       "4                        Urology   \n",
       "...                          ...   \n",
       "2009  Cardiovascular / Pulmonary   \n",
       "2010  Cardiovascular / Pulmonary   \n",
       "2011  Cardiovascular / Pulmonary   \n",
       "2012  Cardiovascular / Pulmonary   \n",
       "2013  Cardiovascular / Pulmonary   \n",
       "\n",
       "                                   age_related_sentence  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     INDICATIONS:  ,This 34-year-old gentleman has ...   \n",
       "3     DESCRIPTION:,  The patient was placed in the s...   \n",
       "4     INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "...                                                 ...   \n",
       "2009  TYPE OF TEST: , Adenosine with nuclear scan as...   \n",
       "2010  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "2011  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  HISTORY OF PRESENT ILLNESS:  ,The patient is a...   \n",
       "\n",
       "                                         extracted_text  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2     This 34-year-old gentleman has come to the off...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a 40-year-old white male who pr...   \n",
       "2011  The patient is a 68-year-old woman whom I have...   \n",
       "2012  ABC is a 60-year-old gentleman who had a marke...   \n",
       "2013  The patient is an 86-year-old female admitted ...   \n",
       "\n",
       "                              med_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                              pii_masked_transcriptions  \\\n",
       "0     HX , (primarily obtained from boyfriend): This...   \n",
       "1     INDICATIONS FOR SURGERY,The patient is a[AGE] ...   \n",
       "2     This[AGE][SEX] has come to the office requesti...   \n",
       "3     The patient was placed in the supine position ...   \n",
       "4     A gentleman who is here today requesting volun...   \n",
       "...                                                 ...   \n",
       "2009  Adenosine with nuclear scan as the patient una...   \n",
       "2010  The patient is a[AGE] white[SEX] who presents ...   \n",
       "2011  The patient is a[AGE][SEX] whom I have been fo...   \n",
       "2012  ABC is a[AGE][SEX] who had a[LAB_VALUE][LAB_VA...   \n",
       "2013  The patient is an[AGE][SEX] admitted for evalu...   \n",
       "\n",
       "                                synthetic_sentence_nodp  \\\n",
       "0     [BOS]Neurology[SEP]The patient is an approxima...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2     [BOS]Urology[SEP]This is an 18-year-old male, ...   \n",
       "3     [BOS]Urology[SEP]After the procedure was over,...   \n",
       "4     [BOS]Urology[SEP]The patient is a 26-year-old ...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]The patien...   \n",
       "\n",
       "                                    synthetic_sentence2  \\\n",
       "0     [BOS]Neurology[SEP] Ne ne]D D (N,D)Ne (D,N)D (...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2     [BOS]Urology[SEP]Puurology [SEuep]QiQiiQivQeiq...   \n",
       "3                    [BOS]Urology[SEP]MM)\\nN\\n\\nM\\nS\\nN   \n",
       "4     [BOS]Urology[SEP]P__ [SE\\n B)\\n\\nUrologic\\nK]\\...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP] heart car...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP] blood pre...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP] cardiac h...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP] card type...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP] cholester...   \n",
       "\n",
       "                         synthetic_sentence_dp_nobossep  \\\n",
       "0     Specialty: Neurology  presentation, she was fo...   \n",
       "1     Specialty: Cardiovascular / Pulmonary ressive ...   \n",
       "2     Specialty: Urology  the need for procedure wit...   \n",
       "3     Specialty: Urology ft vas was grasped in betwe...   \n",
       "4     Specialty: Urology ent was brought to the oper...   \n",
       "...                                                 ...   \n",
       "2009  Specialty: Cardiovascular / Pulmonary ATION:, ...   \n",
       "2010  Specialty: Cardiovascular / Pulmonary f corona...   \n",
       "2011  Specialty: Cardiovascular / Pulmonary pain aft...   \n",
       "2012  Specialty: Cardiovascular / Pulmonary patient ...   \n",
       "2013  Specialty: Cardiovascular / Pulmonary divertic...   \n",
       "\n",
       "                            synthetic_sentence_dp_eps16  \\\n",
       "0     [BOS]Neurology[SEP]This patient has uctal dysf...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has opedic compl...   \n",
       "3     [BOS]Urology[SEP]This patient is iphgym who ha...   \n",
       "4     [BOS]Urology[SEP]This patient has SEP demonstr...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                          synthetic_sentence_dp_eps8_v2  \\\n",
       "0     [BOS]Neurology[SEP]This patient has  epidural ...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has peritoneopil...   \n",
       "3     [BOS]Urology[SEP]This patient is ialectonic pr...   \n",
       "4     [BOS]Urology[SEP]This patient has ienced and e...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                             synthetic_sentence_dp_eps3  \\\n",
       "0     [BOS]Neurology[SEP]This patient has SEPRA, a p...   \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2     [BOS]Urology[SEP]This patient has ergosteretri...   \n",
       "3     [BOS]Urology[SEP]This patient is SEP. The pati...   \n",
       "4     [BOS]Urology[SEP]This patient has bripped up t...   \n",
       "...                                                 ...   \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...   \n",
       "\n",
       "                          synthetic_sentence_nodp_temp1  \n",
       "0     [BOS]Neurology[SEP]This patient has This is a ...  \n",
       "1     [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2     [BOS]Urology[SEP]This patient has  The patient...  \n",
       "3     [BOS]Urology[SEP]This patient is ,Patient pres...  \n",
       "4     [BOS]Urology[SEP]This patient has  He had a ni...  \n",
       "...                                                 ...  \n",
       "2009  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2010  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2011  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2012  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "2013  [BOS]Cardiovascular / Pulmonary[SEP]This patie...  \n",
       "\n",
       "[2014 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1).ipynb Cell 64\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Alternate between \"This patient has\" and \"This patient is\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis patient has \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m index \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThis patient is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     synthetic_sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length\u001b[39m=\u001b[39;49m\u001b[39m125\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     synthetic_sentences\u001b[39m.\u001b[39mappend(synthetic_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1).ipynb Cell 64\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m specialty_label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[BOS]\u001b[39m\u001b[39m{\u001b[39;00mspecialty_label\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-e7njmn952qjhqnfp8efu1gnfxi.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m generated_sentence\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1759\u001b[0m         input_ids,\n\u001b[1;32m   1760\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1761\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1762\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1763\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1764\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1765\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config) \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2412\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2410\u001b[0m next_token_scores \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m   2411\u001b[0m \u001b[39mif\u001b[39;00m do_sample:\n\u001b[0;32m-> 2412\u001b[0m     next_token_scores \u001b[39m=\u001b[39m logits_warper(input_ids, next_token_scores)\n\u001b[1;32m   2414\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/logits_process.py:458\u001b[0m, in \u001b[0;36mTopPLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, scores: torch\u001b[39m.\u001b[39mFloatTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 458\u001b[0m     sorted_logits, sorted_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msort(scores, descending\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    459\u001b[0m     cumulative_probs \u001b[39m=\u001b[39m sorted_logits\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcumsum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Load your dataframe (assuming it's a CSV file)\n",
    "#df = pd.read_csv('path/to/your/dataframe.csv')\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=125)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp_topk50_temp0.6'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_noeps_topk50_temp0.6.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_epsinfin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Out DistilGPT 2 with Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 51.137359619140625\n",
      "Epoch 1, Loss: 39.4566764831543\n",
      "Epoch 1, Loss: 27.521827697753906\n",
      "Epoch 1, Loss: 16.809467315673828\n",
      "Epoch 1, Loss: 9.358817100524902\n",
      "Epoch 1, Loss: 8.272370338439941\n",
      "Epoch 1, Loss: 7.515048980712891\n",
      "Epoch 1, Loss: 6.875888824462891\n",
      "Epoch 1, Loss: 6.785244464874268\n",
      "Epoch 1, Loss: 6.947749137878418\n",
      "Epoch 1, Loss: 6.826881408691406\n",
      "Epoch 1, Loss: 7.114898681640625\n",
      "Epoch 1, Loss: 7.026941299438477\n",
      "Epoch 1, Loss: 6.999791622161865\n",
      "Epoch 1, Loss: 6.914890766143799\n",
      "Epoch 1, Loss: 6.830888748168945\n",
      "Epoch 1, Loss: 6.781040668487549\n",
      "Epoch 1, Loss: 6.481906414031982\n",
      "Epoch 1, Loss: 6.67255973815918\n",
      "Epoch 1, Loss: 6.323824405670166\n",
      "Epoch 1, Loss: 6.165910243988037\n",
      "Epoch 1, Loss: 6.233280658721924\n",
      "Epoch 1, Loss: 6.165066242218018\n",
      "Epoch 1, Loss: 6.108913898468018\n",
      "Epoch 1, Loss: 5.992735862731934\n",
      "Epoch 1, Loss: 5.888227939605713\n",
      "Epoch 1, Loss: 5.845276355743408\n",
      "Epoch 1, Loss: 5.711322784423828\n",
      "Epoch 1, Loss: 5.680025100708008\n",
      "Epoch 1, Loss: 5.732956409454346\n",
      "Epoch 1, Loss: 5.736355781555176\n",
      "Epoch 1, Loss: 5.575199127197266\n",
      "Epoch 1, Loss: 5.662578582763672\n",
      "Epoch 1, Loss: 5.481935024261475\n",
      "Epoch 1, Loss: 5.429534435272217\n",
      "Epoch 1, Loss: 5.357743263244629\n",
      "Epoch 1, Loss: 5.370070457458496\n",
      "Epoch 1, Loss: 5.480813026428223\n",
      "Epoch 1, Loss: 5.35233736038208\n",
      "Epoch 1, Loss: 5.388701438903809\n",
      "Epoch 1, Loss: 5.292302131652832\n",
      "Epoch 1, Loss: 5.317708969116211\n",
      "Epoch 1, Loss: 5.305972099304199\n",
      "Epoch 1, Loss: 5.238076210021973\n",
      "Epoch 1, Loss: 5.036715984344482\n",
      "Epoch 1, Loss: 5.15102481842041\n",
      "Epoch 1, Loss: 5.138328552246094\n",
      "Epoch 1, Loss: 5.321842193603516\n",
      "Epoch 1, Loss: 5.1817121505737305\n",
      "Epoch 1, Loss: 4.970783710479736\n",
      "Epoch 1, Loss: 4.8724045753479\n",
      "Epoch 2, Loss: 4.933994293212891\n",
      "Epoch 2, Loss: 4.738032817840576\n",
      "Epoch 2, Loss: 5.046582221984863\n",
      "Epoch 2, Loss: 4.894203186035156\n",
      "Epoch 2, Loss: 4.879865646362305\n",
      "Epoch 2, Loss: 4.918387413024902\n",
      "Epoch 2, Loss: 4.899734973907471\n",
      "Epoch 2, Loss: 4.81064510345459\n",
      "Epoch 2, Loss: 4.779226303100586\n",
      "Epoch 2, Loss: 4.805920124053955\n",
      "Epoch 2, Loss: 4.890204429626465\n",
      "Epoch 2, Loss: 4.664146423339844\n",
      "Epoch 2, Loss: 4.814478397369385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/rss@vtti.com/Kyra_Test (1) (1).ipynb Cell 60\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m classification_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(classification_logits, classification_labels)\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m classification_loss\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y200sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=3):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Increased epochs\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Example: Combine language modeling and classification loss\n",
    "        classification_labels = torch.randint(0, 3, (input_ids.size(0),)).to(device)  # Random classification labels for example\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.738882177016315, Accuracy: 0.08007448789571694\n",
      "Epoch 2, Loss: 6.0671451792997475, Accuracy: 0.08504034761018\n",
      "Epoch 3, Loss: 5.635770835128485, Accuracy: 0.08752327746741155\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=11):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training settings\n",
    "epochs = 3  # Adjust the number of epochs here\n",
    "\n",
    "# Variables to store loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Assuming classification_labels is provided in the batch\n",
    "        classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(classification_logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == classification_labels)\n",
    "        total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 10.453631176668054, Training Accuracy: 0.09621353196772191\n",
      "Epoch 1, Validation Loss: 8.597789691044735, Validation Accuracy: 0.08684863523573201\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAHWCAYAAADHF/LFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABy+ElEQVR4nO3deXxM5////+dkX0hsaRZiX2KNCiLe1kqFqgpqSbVBqS4oUi1pbV1UF1qU8mlrqdbe2rpRFC1iF43a09gqCYqEICE5vz98za/TBIkmJsbjfrvN7d25znWueV3nnfY1rznnXMdkGIYhAAAAAABgE+ysHQAAAAAAAMg/FPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQD/yezZs2UymbRjxw5rhwIAAHLw6aefymQyKTg42NqhALhHKPQBAAAAGzZ37lyVL19e27Zt05EjR6wdDoB7gEIfAAAAsFEJCQnavHmzPvroI3l5eWnu3LnWDilHaWlp1g4BsCkU+gAK3O7du9W2bVt5eHioSJEiatWqlbZs2WLR59q1a3rzzTdVpUoVubi4qGTJkmrSpIlWr15t7pOUlKTevXurTJkycnZ2lq+vrzp06KCjR4/e4xkBAHB/mDt3rooXL6527drpySefzLHQv3DhgoYMGaLy5cvL2dlZZcqUUWRkpM6ePWvuc/XqVY0ZM0ZVq1aVi4uLfH191alTJ8XHx0uS1q9fL5PJpPXr11uMffToUZlMJs2ePdvc1qtXLxUpUkTx8fF67LHHVLRoUfXo0UOS9Ntvv6lLly4qW7asnJ2d5e/vryFDhujKlSvZ4j5w4IC6du0qLy8vubq6qlq1anrjjTckSevWrZPJZNLSpUuz7Tdv3jyZTCbFxMTk+XgC9wsHawcAwLb98ccfatq0qTw8PPTaa6/J0dFR//d//6cWLVpow4YN5vsFx4wZo3Hjxqlv375q2LChUlNTtWPHDu3atUuPPvqoJKlz5876448/NHDgQJUvX16nT5/W6tWrdfz4cZUvX96KswQAoHCaO3euOnXqJCcnJ0VERGjatGnavn27GjRoIEm6dOmSmjZtqv379+vZZ59VvXr1dPbsWa1YsUInT55UqVKllJmZqccff1xr165V9+7dNWjQIF28eFGrV6/W3r17ValSpTzHdf36dYWFhalJkyYaP3683NzcJEmLFy/W5cuX9eKLL6pkyZLatm2bPvnkE508eVKLFy827//777+radOmcnR0VL9+/VS+fHnFx8fru+++09ixY9WiRQv5+/tr7ty56tixY7ZjUqlSJYWEhPyHIwsUcgYA/AezZs0yJBnbt2/PcXt4eLjh5ORkxMfHm9tOnTplFC1a1GjWrJm5LTAw0GjXrt0tP+f8+fOGJOPDDz/Mv+ABALBhO3bsMCQZq1evNgzDMLKysowyZcoYgwYNMvcZNWqUIclYsmRJtv2zsrIMwzCMmTNnGpKMjz766JZ91q1bZ0gy1q1bZ7E9ISHBkGTMmjXL3NazZ09DkjF8+PBs412+fDlb27hx4wyTyWQcO3bM3NasWTOjaNGiFm3/jMcwDCM6OtpwdnY2Lly4YG47ffq04eDgYIwePTrb5wC2hEv3ARSYzMxM/fzzzwoPD1fFihXN7b6+vnrqqae0ceNGpaamSpKKFSumP/74Q4cPH85xLFdXVzk5OWn9+vU6f/78PYkfAID72dy5c+Xt7a2WLVtKkkwmk7p166YFCxYoMzNTkvTtt98qMDAw21nvm/1v9ilVqpQGDhx4yz5348UXX8zW5urqav7ntLQ0nT17Vo0bN5ZhGNq9e7ck6cyZM/r111/17LPPqmzZsreMJzIyUunp6frmm2/MbQsXLtT169f19NNP33XcwP2AQh9AgTlz5owuX76satWqZdtWvXp1ZWVl6cSJE5Kkt956SxcuXFDVqlVVu3Ztvfrqq/r999/N/Z2dnfX+++/rp59+kre3t5o1a6YPPvhASUlJ92w+AADcLzIzM7VgwQK1bNlSCQkJOnLkiI4cOaLg4GAlJydr7dq1kqT4+HjVqlXrtmPFx8erWrVqcnDIv7t+HRwcVKZMmWztx48fV69evVSiRAkVKVJEXl5eat68uSQpJSVFkvTnn39K0h3jDggIUIMGDSzWJZg7d64aNWqkypUr59dUgEKJQh9AodCsWTPFx8dr5syZqlWrlr744gvVq1dPX3zxhbnP4MGDdejQIY0bN04uLi4aOXKkqlevbv6FHwAA3PDLL78oMTFRCxYsUJUqVcyvrl27SlK+r75/qzP7N68c+DdnZ2fZ2dll6/voo4/qhx9+0LBhw7Rs2TKtXr3avJBfVlZWnuOKjIzUhg0bdPLkScXHx2vLli2czccDgcX4ABQYLy8vubm56eDBg9m2HThwQHZ2dvL39ze3lShRQr1791bv3r116dIlNWvWTGPGjFHfvn3NfSpVqqRXXnlFr7zyig4fPqy6detqwoQJ+vrrr+/JnAAAuB/MnTtXDz30kKZOnZpt25IlS7R06VJNnz5dlSpV0t69e287VqVKlbR161Zdu3ZNjo6OOfYpXry4pBsr+P/TsWPHch1zXFycDh06pC+//FKRkZHm9n8+gUeS+XbAO8UtSd27d1dUVJTmz5+vK1euyNHRUd26dct1TMD9ijP6AAqMvb29WrdureXLl1s8Ai85OVnz5s1TkyZN5OHhIUn6+++/LfYtUqSIKleurPT0dEnS5cuXdfXqVYs+lSpVUtGiRc19AACAdOXKFS1ZskSPP/64nnzyyWyvAQMG6OLFi1qxYoU6d+6sPXv25PgYOsMwJN146s3Zs2c1ZcqUW/YpV66c7O3t9euvv1ps//TTT3Mdt729vcWYN/950qRJFv28vLzUrFkzzZw5U8ePH88xnptKlSqltm3b6uuvv9bcuXPVpk0blSpVKtcxAfcrzugDyBczZ87UypUrs7WPGTNGq1evVpMmTfTSSy/JwcFB//d//6f09HR98MEH5n41atRQixYtFBQUpBIlSmjHjh365ptvNGDAAEnSoUOH1KpVK3Xt2lU1atSQg4ODli5dquTkZHXv3v2ezRMAgMJuxYoVunjxop544okctzdq1EheXl6aO3eu5s2bp2+++UZdunTRs88+q6CgIJ07d04rVqzQ9OnTFRgYqMjISM2ZM0dRUVHatm2bmjZtqrS0NK1Zs0YvvfSSOnToIE9PT3Xp0kWffPKJTCaTKlWqpO+//16nT5/OddwBAQGqVKmShg4dqr/++kseHh769ttvc1yEd/LkyWrSpInq1aunfv36qUKFCjp69Kh++OEHxcbGWvSNjIzUk08+KUl6++23c38ggfuZNZf8B3D/u/l4vVu9Tpw4YezatcsICwszihQpYri5uRktW7Y0Nm/ebDHOO++8YzRs2NAoVqyY4erqagQEBBhjx441MjIyDMMwjLNnzxr9+/c3AgICDHd3d8PT09MIDg42Fi1aZI1pAwBQaLVv395wcXEx0tLSbtmnV69ehqOjo3H27Fnj77//NgYMGGCULl3acHJyMsqUKWP07NnTOHv2rLn/5cuXjTfeeMOoUKGC4ejoaPj4+BhPPvmkxeNzz5w5Y3Tu3Nlwc3Mzihcvbjz//PPG3r17c3y8nru7e45x7du3zwgNDTWKFClilCpVynjuueeMPXv2ZBvDMAxj7969RseOHY1ixYoZLi4uRrVq1YyRI0dmGzM9Pd0oXry44enpaVy5ciWXRxG4v5kM41/XtwAAAACAjbh+/br8/PzUvn17zZgxw9rhAPcE9+gDAAAAsFnLli3TmTNnLBb4A2wdZ/QBAAAA2JytW7fq999/19tvv61SpUpp165d1g4JuGc4ow8AAADA5kybNk0vvviiHnroIc2ZM8fa4QD3FGf0AQAAAACwIZzRBwAAAADAhlDoAwAAAABgQxysHcD9KisrS6dOnVLRokVlMpmsHQ4AADIMQxcvXpSfn5/s7Pgt/78i1wMACpvc5noK/bt06tQp+fv7WzsMAACyOXHihMqUKWPtMO575HoAQGF1p1xPoX+XihYtKunGAfbw8LByNAAASKmpqfL39zfnKPw35HoAQGGT21xPoX+Xbl7C5+HhQfIHABQqXGaeP8j1AIDC6k65nhv4AAAAAACwIRT6AAAAAADYEKsW+r/++qvat28vPz8/mUwmLVu2zGK7YRgaNWqUfH195erqqtDQUB0+fPi2Y44ZM0Ymk8niFRAQYNHn6tWr6t+/v0qWLKkiRYqoc+fOSk5Ozu/pAQAAAABwz1n1Hv20tDQFBgbq2WefVadOnbJt/+CDDzR58mR9+eWXqlChgkaOHKmwsDDt27dPLi4utxy3Zs2aWrNmjfm9g4PlNIcMGaIffvhBixcvlqenpwYMGKBOnTpp06ZN+Tc5AA8swzB0/fp1ZWZmWjsU2Bh7e3s5ODhwDz6ABxp5FrYsv3K9VQv9tm3bqm3btjluMwxDEydO1IgRI9ShQwdJ0pw5c+Tt7a1ly5ape/futxzXwcFBPj4+OW5LSUnRjBkzNG/ePD3yyCOSpFmzZql69erasmWLGjVqlON+6enpSk9PN79PTU3N1RwBPFgyMjKUmJioy5cvWzsU2Cg3Nzf5+vrKycnJ2qEAwD1HnsWDID9yfaFddT8hIUFJSUkKDQ01t3l6eio4OFgxMTG3LfQPHz4sPz8/ubi4KCQkROPGjVPZsmUlSTt37tS1a9csxg0ICFDZsmUVExNzy0J/3LhxevPNN/NpdgBsUVZWlhISEmRvby8/Pz85OTlx5hX5xjAMZWRk6MyZM0pISFCVKlVkZ8dSOwAeHORZ2Lr8zPWFttBPSkqSJHl7e1u0e3t7m7flJDg4WLNnz1a1atWUmJioN998U02bNtXevXtVtGhRJSUlycnJScWKFcvTuNHR0YqKijK/v/n8QgC4KSMjQ1lZWfL395ebm5u1w4ENcnV1laOjo44dO6aMjIzb3sYGALaGPIsHQX7l+kJb6N+tf94KUKdOHQUHB6tcuXJatGiR+vTpc9fjOjs7y9nZOT9CBGDjOMuKgsTfF4AHHf8dhK3Lj7/xQvtvyc177P+9Gn5ycvIt77/PSbFixVS1alUdOXLEPG5GRoYuXLjwn8YFAAAAAKAwKrSFfoUKFeTj46O1a9ea21JTU7V161aFhITkepxLly4pPj5evr6+kqSgoCA5OjpajHvw4EEdP348T+MCAAAAAFAYWbXQv3TpkmJjYxUbGyvpxgJ8sbGxOn78uEwmkwYPHqx33nlHK1asUFxcnCIjI+Xn56fw8HDzGK1atdKUKVPM74cOHaoNGzbo6NGj2rx5szp27Ch7e3tFRERIurGgX58+fRQVFaV169Zp586d6t27t0JCQm65EB8AIO/Kly+viRMn5rr/+vXrZTKZsl1xBQAAsiPP4naseo/+jh071LJlS/P7m4vd9ezZU7Nnz9Zrr72mtLQ09evXTxcuXFCTJk20cuVKiwUJ4uPjdfbsWfP7kydPKiIiQn///be8vLzUpEkTbdmyRV5eXuY+H3/8sezs7NS5c2elp6crLCxMn3766T2YMQAUPndasXj06NEaM2ZMnsfdvn273N3dc92/cePGSkxMlKenZ54/Ky/Wr1+vli1b6vz589kWZgUAIL89aHn2nwICApSQkKBjx45xm/Q9ZjIMw7B2EPej1NRUeXp6KiUlRR4eHtYOB0AhcPXqVSUkJKhChQr31Wro/3ziyMKFCzVq1CgdPHjQ3FakSBEVKVJE0o3HvmRmZsrB4f5dy/V+L/Rv93dGbspfHE+gcCHP3l82btyoHj16qEmTJqpTp46GDRtm1XiuXbsmR0dHq8aQW/mR6wvtPfoAYAsMw9DljOv3/JWX33B9fHzML09PT5lMJvP7AwcOqGjRovrpp58UFBQkZ2dnbdy4UfHx8erQoYO8vb1VpEgRNWjQQGvWrLEY99+XFJpMJn3xxRfq2LGj3NzcVKVKFa1YscK8/d+XFM6ePVvFihXTqlWrVL16dRUpUkRt2rRRYmKieZ/r16/r5ZdfVrFixVSyZEkNGzZMPXv2tLjFK6/Onz+vyMhIFS9eXG5ubmrbtq0OHz5s3n7s2DG1b99exYsXl7u7u2rWrKkff/zRvG+PHj3k5eUlV1dXValSRbNmzbrrWAAAt0eenWh+X9jy7IwZM/TUU0/pmWee0cyZM7Ntv3kldokSJeTu7q769etr69at5u3fffedGjRoIBcXF5UqVUodO3a0mOuyZcssxitWrJhmz54tSTp69KhMJpMWLlyo5s2by8XFRXPnztXff/+tiIgIlS5dWm5ubqpdu7bmz59vMU5WVpY++OADVa5cWc7OzipbtqzGjh0rSXrkkUc0YMAAi/5nzpyRk5OTxRpwhcH9/1MRABRiV65lqsaoVff8c/e9FSY3p/z7T/zw4cM1fvx4VaxYUcWLF9eJEyf02GOPaezYsXJ2dtacOXPUvn17HTx4UGXLlr3lOG+++aY++OADffjhh/rkk0/Uo0cPHTt2TCVKlMix/+XLlzV+/Hh99dVXsrOz09NPP62hQ4dq7ty5kqT3339fc+fO1axZs1S9enVNmjRJy5Yts7gtLK969eqlw4cPa8WKFfLw8NCwYcP02GOPad++fXJ0dFT//v2VkZGhX3/9Ve7u7tq3b5/5TMzIkSO1b98+/fTTTypVqpSOHDmiK1eu3HUsAIDbI89aKix59uLFi1q8eLG2bt2qgIAApaSk6LffflPTpk0l3VirrXnz5ipdurRWrFghHx8f7dq1S1lZWZKkH374QR07dtQbb7yhOXPmKCMjw/yjel6P64QJE/Twww/LxcVFV69eVVBQkIYNGyYPDw/98MMPeuaZZ1SpUiU1bNhQkhQdHa3PP/9cH3/8sZo0aaLExEQdOHBAktS3b18NGDBAEyZMMD96/euvv1bp0qX1yCOP5Dm+gkShDwC4o7feekuPPvqo+X2JEiUUGBhofv/2229r6dKlWrFiRbZfuv+pV69e5sVR3333XU2ePFnbtm1TmzZtcux/7do1TZ8+XZUqVZIkDRgwQG+99ZZ5+yeffKLo6Gjzr/xTpky5qy8CN90s8Ddt2qTGjRtLkubOnSt/f38tW7ZMXbp00fHjx9W5c2fVrl1bklSxYkXz/sePH9fDDz+s+vXrS7pxtgUAgDuxtTy7YMECValSRTVr1pQkde/eXTNmzDAX+vPmzdOZM2e0fft2848QlStXNu8/duxYde/eXW+++aa57Z/HI7cGDx6sTp06WbQNHTrU/M8DBw7UqlWrtGjRIjVs2FAXL17UpEmTNGXKFPXs2VOSVKlSJTVp0kSS1KlTJw0YMEDLly9X165dJd24MqJXr153XIvhXqPQB4AC5Opor31vhVnlc/PTzcL1pkuXLmnMmDH64YcflJiYqOvXr+vKlSs6fvz4bcepU6eO+Z/d3d3l4eGh06dP37K/m5ub+cuHJPn6+pr7p6SkKDk52fwLvCTZ29srKCjIfEYgr/bv3y8HBwcFBweb20qWLKlq1app//79kqSXX35ZL774on7++WeFhoaqc+fO5nm9+OKL6ty5s3bt2qXWrVsrPDzc/IMBACD/kWctFZY8O3PmTD399NPm908//bSaN2+uTz75REWLFlVsbKwefvjhW15pEBsbq+eee+62n5Eb/z6umZmZevfdd7Vo0SL99ddfysjIUHp6utzc3CTd+B6Qnp6uVq1a5Tiei4uL+VaErl27ateuXdq7d6/FLRKFBYU+ABQgk8mUr5f2Wcu/V/UdOnSoVq9erfHjx6ty5cpydXXVk08+qYyMjNuO8+9FcEwm022/LOTU39pryPbt21dhYWH64Ycf9PPPP2vcuHGaMGGCBg4cqLZt2+rYsWP68ccftXr1arVq1Ur9+/fX+PHjrRozANgq8qylwpBn9+3bpy1btmjbtm0WC/BlZmZqwYIFeu655+Tq6nrbMe60Pac4r127lq3fv4/rhx9+qEmTJmnixImqXbu23N3dNXjwYPNxvdPnSje+B9StW1cnT57UrFmz9Mgjj6hcuXJ33O9eYzE+AECebdq0Sb169VLHjh1Vu3Zt+fj46OjRo/c0Bk9PT3l7e2v79u3mtszMTO3ateuux6xevbquX79usRjQ33//rYMHD6pGjRrmNn9/f73wwgtasmSJXnnlFX3++efmbV5eXurZs6e+/vprTZw4UZ999tldx2NLpk6dqvLly8vFxUXBwcHatm3bbfsvXrxYAQEBcnFxUe3atbNdKpqcnKxevXrJz89Pbm5uatOmjcWiiTfFxMTokUceMZ/ZatasGesmACj07uc8O2PGDDVr1kx79uxRbGys+RUVFaUZM2ZIunHlQWxsrM6dO5fjGHXq1Lnt4nZeXl4WiwYePnxYly9fvuOcNm3apA4dOujpp59WYGCgKlasqEOHDpm3V6lSRa6urrf97Nq1a6t+/fr6/PPPNW/ePD377LN3/FxroNAHAORZlSpVtGTJEsXGxmrPnj166qmn7vpy+f9i4MCBGjdunJYvX66DBw9q0KBBOn/+fK7uk4uLi7P4ArJnzx5VqVJFHTp00HPPPaeNGzdqz549evrpp1W6dGl16NBB0o37/VatWqWEhATt2rVL69atU/Xq1SVJo0aN0vLly3XkyBH98ccf+v77783bHmQLFy5UVFSURo8erV27dikwMFBhYWG3vJx08+bNioiIUJ8+fbR7926Fh4crPDxce/fulXRjle3w8HD9+eefWr58uXbv3q1y5copNDRUaWlp5nFiYmLUpk0btW7dWtu2bdP27ds1YMAA2dnx9QdA4Xa/5tlr167pq6++UkREhGrVqmXx6tu3r7Zu3ao//vhDERER8vHxUXh4uDZt2qQ///xT3377rWJiYiRJo0eP1vz58zV69Gjt379fcXFxev/9982f88gjj2jKlCnavXu3duzYoRdeeCFXj86rUqWKVq9erc2bN2v//v16/vnnlZycbN7u4uKiYcOG6bXXXtOcOXMUHx+vLVu2mH+guKlv37567733ZBiGxdMAChMyHQAgzz766CMVL15cjRs3Vvv27RUWFqZ69erd8ziGDRumiIgIRUZGKiQkREWKFFFYWFiunq/crFkzPfzww+ZXUFCQJGnWrFkKCgrS448/rpCQEBmGoR9//NH8BSIzM1P9+/dX9erV1aZNG1WtWlWffvqpJMnJyUnR0dGqU6eOmjVrJnt7ey1YsKDgDsB94qOPPtJzzz2n3r17q0aNGpo+fbrc3NxyfNySJE2aNElt2rTRq6++qurVq+vtt99WvXr1NGXKFEk3ztxs2bJF06ZNU4MGDVStWjVNmzZNV65csXhM0pAhQ/Tyyy9r+PDhqlmzpqpVq6auXbuaV0oGgMLqfs2zK1as0N9//51j8Vu9enVVr15dM2bMkJOTk37++Wc99NBDeuyxx1S7dm299957sre/sfZBixYttHjxYq1YsUJ169bVI488YnEl2IQJE+Tv76+mTZvqqaee0tChQ8332d/OiBEjVK9ePYWFhalFixbmHxv+aeTIkXrllVc0atQoVa9eXd26dcv2w3RERIQcHBwUERGRq+8c1mAyrH2z430qNTVVnp6eSklJkYeHh7XDAVAIXL16VQkJCapQoUKh/Y++rcvKylL16tXVtWtXvf3229YOp0Dc7u+sMOamjIwMubm56ZtvvrH4MtWzZ09duHBBy5cvz7ZP2bJlFRUVpcGDB5vbRo8erWXLlmnPnj2Ki4tTnTp1dOTIEYtFpPz9/dWqVSvNnj1bp0+flre3tyZPnqz58+crPj5eAQEBGjt2rHn15H9LT09Xenq6+X1qaqr8/f0L1fEEHmTkWet7EPJsbhw9elSVKlXS9u3bC+QHmPzI9ZzRBwDct44dO6bPP/9chw4dUlxcnF588UUlJCToqaeesnZo+H/Onj2rzMxMeXt7W7R7e3srKSkpx32SkpJu2z8gIEBly5ZVdHS0zp8/r4yMDL3//vs6efKk+Z7NP//8U5I0ZswYPffcc1q5cqXq1aunVq1a5XgvvySNGzdOnp6e5pe/v/9/mjsA3O/Is5auXbumpKQkjRgxQo0aNbLKVRa5RaEPALhv2dnZafbs2WrQoIH+97//KS4uTmvWrOG+eBvn6OioJUuW6NChQypRooTc3Ny0bt06tW3b1nz//c17WZ9//nn17t1bDz/8sD7++GNVq1btlrcMREdHKyUlxfw6ceLEPZsTABRG5FlLmzZtkq+vr7Zv367p06dbO5zbuv+fRQEAeGD5+/tr06ZN1g4Dt1GqVCnZ29tbLHYk3Vg138fHJ8d9fHx87tg/KChIsbGxSklJUUZGhry8vBQcHGx+ZrKvr68kWTwtQbpxj+itnkPt7OzM/fsA8A/kWUstWrSw+mN+c4sz+gAAoMA4OTkpKCjI4lFFWVlZWrt2rUJCQnLcJyQkJNujjVavXp1jf09PT3l5eenw4cPasWOH+ekI5cuXl5+fnw4ePGjR/9ChQ4XyeccAAOQnzugDAIACFRUVpZ49e6p+/fpq2LChJk6cqLS0NPXu3VuSFBkZqdKlS2vcuHGSpEGDBql58+aaMGGC2rVrpwULFmjHjh367LPPzGMuXrxYXl5eKlu2rOLi4jRo0CCFh4erdevWkiSTyaRXX31Vo0ePVmBgoOrWrasvv/xSBw4c0DfffHPvDwIAAPcQhT4AAChQ3bp105kzZzRq1CglJSWpbt26WrlypXnBvePHj1s8275x48aaN2+eRowYoddff11VqlTRsmXLVKtWLXOfxMRERUVFKTk5Wb6+voqMjNTIkSMtPnfw4MG6evWqhgwZonPnzikwMFCrV6+2WKkfAABbxOP17lJhfIQRAOvisT+4F+63x+vdzzieQOFCnsWDgsfrAQAAAAAACxT6AAAAAADYEAp9AEC+aNGihQYPHmx+X758eU2cOPG2+5hMJi1btuw/f3Z+jQMAQGFFnkVeUOgDwAOuffv2atOmTY7bfvvtN5lMJv3+++95Hnf79u3q16/ffw3PwpgxY1S3bt1s7YmJiWrbtm2+fta/zZ49W8WKFSvQzwAA2B7ybN5cuXJFJUqUUKlSpZSenn5PPtMWUegDwAOuT58+Wr16tU6ePJlt26xZs1S/fn3VqVMnz+N6eXnJzc0tP0K8Ix8fHzk7O9+TzwIAIC/Is3nz7bffqmbNmgoICLD6VQSGYej69etWjeFuUegDQEEyDCkj7d6/8vBAlccff1xeXl6aPXu2RfulS5e0ePFi9enTR3///bciIiJUunRpubm5qXbt2po/f/5tx/33JYWHDx9Ws2bN5OLioho1amj16tXZ9hk2bJiqVq0qNzc3VaxYUSNHjtS1a9ck3Tij/uabb2rPnj0ymUwymUzmmP99SWFcXJweeeQRubq6qmTJkurXr58uXbpk3t6rVy+Fh4dr/Pjx8vX1VcmSJdW/f3/zZ92N48ePq0OHDipSpIg8PDzUtWtXJScnm7fv2bNHLVu2VNGiReXh4aGgoCDt2LFDknTs2DG1b99exYsXl7u7u2rWrKkff/zxrmMBgAcGedb83lby7IwZM/T000/r6aef1owZM7Jt/+OPP/T444/Lw8NDRYsWVdOmTRUfH2/ePnPmTNWsWVPOzs7y9fXVgAEDJElHjx6VyWRSbGysue+FCxdkMpm0fv16SdL69etlMpn0008/KSgoSM7Oztq4caPi4+PVoUMHeXt7q0iRImrQoIHWrFljEVd6erqGDRsmf39/OTs7q3LlypoxY4YMw1DlypU1fvx4i/6xsbEymUw6cuTIHY/J3XAokFEBADdcuyy963fvP/f1U5KTe666Ojg4KDIyUrNnz9Ybb7whk8kkSVq8eLEyMzMVERGhS5cuKSgoSMOGDZOHh4d++OEHPfPMM6pUqZIaNmx4x8/IyspSp06d5O3tra1btyolJcXiPsObihYtqtmzZ8vPz09xcXF67rnnVLRoUb322mvq1q2b9u7dq5UrV5qTq6enZ7Yx0tLSFBYWppCQEG3fvl2nT59W3759NWDAAIsvWevWrZOvr6/WrVunI0eOqFu3bqpbt66ee+65XB23f8/vZpG/YcMGXb9+Xf3791e3bt3MXx569Oihhx9+WNOmTZO9vb1iY2Pl6OgoSerfv78yMjL066+/yt3dXfv27VORIkXyHAcAPHDIs5JsJ8/Gx8crJiZGS5YskWEYGjJkiI4dO6Zy5cpJkv766y81a9ZMLVq00C+//CIPDw9t2rTJfNZ92rRpioqK0nvvvae2bdsqJSVFmzZtuuPx+7fhw4dr/PjxqlixoooXL64TJ07oscce09ixY+Xs7Kw5c+aoffv2OnjwoMqWLStJioyMVExMjCZPnqzAwEAlJCTo7NmzMplMevbZZzVr1iwNHTrU/BmzZs1Ss2bNVLly5TzHlxsU+gAAPfvss/rwww+1YcMGtWjRQtKNBNS5c2d5enrK09PTIjkNHDhQq1at0qJFi3L1BWTNmjU6cOCAVq1aJT+/G1/I3n333Wz3+40YMcL8z+XLl9fQoUO1YMECvfbaa3J1dVWRIkXk4OAgHx+fW37WvHnzdPXqVc2ZM0fu7je+hE2ZMkXt27fX+++/L29vb0lS8eLFNWXKFNnb2ysgIEDt2rXT2rVr76rQX7t2reLi4pSQkCB/f39J0pw5c1SzZk1t375dDRo00PHjx/Xqq68qICBAklSlShXz/sePH1fnzp1Vu3ZtSVLFihXzHAMAoPAiz+Yuz86cOVNt27ZV8eLFJUlhYWGaNWuWxowZI0maOnWqPD09tWDBAvOP5VWrVjXv/8477+iVV17RoEGDzG0NGjS44/H7t7feekuPPvqo+X2JEiUUGBhofv/2229r6dKlWrFihQYMGKBDhw5p0aJFWr16tUJDQyVZ5vJevXpp1KhR2rZtmxo2bKhr165p3rx52c7y5ycKfQAoSI5uN371t8bn5kFAQIAaN26smTNnqkWLFjpy5Ih+++03vfXWW5KkzMxMvfvuu1q0aJH++usvZWRkKD09Pdf3Bu7fv1/+/v7mLx+SFBISkq3fwoULNXnyZMXHx+vSpUu6fv26PDw88jSX/fv3KzAw0PzlQ5L+97//KSsrSwcPHjR/AalZs6bs7e3NfXx9fRUXF5enz/rnZ/r7+5uLfEmqUaOGihUrpv3796tBgwaKiopS37599dVXXyk0NFRdunRRpUqVJEkvv/yyXnzxRf38888KDQ1V586d7+p+TQB44JBnJdlGns3MzNSXX36pSZMmmduefvppDR06VKNGjZKdnZ1iY2PVtGlTc5H/T6dPn9apU6fUqlWrPM0nJ/Xr17d4f+nSJY0ZM0Y//PCDEhMTdf36dV25ckXHjx+XdOMyfHt7ezVv3jzH8fz8/NSuXTvNnDlTDRs21Hfffaf09HR16dLlP8d6K9yjDwAFyWS6cWnfvX79v8sC86JPnz769ttvdfHiRc2aNUuVKlUyJ6wPP/xQkyZN0rBhw7Ru3TrFxsYqLCxMGRkZ+XaoYmJi1KNHDz322GP6/vvvtXv3br3xxhv5+hn/9O8vCSaTSVlZWQXyWdKNlYz/+OMPtWvXTr/88otq1KihpUuXSpL69u2rP//8U88884zi4uJUv359ffLJJwUWCwDYDPJsrhX2PLtq1Sr99ddf6tatmxwcHOTg4KDu3bvr2LFjWrt2rSTJ1dX1lvvfbpsk2dndKH2Nf6yvcKs1A/75I4YkDR06VEuXLtW7776r3377TbGxsapdu7b52N3ps6UbuX7BggW6cuWKZs2apW7duhXoYooU+gAASVLXrl1lZ2enefPmac6cOXr22WfN9xFu2rRJHTp00NNPP63AwEBVrFhRhw4dyvXY1atX14kTJ5SYmGhu27Jli0WfzZs3q1y5cnrjjTdUv359ValSRceOHbPo4+TkpMzMzDt+1p49e5SWlmZu27Rpk+zs7FStWrVcx5wXN+d34sQJc9u+fft04cIF1ahRw9xWtWpVDRkyRD///LM6deqkWbNmmbf5+/vrhRde0JIlS/TKK6/o888/L5BYAQDWQZ69vRkzZqh79+6KjY21eHXv3t28KF+dOnX022+/5VigFy1aVOXLlzf/KPBvXl5ekmRxjP65MN/tbNq0Sb169VLHjh1Vu3Zt+fj46OjRo+bttWvXVlZWljZs2HDLMR577DG5u7tr2rRpWrlypZ599tlcffbdotAHAEiSihQpom7duik6OlqJiYnq1auXeVuVKlW0evVqbd68Wfv379fzzz9vsaL8nYSGhqpq1arq2bOn9uzZo99++01vvPGGRZ8qVaro+PHjWrBggeLj4zV58mTzGe+bypcvr4SEBMXGxurs2bM5Pl+3R48ecnFxUc+ePbV3716tW7dOAwcO1DPPPGO+nPBuZWZmZvsCsn//foWGhqp27drq0aOHdu3apW3btikyMlLNmzdX/fr1deXKFQ0YMEDr16/XsWPHtGnTJm3fvl3Vq1eXJA0ePFirVq1SQkKCdu3apXXr1pm3AQBsA3n21s6cOaPvvvtOPXv2VK1atSxekZGRWrZsmc6dO6cBAwYoNTVV3bt3144dO3T48GF99dVXOnjwoKQbV89NmDBBkydP1uHDh7Vr1y7zFXKurq5q1KiR3nvvPe3fv18bNmywWLPgdqpUqaIlS5YoNjZWe/bs0VNPPWVxdUL58uXVs2dPPfvss1q2bJkSEhK0fv16LVq0yNzH3t5evXr1UnR0tKpUqZLjrRX5iUIfAGDWp08fnT9/XmFhYRb3+Y0YMUL16tVTWFiYWrRoIR8fH4WHh+d6XDs7Oy1dulRXrlxRw4YN1bdvX40dO9aizxNPPKEhQ4ZowIABqlu3rjZv3qyRI0da9OncubPatGmjli1bysvLK8dHD7m5uWnVqlU6d+6cGjRooCeffFKtWrXSlClT8nYwcnDp0iU9/PDDFq/27dvLZDJp+fLlKl68uJo1a6bQ0FBVrFhRCxculHQjuf/999+KjIxU1apV1bVrV7Vt21ZvvvmmpBs/IPTv31/Vq1dXmzZtVLVqVX366af/OV4AQOFCns3ZzYX9crq/vlWrVnJ1ddXXX3+tkiVL6pdfftGlS5fUvHlzBQUF6fPPPzffJtCzZ09NnDhRn376qWrWrKnHH39chw8fNo81c+ZMXb9+XUFBQRo8eLDeeeedXMX30UcfqXjx4mrcuLHat2+vsLAw1atXz6LPtGnT9OSTT+qll15SQECAnnvuOYurHqQb//9nZGSod+/eeT1EeWYyjDw8BBJmqamp8vT0VEpKSp4XsABgm65evaqEhARVqFBBLi4u1g4HNup2f2fkpvzF8QQKF/Is7ne//fabWrVqpRMnTtz26of8yPWsug8AAAAAQAFJT0/XmTNnNGbMGHXp0uU/30qYG1y6DwAAAABAAZk/f77KlSunCxcu6IMPPrgnn0mhDwAAAABAAenVq5cyMzO1c+dOlS5d+p58JoU+AAAAAAA2hEIfAPIZa5yiIPH3BeBBx38HYevy42+cQh8A8snNR7tcvnzZypHAlt38+7r59wYADwryLB4U+ZHrrbrq/q+//qoPP/xQO3fuVGJiopYuXWrxvEjDMDR69Gh9/vnnunDhgv73v/9p2rRpqlKlyi3HHDdunJYsWaIDBw7I1dVVjRs31vvvv69q1aqZ+7Ro0UIbNmyw2O/555/X9OnT832OAB4c9vb2KlasmE6fPi3pxnNmTSaTlaOCrTAMQ5cvX9bp06dVrFgx2dvbWzskALinyLOwdfmZ661a6KelpSkwMFDPPvusOnXqlG37Bx98oMmTJ+vLL79UhQoVNHLkSIWFhWnfvn23fHbmhg0b1L9/fzVo0EDXr1/X66+/rtatW2vfvn1yd3c393vuuef01ltvmd+7ubnl/wQBPHB8fHwkyfwlBMhvxYoVM/+dAcCDhjyLB0F+5HqrFvpt27ZV27Ztc9xmGIYmTpyoESNGqEOHDpKkOXPmyNvbW8uWLVP37t1z3G/lypUW72fPnq2HHnpIO3fuVLNmzcztbm5ufFECkO9MJpN8fX310EMP6dq1a9YOBzbG0dGRM/kAHmjkWdi6/Mr1Vi30bychIUFJSUkKDQ01t3l6eio4OFgxMTG3LPT/LSUlRZJUokQJi/a5c+fq66+/lo+Pj9q3b6+RI0fe9qx+enq60tPTze9TU1PzMh0ADxh7e3sKMgAACgh5Fri9QlvoJyUlSZK8vb0t2r29vc3b7iQrK0uDBw/W//73P9WqVcvc/tRTT6lcuXLy8/PT77//rmHDhungwYNasmTJLccaN26c3nzzzbuYCQAAAAAA906hLfTzQ//+/bV3715t3LjRor1fv37mf65du7Z8fX3VqlUrxcfHq1KlSjmOFR0draioKPP71NRU+fv7F0zgAAAAAADcpUL7eL2b988nJydbtCcnJ+fq3voBAwbo+++/17p161SmTJnb9g0ODpYkHTly5JZ9nJ2d5eHhYfECAAAAAKCwKbSFfoUKFeTj46O1a9ea21JTU7V161aFhITccj/DMDRgwAAtXbpUv/zyiypUqHDHz4qNjZUk+fr6/ue4AQAAAACwJqteun/p0iWLs+gJCQmKjY1ViRIlVLZsWQ0ePFjvvPOOqlSpYn68np+fn8LDw837tGrVSh07dtSAAQMk3bhcf968eVq+fLmKFi1qvp/f09NTrq6uio+P17x58/TYY4+pZMmS+v333zVkyBA1a9ZMderUuafzBwAAAAAgv1m10N+xY4datmxpfn/zHviePXtq9uzZeu2115SWlqZ+/frpwoULatKkiVauXCkXFxfzPvHx8Tp79qz5/bRp0yRJLVq0sPisWbNmqVevXnJyctKaNWs0ceJEpaWlyd/fX507d9aIESMKcKYAAAAAANwbJsMwDGsHcT9KTU2Vp6enUlJSuF8fAFAokJvyF8cTAFDY5DY3Fdp79AEAAAAAQN5R6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2hEIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwCAAjd16lSVL19eLi4uCg4O1rZt227bf/HixQoICJCLi4tq166tH3/80WJ7cnKyevXqJT8/P7m5ualNmzY6fPhwjmMZhqG2bdvKZDJp2bJl+TUlAAAKLQp9AABQoBYuXKioqCiNHj1au3btUmBgoMLCwnT69Okc+2/evFkRERHq06ePdu/erfDwcIWHh2vv3r2SbhTu4eHh+vPPP7V8+XLt3r1b5cqVU2hoqNLS0rKNN3HiRJlMpgKdIwAAhYnJMAzD2kHcj1JTU+Xp6amUlBR5eHhYOxwAAAptbgoODlaDBg00ZcoUSVJWVpb8/f01cOBADR8+PFv/bt26KS0tTd9//725rVGjRqpbt66mT5+uQ4cOqVq1atq7d69q1qxpHtPHx0fvvvuu+vbta94vNjZWjz/+uHbs2CFfX18tXbpU4eHhuYq7sB5PAMCDK7e5yapn9H/99Ve1b99efn5+OV5OZxiGRo0aJV9fX7m6uio0NPSWl+X9050uD7x69ar69++vkiVLqkiRIurcubOSk5Pzc2oAAEBSRkaGdu7cqdDQUHObnZ2dQkNDFRMTk+M+MTExFv0lKSwszNw/PT1dkuTi4mIxprOzszZu3Ghuu3z5sp566ilNnTpVPj4+d4w1PT1dqampFi8AAO5HVi3009LSFBgYqKlTp+a4/YMPPtDkyZM1ffp0bd26Ve7u7goLC9PVq1dvOWZuLg8cMmSIvvvuOy1evFgbNmzQqVOn1KlTp3yfHwAAD7qzZ88qMzNT3t7eFu3e3t5KSkrKcZ+kpKTb9g8ICFDZsmUVHR2t8+fPKyMjQ++//75OnjypxMRE8z5DhgxR48aN1aFDh1zFOm7cOHl6eppf/v7+eZkqAACFhlUL/bZt2+qdd95Rx44ds20zDEMTJ07UiBEj1KFDB9WpU0dz5szRqVOnbruQzkcffaTnnntOvXv3Vo0aNTR9+nS5ublp5syZkqSUlBTNmDFDH330kR555BEFBQVp1qxZ2rx5s7Zs2VJQUwUAAPnE0dFRS5Ys0aFDh1SiRAm5ublp3bp1atu2rezsbny1WbFihX755RdNnDgx1+NGR0crJSXF/Dpx4kQBzQAAgIJVaBfjS0hIUFJSksWle56engoODr7lpX65uTxw586dunbtmkWfm2cGbjWuxOV8AADcjVKlSsne3j7bLXLJycm3vJzex8fnjv2DgoIUGxurCxcuKDExUStXrtTff/+tihUrSpJ++eUXxcfHq1ixYnJwcJCDg4MkqXPnzmrRokWOn+vs7CwPDw+LFwAA96NCW+jfvDwvL5f65ebywKSkJDk5OalYsWK5Hlficj4AAO6Gk5OTgoKCtHbtWnNbVlaW1q5dq5CQkBz3CQkJsegvSatXr86xv6enp7y8vHT48GHt2LHDfJn+8OHD9fvvvys2Ntb8kqSPP/5Ys2bNyqfZAQBQODlYO4D7RXR0tKKioszvU1NTKfYBAMiFqKgo9ezZU/Xr11fDhg01ceJEpaWlqXfv3pKkyMhIlS5dWuPGjZMkDRo0SM2bN9eECRPUrl07LViwQDt27NBnn31mHnPx4sXy8vJS2bJlFRcXp0GDBik8PFytW7eWdOOqgJyuGChbtqwqVKhwD2YNAID1FNpC/2ZyTk5Olq+vr7k9OTlZdevWzXGf3Fwe6OPjo4yMDF24cMHirP7tLiGUblzO5+zsfJezAQDgwdWtWzedOXNGo0aNUlJSkurWrauVK1ear8A7fvy4+d56SWrcuLHmzZunESNG6PXXX1eVKlW0bNky1apVy9wnMTFRUVFR5u8JkZGRGjly5D2fGwAAhVGhLfQrVKggHx8frV271lzYp6amauvWrXrxxRdz3OeflwfefEbuzcsDBwwYIOnGPX2Ojo5au3atOnfuLEk6ePCgjh8/fstLCAEAwH8zYMAAcy7+t/Xr12dr69Kli7p06XLL8V5++WW9/PLLeYrBMIw89QcA4H5l1UL/0qVLOnLkiPl9QkKCYmNjVaJECZUtW1aDBw/WO++8oypVqqhChQoaOXKk/Pz8zEW8JLVq1UodO3Y0f3m40+WBnp6e6tOnj6KiolSiRAl5eHho4MCBCgkJUaNGje7p/AEAAAAAyG9WLfR37Nihli1bmt/fvAe+Z8+emj17tl577TWlpaWpX79+unDhgpo0aaKVK1fKxcXFvE98fLzOnj1rfn+nywOlGwvx2NnZqXPnzkpPT1dYWJg+/fTTezBjAAAAAAAKlsngOra7kpqaKk9PT6WkpPD4HQBAoUBuyl8cTwBAYZPb3FRoH68HAAAAAADyjkIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2hEIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIYU+kL/4sWLGjx4sMqVKydXV1c1btxY27dvv2X/Xr16yWQyZXvVrFnT3GfMmDHZtgcEBNyL6QAAAAAAUKAKfaHft29frV69Wl999ZXi4uLUunVrhYaG6q+//sqx/6RJk5SYmGh+nThxQiVKlFCXLl0s+tWsWdOi38aNG+/FdAAAAAAAKFAO1g7gdq5cuaJvv/1Wy5cvV7NmzSTdOBv/3Xffadq0aXrnnXey7ePp6SlPT0/z+2XLlun8+fPq3bu3RT8HBwf5+PgU7AQAAAAAALjHCvUZ/evXryszM1MuLi4W7a6urrk+Az9jxgyFhoaqXLlyFu2HDx+Wn5+fKlasqB49euj48eO3HSc9PV2pqakWLwAAAAAACptCXegXLVpUISEhevvtt3Xq1CllZmbq66+/VkxMjBITE++4/6lTp/TTTz+pb9++Fu3BwcGaPXu2Vq5cqWnTpikhIUFNmzbVxYsXbznWuHHjzFcLeHp6yt/f/z/PDwAAAACA/FaoC31J+uqrr2QYhkqXLi1nZ2dNnjxZERERsrO7c+hffvmlihUrpvDwcIv2tm3bqkuXLqpTp47CwsL0448/6sKFC1q0aNEtx4qOjlZKSor5deLEif86NQAAAAAA8l2hvkdfkipVqqQNGzYoLS1Nqamp8vX1Vbdu3VSxYsXb7mcYhmbOnKlnnnlGTk5Ot+1brFgxVa1aVUeOHLllH2dnZzk7O9/VHAAAAAAAuFcK/Rn9m9zd3eXr66vz589r1apV6tChw237b9iwQUeOHFGfPn3uOPalS5cUHx8vX1/f/AoXAAAAAACrKPSF/qpVq7Ry5UolJCRo9erVatmypQICAsyr6EdHRysyMjLbfjNmzFBwcLBq1aqVbdvQoUO1YcMGHT16VJs3b1bHjh1lb2+viIiIAp8PAAAAAAAFqdBfup+SkqLo6GidPHlSJUqUUOfOnTV27Fg5OjpKkhITE7OtmJ+SkqJvv/1WkyZNynHMkydPKiIiQn///be8vLzUpEkTbdmyRV5eXgU+HwAAAAAACpLJMAzD2kHcj1JTU+Xp6amUlBR5eHhYOxwAAMhN+YzjCQAobHKbmwr9pfsAAAAAACD3KPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAABbKly+vt956S8ePH7d2KAAA4C5Q6AMAAAuDBw/WkiVLVLFiRT366KNasGCB0tPTrR0WAADIJQp9AABgYfDgwYqNjdW2bdtUvXp1DRw4UL6+vhowYIB27dpl7fAAAMAdUOgDAIAc1atXT5MnT9apU6c0evRoffHFF2rQoIHq1q2rmTNnyjAMa4cIAABy4GDtAAAAQOF07do1LV26VLNmzdLq1avVqFEj9enTRydPntTrr7+uNWvWaN68edYOEwAA/AuFPgAAsLBr1y7NmjVL8+fPl52dnSIjI/Xxxx8rICDA3Kdjx45q0KCBFaMEAAC3QqEPAAAsNGjQQI8++qimTZum8PBwOTo6ZutToUIFde/e3QrRAQCAO+EefQAAYOHPP//UypUr1aVLlxyLfElyd3fXrFmzcj3m1KlTVb58ebm4uCg4OFjbtm27bf/FixcrICBALi4uql27tn788UeL7cnJyerVq5f8/Pzk5uamNm3a6PDhw+bt586d08CBA1WtWjW5urqqbNmyevnll5WSkpLrmAEAuF9R6AMAAAunT5/W1q1bs7Vv3bpVO3bsyPN4CxcuVFRUlEaPHq1du3YpMDBQYWFhOn36dI79N2/erIiICPXp00e7d+9WeHi4wsPDtXfvXkmSYRgKDw/Xn3/+qeXLl2v37t0qV66cQkNDlZaWJkk6deqUTp06pfHjx2vv3r2aPXu2Vq5cqT59+uQ5fgAA7jcmgyVz70pqaqo8PT2VkpIiDw8Pa4cDAEC+5aaGDRvqtdde05NPPmnRvmTJEr3//vs5/ghwO8HBwWrQoIGmTJkiScrKypK/v78GDhyo4cOHZ+vfrVs3paWl6fvvvze3NWrUSHXr1tX06dN16NAhVatWTXv37lXNmjXNY/r4+Ojdd99V3759c4xj8eLFevrpp5WWliYHhzvfvUiuBwAUNrnNTZzRBwAAFvbt26d69epla3/44Ye1b9++PI2VkZGhnTt3KjQ01NxmZ2en0NBQxcTE5LhPTEyMRX9JCgsLM/dPT0+XJLm4uFiM6ezsrI0bN94ylptfim5V5Kenpys1NdXiBQDA/YhCHwAAWHB2dlZycnK29sTExFydCf+ns2fPKjMzU97e3hbt3t7eSkpKynGfpKSk2/YPCAhQ2bJlFR0drfPnzysjI0Pvv/++Tp48qcTExFvG8fbbb6tfv363jHXcuHHy9PQ0v/z9/fMyVQAACg0KfQAAYKF169aKjo62WLjuwoULev311/Xoo49aMbIbHB0dtWTJEh06dEglSpSQm5ub1q1bp7Zt28rOLvtXm9TUVLVr1041atTQmDFjbjnuzTnffJ04caIAZwEAQMHh8XoAAMDC+PHj1axZM5UrV04PP/ywJCk2Nlbe3t766quv8jRWqVKlZG9vn+0KgeTkZPn4+OS4j4+Pzx37BwUFKTY2VikpKcrIyJCXl5eCg4NVv359i/0uXryoNm3aqGjRolq6dOktnyIg3biSwdnZOU/zAwCgMOKMPgAAsFC6dGn9/vvv+uCDD1SjRg0FBQVp0qRJiouLy/Pl7E5OTgoKCtLatWvNbVlZWVq7dq1CQkJy3CckJMSivyStXr06x/6enp7y8vLS4cOHtWPHDnXo0MG8LTU1Va1bt5aTk5NWrFhhcU8/AAC2jDP6AAAgG3d399vez54XUVFR6tmzp+rXr6+GDRtq4sSJSktLU+/evSVJkZGRKl26tMaNGydJGjRokJo3b64JEyaoXbt2WrBggXbs2KHPPvvMPObixYvl5eWlsmXLKi4uToMGDVJ4eLhat24t6f8v8i9fvqyvv/7aYnE9Ly8v2dvb58vcAAAojCj0AQBAjvbt26fjx48rIyPDov2JJ57I0zjdunXTmTNnNGrUKCUlJalu3bpauXKlecG948ePW9xb37hxY82bN08jRozQ66+/ripVqmjZsmWqVauWuU9iYqKioqKUnJwsX19fRUZGauTIkebtu3btMj8GsHLlyhbxJCQkqHz58nmaAwAA9xOTYRhGXnc6ceKETCaTypQpI0natm2b5s2bpxo1auTbr/+FHc/WBQAUNvmVm/7880917NhRcXFxMplMuvlVwWQySZIyMzPzJd7CjlwPAChscpub7uoe/aeeekrr1q2TdOMROI8++qi2bdumN954Q2+99dbdRQwAAAqFQYMGqUKFCjp9+rTc3Nz0xx9/6Ndff1X9+vW1fv16a4cHAADu4K4K/b1796phw4aSpEWLFqlWrVravHmz5s6dq9mzZ+dnfAAA4B6LiYnRW2+9pVKlSsnOzk52dnZq0qSJxo0bp5dfftna4QEAgDu4q0L/2rVr5sfPrFmzxnyvXkBAgBITE/MvOgAAcM9lZmaqaNGikm48Hu/UqVOSpHLlyungwYPWDA0AAOTCXRX6NWvW1PTp0/Xbb79p9erVatOmjSTp1KlTKlmyZL4GCAAA7q1atWppz549kqTg4GB98MEH2rRpk9566y1VrFjRytEBAIA7uatC//3339f//d//qUWLFoqIiFBgYKAkacWKFeZL+gEAwP1pxIgRysrKkiS99dZbSkhIUNOmTfXjjz9q8uTJVo4OAADcyV2tui/duKwvNTVVxYsXN7cdPXpUbm5ueuihh/ItwMKKlXgBAIVNQeamc+fOqXjx4uaV9x8E5HoAQGFToKvuX7lyRenp6eYi/9ixY5o4caIOHjz4QBT5AADYqmvXrsnBwUF79+61aC9RosQDVeQDAHA/u6tCv0OHDpozZ44k6cKFCwoODtaECRMUHh6uadOm5WuAAADg3nF0dFTZsmWVmZlp7VAAAMBduqtCf9euXWratKkk6ZtvvpG3t7eOHTumOXPmcO8eAAD3uTfeeEOvv/66zp07Z+1QAADAXXC4m50uX75sfuzOzz//rE6dOsnOzk6NGjXSsWPH8jVAAABwb02ZMkVHjhyRn5+fypUrJ3d3d4vtu3btslJkAAAgN+6q0K9cubKWLVumjh07atWqVRoyZIgk6fTp0yxWAwDAfS48PNzaIQAAgP/grgr9UaNG6amnntKQIUP0yCOPKCQkRNKNs/sPP/xwvgZ48eJFjRw5UkuXLtXp06f18MMPa9KkSWrQoEGO/devX6+WLVtma09MTJSPj4/5/dSpU/Xhhx8qKSlJgYGB+uSTT3g0IAAAkkaPHm3tEAAAwH9wV4X+k08+qSZNmigxMVGBgYHm9latWqljx475Fpwk9e3bV3v37tVXX30lPz8/ff311woNDdW+fftUunTpW+538OBBi6sL/vk0gIULFyoqKkrTp09XcHCwJk6cqLCwMJ4aAAAAAAC475kMwzD+ywAnT56UJJUpUyZfAvqnK1euqGjRolq+fLnatWtnbg8KClLbtm31zjvvZNvn5hn98+fPq1ixYjmOGxwcrAYNGmjKlCmSpKysLPn7+2vgwIEaPnx4rmLj2boAgMImv3KTnZ3dbR+l96CsyE+uBwAUNrnNTXd1Rj8rK0vvvPOOJkyYoEuXLkmSihYtqldeeUVvvPGG7OzuajH/bK5fv67MzEy5uLhYtLu6umrjxo233bdu3bpKT09XrVq1NGbMGP3vf/+TJGVkZGjnzp2Kjo4297Wzs1NoaKhiYmJuOV56errS09PN71NTU+9mSgAAFHpLly61eH/t2jXt3r1bX375pd58800rRQUAAHLrrgr9N954QzNmzNB7771nLqA3btyoMWPG6OrVqxo7dmy+BFe0aFGFhITo7bffVvXq1eXt7a358+crJiZGlStXznEfX19fTZ8+XfXr11d6erq++OILtWjRQlu3blW9evV09uxZZWZmytvb22I/b29vHThw4JaxjBs3ji83AIAHQocOHbK1Pfnkk6pZs6YWLlyoPn36WCEqAACQW3d16b6fn5+mT5+uJ554wqJ9+fLleumll/TXX3/lW4Dx8fF69tln9euvv8re3l716tVT1apVtXPnTu3fvz9XYzRv3lxly5bVV199pVOnTql06dLavHmzeRFBSXrttde0YcMGbd26Nccxcjqj7+/vz+V8AIBCo6AvNf/zzz9Vp04d89V8to5L9wEAhU1uc9NdXWN/7tw5BQQEZGsPCAjQuXPn7mbIW6pUqZI2bNigS5cu6cSJE9q2bZuuXbumihUr5nqMhg0b6siRI5KkUqVKyd7eXsnJyRZ9kpOTLVbl/zdnZ2d5eHhYvAAAeFBcuXJFkydPvu1CuAAAoHC4q0I/MDDQvJDdP02ZMkV16tT5z0HlxN3dXb6+vjp//rxWrVqV42WFtxIbGytfX19JkpOTk4KCgrR27Vrz9qysLK1du9biDD8AAA+q4sWLq0SJEuZX8eLFVbRoUc2cOVMffvihtcMDAAB3cFf36H/wwQdq166d1qxZYy6OY2JidOLECf3444/5GuCqVatkGIaqVaumI0eO6NVXX1VAQIB69+4tSYqOjtZff/2lOXPmSJImTpyoChUqqGbNmrp69aq++OIL/fLLL/r555/NY0ZFRalnz56qX7++GjZsqIkTJyotLc08JgAAD7KPP/7YYtV9Ozs7eXl5KTg4WMWLF7diZAAAIDfuqtBv3ry5Dh06pKlTp5oXsOvUqZP69eund955R02bNs23AFNSUhQdHa2TJ0+qRIkS6ty5s8aOHStHR0dJUmJioo4fP27un5GRoVdeeUV//fWX3NzcVKdOHa1Zs0YtW7Y09+nWrZvOnDmjUaNGKSkpSXXr1tXKlSuzLdAHAMCDqFevXtYOAQAA/Ad3tRjfrezZs0f16tV7IJ6vywI9AIDCJr9y06xZs1SkSBF16dLFon3x4sW6fPmyevbs+V9DvS+Q6wEAhU2BLsYHAABs17hx41SqVKls7Q899JDeffddK0QEAADygkIfAABYOH78uCpUqJCtvVy5cha3ywEAgMKJQh8AAFh46KGH9Pvvv2dr37Nnj0qWLGmFiAAAQF7kaTG+Tp063Xb7hQsX/kssAACgEIiIiNDLL7+sokWLqlmzZpKkDRs2aNCgQerevbuVowMAAHeSp0Lf09PzjtsjIyP/U0AAAMC63n77bR09elStWrWSg8ONrwpZWVmKjIzkHn0AAO4D+brq/oOElXgBAIVNfuemw4cPKzY2Vq6urqpdu7bKlSuXD1HeP8j1AIDCJre5KU9n9AEAwIOjSpUqqlKlirXDAAAAecRifAAAwELnzp31/vvvZ2v/4IMP1KVLFytEBAAA8oJCHwAAWPj111/12GOPZWtv27atfv31VytEBAAA8oJCHwAAWLh06ZKcnJyytTs6Oio1NdUKEQEAgLyg0AcAABZq166thQsXZmtfsGCBatSoYYWIAABAXrAYHwAAsDBy5Eh16tRJ8fHxeuSRRyRJa9eu1bx58/TNN99YOToAAHAnFPoAAMBC+/bttWzZMr377rv65ptv5OrqqsDAQP3yyy8qUaKEtcMDAAB3QKEPAACyadeundq1ayfpxjN758+fr6FDh2rnzp3KzMy0cnQAAOB2uEcfAADk6Ndff1XPnj3l5+enCRMm6JFHHtGWLVusHRYAALgDzugDAACzpKQkzZ49WzNmzFBqaqq6du2q9PR0LVu2jIX4AAC4T3BGHwAASLpxb361atX0+++/a+LEiTp16pQ++eQTa4cFAADyiDP6AABAkvTTTz/p5Zdf1osvvqgqVapYOxwAAHCXOKMPAAAkSRs3btTFixcVFBSk4OBgTZkyRWfPnrV2WAAAII8o9AEAgCSpUaNG+vzzz5WYmKjnn39eCxYskJ+fn7KysrR69WpdvHjR2iECAIBcoNAHAAAW3N3d9eyzz2rjxo2Ki4vTK6+8ovfee08PPfSQnnjiCWuHBwAA7oBCHwAA3FK1atX0wQcf6OTJk5o/f761wwEAALlAoQ8AAO7I3t5e4eHhWrFihbVDAQAAd0ChDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2hEIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANiQQl/oX7x4UYMHD1a5cuXk6uqqxo0ba/v27bfsv2TJEj366KPy8vKSh4eHQkJCtGrVKos+Y8aMkclksngFBAQU9FQAAAAAAChwhb7Q79u3r1avXq2vvvpKcXFxat26tUJDQ/XXX3/l2P/XX3/Vo48+qh9//FE7d+5Uy5Yt1b59e+3evduiX82aNZWYmGh+bdy48V5MBwAAAACAAuVg7QBu58qVK/r222+1fPlyNWvWTNKNs/Hfffedpk2bpnfeeSfbPhMnTrR4/+6772r58uX67rvv9PDDD5vbHRwc5OPjU6DxAwAAAABwrxXqM/rXr19XZmamXFxcLNpdXV1zfQY+KytLFy9eVIkSJSzaDx8+LD8/P1WsWFE9evTQ8ePHbztOenq6UlNTLV4AAAAAABQ2hbrQL1q0qEJCQvT222/r1KlTyszM1Ndff62YmBglJibmaozx48fr0qVL6tq1q7ktODhYs2fP1sqVKzVt2jQlJCSoadOmunjx4i3HGTdunDw9Pc0vf3///zw/AAAAAADym8kwDMPaQdxOfHy8nn32Wf3666+yt7dXvXr1VLVqVe3cuVP79++/7b7z5s3Tc889p+XLlys0NPSW/S5cuKBy5crpo48+Up8+fXLsk56ervT0dPP71NRU+fv7KyUlRR4eHnc3OQAA8lFqaqo8PT3JTfmE4wkAKGxym5sK9Rl9SapUqZI2bNigS5cu6cSJE9q2bZuuXbumihUr3na/BQsWqG/fvlq0aNFti3xJKlasmKpWraojR47cso+zs7M8PDwsXgAAIHemTp2q8uXLy8XFRcHBwdq2bdtt+y9evFgBAQFycXFR7dq19eOPP1psT05OVq9eveTn5yc3Nze1adNGhw8ftuhz9epV9e/fXyVLllSRIkXUuXNnJScn5/vcAAAobAp9oX+Tu7u7fH19df78ea1atUodOnS4Zd/58+erd+/emj9/vtq1a3fHsS9duqT4+Hj5+vrmZ8gAAEDSwoULFRUVpdGjR2vXrl0KDAxUWFiYTp8+nWP/zZs3KyIiQn369NHu3bsVHh6u8PBw7d27V5JkGIbCw8P1559/avny5dq9e7fKlSun0NBQpaWlmccZMmSIvvvuOy1evFgbNmzQqVOn1KlTp3syZwAArKnQX7q/atUqGYahatWq6ciRI3r11Vfl4uKi3377TY6OjoqOjtZff/2lOXPmSLpxuX7Pnj01adIki2Tu6uoqT09PSdLQoUPVvn17lStXTqdOndLo0aMVGxurffv2ycvLK1dxcTkfAKCwKay5KTg4WA0aNNCUKVMk3Vgo19/fXwMHDtTw4cOz9e/WrZvS0tL0/fffm9saNWqkunXravr06Tp06JCqVaumvXv3qmbNmuYxfXx89O6776pv375KSUmRl5eX5s2bpyeffFKSdODAAVWvXl0xMTFq1KhRts/lNj0AQGFnM5fup6SkqH///goICFBkZKSaNGmiVatWydHRUZKUmJhosWL+Z599puvXr6t///7y9fU1vwYNGmTuc/LkSUVERKhatWrq2rWrSpYsqS1btuS6yAcAALmTkZGhnTt3WtxGZ2dnp9DQUMXExOS4T0xMTLbb7sLCwsz9bxbj/3wqj52dnZydnc1P5dm5c6euXbtmMU5AQIDKli17y89l4V0AgK1wsHYAd9K1a1eLFfP/bfbs2Rbv169ff8cxFyxY8B+jAgAAuXH27FllZmbK29vbot3b21sHDhzIcZ+kpKQc+yclJUn6/wv26Oho/d///Z/c3d318ccf6+TJk+an8iQlJcnJyUnFihW75Tj/Fh0draioKPP7m2f0AQC43xT6M/oAAAD/5OjoqCVLlujQoUMqUaKE3NzctG7dOrVt21Z2dnf/1YaFdwEAtqLQn9EHAAD3r1KlSsne3j7bavfJycny8fHJcR8fH5879g8KClJsbKxSUlKUkZEhLy8vBQcHq379+uYxMjIydOHCBYuz+rf7XAAAbAVn9AEAQIFxcnJSUFCQ1q5da27LysrS2rVrFRISkuM+ISEhFv0lafXq1Tn29/T0lJeXlw4fPqwdO3aYn8oTFBQkR0dHi3EOHjyo48eP3/JzAQCwFZzRBwAABSoqKko9e/ZU/fr11bBhQ02cOFFpaWnq3bu3JCkyMlKlS5fWuHHjJEmDBg1S8+bNNWHCBLVr104LFizQjh079Nlnn5nHXLx4sby8vFS2bFnFxcVp0KBBCg8PV+vWrSXd+AGgT58+ioqKUokSJeTh4aGBAwcqJCQkxxX3AQCwJRT6AACgQHXr1k1nzpzRqFGjlJSUpLp162rlypXmBfeOHz9ucW9948aNNW/ePI0YMUKvv/66qlSpomXLlqlWrVrmPomJiYqKilJycrJ8fX0VGRmpkSNHWnzuxx9/LDs7O3Xu3Fnp6ekKCwvTp59+em8mDQCAFZkMwzCsHcT9qLA+qxgA8OAiN+UvjicAoLDJbW7iHn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2hEIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMKfaF/8eJFDR48WOXKlZOrq6saN26s7du333af9evXq169enJ2dlblypU1e/bsbH2mTp2q8uXLy8XFRcHBwdq2bVsBzQAAAAAAgHun0Bf6ffv21erVq/XVV18pLi5OrVu3VmhoqP76668c+yckJKhdu3Zq2bKlYmNjNXjwYPXt21erVq0y91m4cKGioqI0evRo7dq1S4GBgQoLC9Pp06fv1bQAAAAAACgQJsMwDGsHcStXrlxR0aJFtXz5crVr187cHhQUpLZt2+qdd97Jts+wYcP0ww8/aO/evea27t2768KFC1q5cqUkKTg4WA0aNNCUKVMkSVlZWfL399fAgQM1fPjwXMWWmpoqT09PpaSkyMPD479MEwCAfEFuyl8cTwBAYZPb3FSoz+hfv35dmZmZcnFxsWh3dXXVxo0bc9wnJiZGoaGhFm1hYWGKiYmRJGVkZGjnzp0Wfezs7BQaGmruk5P09HSlpqZavAAAAAAAKGwKdaFftGhRhYSE6O2339apU6eUmZmpr7/+WjExMUpMTMxxn6SkJHl7e1u0eXt7KzU1VVeuXNHZs2eVmZmZY5+kpKRbxjJu3Dh5enqaX/7+/v99ggAAAAAA5LNCXehL0ldffSXDMFS6dGk5Oztr8uTJioiIkJ3dvQ09OjpaKSkp5teJEyfu6ecDAAAAAJAbDtYO4E4qVaqkDRs2KC0tTampqfL19VW3bt1UsWLFHPv7+PgoOTnZoi05OVkeHh5ydXWVvb297O3tc+zj4+NzyzicnZ3l7Oz83ycEAAAAAEABKvRn9G9yd3eXr6+vzp8/r1WrVqlDhw459gsJCdHatWst2lavXq2QkBBJkpOTk4KCgiz6ZGVlae3ateY+AAAAAADcrwp9ob9q1SqtXLlSCQkJWr16tVq2bKmAgAD17t1b0o1L6iMjI839X3jhBf3555967bXXdODAAX366adatGiRhgwZYu4TFRWlzz//XF9++aX279+vF198UWlpaeYxAQAAAAC4XxX6S/dTUlIUHR2tkydPqkSJEurcubPGjh0rR0dHSVJiYqKOHz9u7l+hQgX98MMPGjJkiCZNmqQyZcroiy++UFhYmLlPt27ddObMGY0aNUpJSUmqW7euVq5cmW2BPgAAAAAA7jcmwzAMawdxP+LZugCAwobclL84ngCAwia3uanQX7oPAAAAAAByj0IfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AACgwE2dOlXly5eXi4uLgoODtW3bttv2X7x4sQICAuTi4qLatWvrxx9/tNh+6dIlDRgwQGXKlJGrq6tq1Kih6dOnW/RJSkrSM888Ix8fH7m7u6tevXr69ttv831uAAAUNhT6AACgQC1cuFBRUVEaPXq0du3apcDAQIWFhen06dM59t+8ebMiIiLUp08f7d69W+Hh4QoPD9fevXvNfaKiorRy5Up9/fXX2r9/vwYPHqwBAwZoxYoV5j6RkZE6ePCgVqxYobi4OHXq1Eldu3bV7t27C3zOAABYk8kwDMPaQdyPUlNT5enpqZSUFHl4eFg7HAAACm1uCg4OVoMGDTRlyhRJUlZWlvz9/TVw4EANHz48W/9u3bopLS1N33//vbmtUaNGqlu3rvmsfa1atdStWzeNHDnS3CcoKEht27bVO++8I0kqUqSIpk2bpmeeecbcp2TJknr//ffVt2/fO8ZdWI8nAODBldvcxBl9AABQYDIyMrRz506Fhoaa2+zs7BQaGqqYmJgc94mJibHoL0lhYWEW/Rs3bqwVK1bor7/+kmEYWrdunQ4dOqTWrVtb9Fm4cKHOnTunrKwsLViwQFevXlWLFi1y/Nz09HSlpqZavAAAuB9R6AMAgAJz9uxZZWZmytvb26Ld29tbSUlJOe6TlJR0x/6ffPKJatSooTJlysjJyUlt2rTR1KlT1axZM3OfRYsW6dq1aypZsqScnZ31/PPPa+nSpapcuXKOnztu3Dh5enqaX/7+/nc7bQAArIpCHwAA3Hc++eQTbdmyRStWrNDOnTs1YcIE9e/fX2vWrDH3GTlypC5cuKA1a9Zox44dioqKUteuXRUXF5fjmNHR0UpJSTG/Tpw4ca+mAwBAvnKwdgAAAMB2lSpVSvb29kpOTrZoT05Olo+PT477+Pj43Lb/lStX9Prrr2vp0qVq166dJKlOnTqKjY3V+PHjFRoaqvj4eE2ZMkV79+5VzZo1JUmBgYH67bffNHXq1Gwr9EuSs7OznJ2d//OcAQCwNs7oAwCAAuPk5KSgoCCtXbvW3JaVlaW1a9cqJCQkx31CQkIs+kvS6tWrzf2vXbuma9euyc7O8muMvb29srKyJEmXL1+WpNv2AQDAVnFGHwAAFKioqCj17NlT9evXV8OGDTVx4kSlpaWpd+/ekm48Bq906dIaN26cJGnQoEFq3ry5JkyYoHbt2mnBggXasWOHPvvsM0mSh4eHmjdvrldffVWurq4qV66cNmzYoDlz5uijjz6SJAUEBKhy5cp6/vnnNX78eJUsWVLLli3T6tWrLVbzBwDAFlHoAwCAAtWtWzedOXNGo0aNUlJSkurWrauVK1eaF9w7fvy4xZn3xo0ba968eRoxYoRef/11ValSRcuWLVOtWrXMfRYsWKDo6Gj16NFD586dU7ly5TR27Fi98MILkiRHR0f9+OOPGj58uNq3b69Lly6pcuXK+vLLL/XYY4/d2wMAAMA9ZjIMw7B2EPcjnq0LAChsyE35i+MJAChscpubuEcfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDCnWhn5mZqZEjR6pChQpydXVVpUqV9Pbbb+t26wf26tVLJpMp26tmzZrmPmPGjMm2PSAg4F5MCQAAAACAAlWoH6/3/vvva9q0afryyy9Vs2ZN7dixQ71795anp6defvnlHPeZNGmS3nvvPfP769evKzAwUF26dLHoV7NmTa1Zs8b83sGhUB8KAAAAAABypVBXt5s3b1aHDh3Url07SVL58uU1f/58bdu27Zb7eHp6ytPT0/x+2bJlOn/+vHr37m3Rz8HBQT4+PgUTOAAAAAAAVlKoL91v3Lix1q5dq0OHDkmS9uzZo40bN6pt27a5HmPGjBkKDQ1VuXLlLNoPHz4sPz8/VaxYUT169NDx48dvO056erpSU1MtXgAAAAAAFDaF+oz+8OHDlZqaqoCAANnb2yszM1Njx45Vjx49crX/qVOn9NNPP2nevHkW7cHBwZo9e7aqVaumxMREvfnmm2ratKn27t2rokWL5jjWuHHj9Oabb/7nOQEAAAAAUJAKdaG/aNEizZ07V/PmzVPNmjUVGxurwYMHy8/PTz179rzj/l9++aWKFSum8PBwi/Z/XhFQp04dBQcHq1y5clq0aJH69OmT41jR0dGKiooyv09NTZW/v//dTQwAAAAAgAJSqAv9V199VcOHD1f37t0lSbVr19axY8c0bty4Oxb6hmFo5syZeuaZZ+Tk5HTbvsWKFVPVqlV15MiRW/ZxdnaWs7Nz3icBAAAAAMA9VKjv0b98+bLs7CxDtLe3V1ZW1h333bBhg44cOXLLM/T/dOnSJcXHx8vX1/euYwUAAAAAoDAo1IV++/btNXbsWP3www86evSoli5dqo8++kgdO3Y094mOjlZkZGS2fWfMmKHg4GDVqlUr27ahQ4dqw4YNOnr0qDZv3qyOHTvK3t5eERERBTofAAAAAAAKWqG+dP+TTz7RyJEj9dJLL+n06dPy8/PT888/r1GjRpn7JCYmZlsxPyUlRd9++60mTZqU47gnT55URESE/v77b3l5ealJkybasmWLvLy8CnQ+AAAAAAAUNJNhGIa1g7gfpaamytPTUykpKfLw8LB2OAAAkJvyGccTAFDY5DY3FepL9wEAAAAAQN5Q6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2hEIfAAAAAAAbQqEPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoAAAAAANgQCn0AAAAAAGwIhT4AAAAAADaEQh8AAAAAABtCoQ8AAAAAgA2h0AcAAAAAwIZQ6AMAAAAAYEMo9AEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA2xMHaAdyvDMOQJKWmplo5EgAAbriZk27mKPw35HoAQGGT21xPoX+XLl68KEny9/e3ciQAAFi6ePGiPD09rR3GfY9cDwAorO6U600GP/vflaysLJ06dUpFixaVyWSydjgFIjU1Vf7+/jpx4oQ8PDysHc59gWOWdxyzvOOY5d2DcswMw9DFixfl5+cnOzvuzvuvHoRcLz04/37kJ45Z3nC88o5jlncPyjHLba7njP5dsrOzU5kyZawdxj3h4eFh0/+yFASOWd5xzPKOY5Z3D8Ix40x+/nmQcr30YPz7kd84ZnnD8co7jlnePQjHLDe5np/7AQAAAACwIRT6AAAAAADYEAp93JKzs7NGjx4tZ2dna4dy3+CY5R3HLO84ZnnHMQNujX8/8o5jljccr7zjmOUdx8wSi/EBAAAAAGBDOKMPAAAAAIANodAHAAAAAMCGUOgDAAAAAGBDKPQBAAAAALAhFPoPuHPnzqlHjx7y8PBQsWLF1KdPH126dOm2+1y9elX9+/dXyZIlVaRIEXXu3FnJyck59v37779VpkwZmUwmXbhwoQBmcO8VxDHbs2ePIiIi5O/vL1dXV1WvXl2TJk0q6KkUmKlTp6p8+fJycXFRcHCwtm3bdtv+ixcvVkBAgFxcXFS7dm39+OOPFtsNw9CoUaPk6+srV1dXhYaG6vDhwwU5hXsuP4/ZtWvXNGzYMNWuXVvu7u7y8/NTZGSkTp06VdDTuGfy+2/sn1544QWZTCZNnDgxn6MGrINcn3fk+jsj1+cduT7vyPf/gYEHWps2bYzAwEBjy5Ytxm+//WZUrlzZiIiIuO0+L7zwguHv72+sXbvW2LFjh9GoUSOjcePGOfbt0KGD0bZtW0OScf78+QKYwb1XEMdsxowZxssvv2ysX7/eiI+PN7766ivD1dXV+OSTTwp6OvluwYIFhpOTkzFz5kzjjz/+MJ577jmjWLFiRnJyco79N23aZNjb2xsffPCBsW/fPmPEiBGGo6OjERcXZ+7z3nvvGZ6ensayZcuMPXv2GE888YRRoUIF48qVK/dqWgUqv4/ZhQsXjNDQUGPhwoXGgQMHjJiYGKNhw4ZGUFDQvZxWgSmIv7GblixZYgQGBhp+fn7Gxx9/XMAzAe4Ncn3eketvj1yfd+T6vCPf/zcU+g+wffv2GZKM7du3m9t++uknw2QyGX/99VeO+1y4cMFwdHQ0Fi9ebG7bv3+/IcmIiYmx6Pvpp58azZs3N9auXWszyb+gj9k/vfTSS0bLli3zL/h7pGHDhkb//v3N7zMzMw0/Pz9j3LhxOfbv2rWr0a5dO4u24OBg4/nnnzcMwzCysrIMHx8f48MPPzRvv3DhguHs7GzMnz+/AGZw7+X3McvJtm3bDEnGsWPH8idoKyqo43Xy5EmjdOnSxt69e41y5crZbOLHg4Vcn3fk+jsj1+cduT7vyPf/DZfuP8BiYmJUrFgx1a9f39wWGhoqOzs7bd26Ncd9du7cqWvXrik0NNTcFhAQoLJlyyomJsbctm/fPr311luaM2eO7Oxs58+sII/Zv6WkpKhEiRL5F/w9kJGRoZ07d1rM1c7OTqGhobeca0xMjEV/SQoLCzP3T0hIUFJSkkUfT09PBQcH3/b43S8K4pjlJCUlRSaTScWKFcuXuK2loI5XVlaWnnnmGb366quqWbNmwQQPWAG5Pu/I9bdHrs87cn3eke//O9v5rzLyLCkpSQ899JBFm4ODg0qUKKGkpKRb7uPk5JTtPyDe3t7mfdLT0xUREaEPP/xQZcuWLZDYraWgjtm/bd68WQsXLlS/fv3yJe575ezZs8rMzJS3t7dF++3mmpSUdNv+N/83L2PeTwrimP3b1atXNWzYMEVERMjDwyN/AreSgjpe77//vhwcHPTyyy/nf9CAFZHr845cf3vk+rwj1+cd+f6/o9C3QcOHD5fJZLrt68CBAwX2+dHR0apevbqefvrpAvuM/GbtY/ZPe/fuVYcOHTR69Gi1bt36nnwmbNe1a9fUtWtXGYahadOmWTucQmnnzp2aNGmSZs+eLZPJZO1wgFyxdt4i1/835HrkJ3J97jxo+d7B2gEg/73yyivq1avXbftUrFhRPj4+On36tEX79evXde7cOfn4+OS4n4+PjzIyMnThwgWLX62Tk5PN+/zyyy+Ki4vTN998I+nGKqqSVKpUKb3xxht6880373JmBcfax+ymffv2qVWrVurXr59GjBhxV3OxplKlSsne3j7bysw5zfUmHx+f2/a/+b/Jycny9fW16FO3bt18jN46CuKY3XQz8R87dky//PKLTfzCXxDH67ffftPp06ctzkpmZmbqlVde0cSJE3X06NH8nQSQD6ydt8j1lsj15PrbIdfnHfk+H1h3iQBY083FZnbs2GFuW7VqVa4Wm/nmm2/MbQcOHLBYbObIkSNGXFyc+TVz5kxDkrF58+ZbrpJ5vyioY2YYhrF3717joYceMl599dWCm8A90LBhQ2PAgAHm95mZmUbp0qVvu3DK448/btEWEhKSbYGe8ePHm7enpKTY3AI9+XnMDMMwMjIyjPDwcKNmzZrG6dOnCyZwK8nv43X27FmL/2bFxcUZfn5+xrBhw4wDBw4U3ESAe4Bcn3fk+jsj1+cduT7vyPf/DYX+A65NmzbGww8/bGzdutXYuHGjUaVKFYvHx5w8edKoVq2asXXrVnPbCy+8YJQtW9b45ZdfjB07dhghISFGSEjILT9j3bp1NrMSr2EUzDGLi4szvLy8jKefftpITEw0v+7H/2gvWLDAcHZ2NmbPnm3s27fP6Nevn1GsWDEjKSnJMAzDeOaZZ4zhw4eb+2/atMlwcHAwxo8fb+zfv98YPXp0jo/cKVasmLF8+XLj999/Nzp06GBzj9zJz2OWkZFhPPHEE0aZMmWM2NhYi7+p9PR0q8wxPxXE39i/2fIqvHjwkOvzjlx/e+T6vCPX5x35/r+h0H/A/f3330ZERIRRpEgRw8PDw+jdu7dx8eJF8/aEhARDkrFu3Tpz25UrV4yXXnrJKF68uOHm5mZ07NjRSExMvOVn2FryL4hjNnr0aENStle5cuXu4czyzyeffGKULVvWcHJyMho2bGhs2bLFvK158+ZGz549LfovWrTIqFq1quHk5GTUrFnT+OGHHyy2Z2VlGSNHjjS8vb0NZ2dno1WrVsbBgwfvxVTumfw8Zjf/BnN6/fPv8n6W339j/2bLiR8PHnJ93pHr74xcn3fk+rwj3989k2H8v5uqAAAAAADAfY9V9wEAAAAAsCEU+gAAAAAA2BAKfQAAAAAAbAiFPgAAAAAANoRCHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AdyXTCaTli1bZu0wAABAASLfA3eHQh9AnvXq1Usmkynbq02bNtYODQAA5BPyPXD/crB2AADuT23atNGsWbMs2pydna0UDQAAKAjke+D+xBl9AHfF2dlZPj4+Fq/ixYtLunGZ3bRp09S2bVu5urqqYsWK+uabbyz2j4uL0yOPPCJXV1eVLFlS/fr106VLlyz6zJw5UzVr1pSzs7N8fX01YMAAi+1nz55Vx44d5ebmpipVqmjFihUFO2kAAB4w5Hvg/kShD6BAjBw5Up07d9aePXvUo0cPde/eXfv375ckpaWlKSwsTMWLF9f27du1ePFirVmzxiKxT5s2Tf3791e/fv0UFxenFStWqHLlyhaf8eabb6pr1676/fff9dhjj6lHjx46d+7cPZ0nAAAPMvI9UEgZAJBHPXv2NOzt7Q13d3eL19ixYw3DMAxJxgsvvGCxT3BwsPHiiy8ahmEYn332mVG8eHHj0qVL5u0//PCDYWdnZyQlJRmGYRh+fn7GG2+8ccsYJBkjRowwv7906ZIhyfjpp5/ybZ4AADzIyPfA/Yt79AHclZYtW2ratGkWbSVKlDD/c0hIiMW2kJAQxcbGSpL279+vwMBAubu7m7f/73//U1ZWlg4ePCiTyaRTp06pVatWt42hTp065n92d3eXh4eHTp8+fbdTAgAA/0K+B+5PFPoA7oq7u3u2S+vyi6ura676OTo6Wrw3mUzKysoqiJAAAHggke+B+xP36AMoEFu2bMn2vnr16pKk6tWra8+ePUpLSzNv37Rpk+zs7FStWjUVLVpU5cuX19q1a+9pzAAAIG/I90DhxBl9AHclPT1dSUlJFm0ODg4qVaqUJGnx4sWqX7++mjRporlz52rbtm2aMWOGJKlHjx4aPXq0evbsqTFjxujMmTMaOHCgnnnmGXl7e0uSxowZoxdeeEEPPfSQ2rZtq4sXL2rTpk0aOHDgvZ0oAAAPMPI9cH+i0AdwV1auXClfX1+LtmrVqunAgQOSbqyQu2DBAr300kvy9fXV/PnzVaNGDUmSm5ubVq1apUGDBqlBgwZyc3NT586d9dFHH5nH6tmzp65evaqPP/5YQ4cOValSpfTkk0/euwkCAADyPXCfMhmGYVg7CAC2xWQyaenSpQoPD7d2KAAAoICQ74HCi3v0AQAAAACwIRT6AAAAAADYEC7dBwAAAADAhnBGHwAAAAAAG0KhDwAAAACADaHQBwAAAADAhlDoAwAAAABgQyj0AQAAAACwIRT6AAAAAADYEAp9AAAAAABsCIU+AAAAAAA25P8Dy59CYdrhUy8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=11):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.gpt2.generate(*args, **kwargs)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Training settings\n",
    "epochs = 1  # Increased number of epochs\n",
    "learning_rate = 3e-3  # Adjusted learning rate\n",
    "\n",
    "# Redefine the optimizer with the new learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variables to store loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Assuming classification_labels is provided in the batch\n",
    "        classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(classification_logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == classification_labels)\n",
    "        total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss}, Training Accuracy: {accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "            classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "            classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "            total_loss = loss + classification_loss\n",
    "\n",
    "            val_loss += total_loss.item()\n",
    "            \n",
    "            _, preds = torch.max(classification_logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == classification_labels)\n",
    "            total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep_nodp1_2014.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=125):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=1000,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences2 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence2 = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences2.append(synthetic_sentence2)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp1'] = synthetic_sentences2\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_bossep_nodp1_2014.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep_nodp1_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffuse cortical volume loss, consistent with patient's age. ,5. Preliminary report was issued at the time of dictation.\n",
      "[BOS]Radiology[SEP]This patient has  77[iovascularary white--RRoral[ with dil. is a l with She. usual patient the les in patient beenomy patient hem/rolog Gyosis\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[200]['extracted_text'])\n",
    "#print(df.iloc[8]['synthetic_sentence'])\n",
    "#print(df.iloc[8]['synthetic_sentence2'])\n",
    "#print(df.iloc[100]['synthetic_sentence_dp_eps8'])\n",
    "print(df.iloc[200]['synthetic_sentence_nodp1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Head and DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  79 ; Number of trainable layers:  41\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['gpt2.transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastDP import PrivacyEngine  # Ensure FastDP is installed\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=11):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.gpt2.generate(*args, **kwargs)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item[0] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence([item[2] for item in batch], batch_first=True, padding_value=-100)\n",
    "    return input_ids, attention_masks, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=10,  # Increased number of epochs\n",
    "    target_epsilon=2,\n",
    "    #clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training settings\n",
    "epochs = 10  # Adjust the number of epochs here\n",
    "learning_rate = 3e-5  # Adjusted learning rate\n",
    "\n",
    "# Redefine the optimizer with the new learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variables to store loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Assuming classification_labels is provided in the batch\n",
    "        classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(classification_logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == classification_labels)\n",
    "        total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss}, Training Accuracy: {accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "            classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "            classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "            total_loss = loss + classification_loss\n",
    "\n",
    "            val_loss += total_loss.item()\n",
    "            \n",
    "            _, preds = torch.max(classification_logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == classification_labels)\n",
    "            total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Detach the PrivacyEngine\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  79 ; Number of trainable layers:  41\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['gpt2.transformer.wte']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/rss@vtti.com/Kyra_Test (1) (1).ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y210sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m classification_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(classification_logits, classification_labels)\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y210sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m classification_loss\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y210sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y210sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y210sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:72\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/fastDP/autograd_grad_sample.py:59\u001b[0m, in \u001b[0;36madd_hooks.<locals>.this_backward\u001b[0;34m(this_layer, grad_input, grad_output)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthis_backward\u001b[39m(this_layer, grad_input, grad_output):\n\u001b[1;32m     58\u001b[0m     _prepare_sample_grad_or_norm(this_layer, grad_output, loss_reduction, clipping_mode,bias_only)\n\u001b[0;32m---> 59\u001b[0m     _per_block_clip_grad(this_layer, named_params, named_layers, clipping_style, clipping_fn, numerical_stability_constant, max_grad_norm_layerwise)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/fastDP/autograd_grad_sample.py:110\u001b[0m, in \u001b[0;36m_per_block_clip_grad\u001b[0;34m(layer, named_params, named_layers, clipping_style, clipping_fn, numerical_stability_constant, max_grad_norm_layerwise)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_per_block_clip_grad\u001b[39m(\n\u001b[1;32m    104\u001b[0m     layer: nn\u001b[39m.\u001b[39mModule, named_params, named_layers, clipping_style, clipping_fn,\n\u001b[1;32m    105\u001b[0m     numerical_stability_constant,max_grad_norm_layerwise\n\u001b[1;32m    106\u001b[0m     ):\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m clipping_style \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mlayer-wise\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mparam-wise\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         norm_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([param\u001b[39m.\u001b[39;49mnorm_sample \u001b[39mfor\u001b[39;49;00m name, param \u001b[39min\u001b[39;49;00m named_params \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(param,\u001b[39m'\u001b[39;49m\u001b[39mnorm_sample\u001b[39;49m\u001b[39m'\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m         \u001b[39m# compute per-sample grad norm and clipping factor\u001b[39;00m\n\u001b[1;32m    112\u001b[0m         \u001b[39mif\u001b[39;00m clipping_fn\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mautomatic\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastDP import PrivacyEngine  # Ensure FastDP is installed\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=11):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.gpt2.generate(*args, **kwargs)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=6,  # Increased number of epochs\n",
    "    target_epsilon=16,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training settings\n",
    "epochs = 6  # Adjust the number of epochs here\n",
    "learning_rate = 3e-3  # Adjusted learning rate\n",
    "\n",
    "# Redefine the optimizer with the new learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variables to store loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Assuming classification_labels is provided in the batch\n",
    "        classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(classification_logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == classification_labels)\n",
    "        total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss}, Training Accuracy: {accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "            classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "            classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "            total_loss = loss + classification_loss\n",
    "\n",
    "            val_loss += total_loss.item()\n",
    "            \n",
    "            _, preds = torch.max(classification_logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == classification_labels)\n",
    "            total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Detach the PrivacyEngine\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  79 ; Number of trainable layers:  41\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['gpt2.transformer.wte']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/rss@vtti.com/Kyra_Test (1) (1).ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y212sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m classification_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(classification_logits, classification_labels)\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y212sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m classification_loss\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y212sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y212sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-23kacln4wp44snpu9yfili2l8b.i.anyscaleuserdata.com/home/ray/default/rss%40vtti.com/Kyra_Test%20%281%29%20%281%29.ipynb#Y212sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:72\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/fastDP/autograd_grad_sample.py:59\u001b[0m, in \u001b[0;36madd_hooks.<locals>.this_backward\u001b[0;34m(this_layer, grad_input, grad_output)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthis_backward\u001b[39m(this_layer, grad_input, grad_output):\n\u001b[1;32m     58\u001b[0m     _prepare_sample_grad_or_norm(this_layer, grad_output, loss_reduction, clipping_mode,bias_only)\n\u001b[0;32m---> 59\u001b[0m     _per_block_clip_grad(this_layer, named_params, named_layers, clipping_style, clipping_fn, numerical_stability_constant, max_grad_norm_layerwise)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/fastDP/autograd_grad_sample.py:110\u001b[0m, in \u001b[0;36m_per_block_clip_grad\u001b[0;34m(layer, named_params, named_layers, clipping_style, clipping_fn, numerical_stability_constant, max_grad_norm_layerwise)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_per_block_clip_grad\u001b[39m(\n\u001b[1;32m    104\u001b[0m     layer: nn\u001b[39m.\u001b[39mModule, named_params, named_layers, clipping_style, clipping_fn,\n\u001b[1;32m    105\u001b[0m     numerical_stability_constant,max_grad_norm_layerwise\n\u001b[1;32m    106\u001b[0m     ):\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m clipping_style \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mlayer-wise\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mparam-wise\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         norm_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([param\u001b[39m.\u001b[39;49mnorm_sample \u001b[39mfor\u001b[39;49;00m name, param \u001b[39min\u001b[39;49;00m named_params \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(param,\u001b[39m'\u001b[39;49m\u001b[39mnorm_sample\u001b[39;49m\u001b[39m'\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m         \u001b[39m# compute per-sample grad norm and clipping factor\u001b[39;00m\n\u001b[1;32m    112\u001b[0m         \u001b[39mif\u001b[39;00m clipping_fn\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mautomatic\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastDP import PrivacyEngine  # Ensure FastDP is installed\n",
    "\n",
    "class ConditionalGPT2(nn.Module):\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_labels=11):\n",
    "        super(ConditionalGPT2, self).__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.classification_head = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, classification_labels=None):\n",
    "        # GPT-2 forward pass\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        loss, logits, hidden_states = outputs.loss, outputs.logits, outputs.hidden_states\n",
    "\n",
    "        # Classification head forward pass\n",
    "        classification_logits = self.classification_head(hidden_states[-1].mean(dim=1))\n",
    "\n",
    "        return loss, logits, classification_logits\n",
    "    \n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        self.gpt2.resize_token_embeddings(new_num_tokens)\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.gpt2.generate(*args, **kwargs)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = ConditionalGPT2().to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item[0] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence([item[2] for item in batch], batch_first=True, padding_value=-100)\n",
    "    return input_ids, attention_masks, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=10,  # Increased number of epochs\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training settings\n",
    "epochs = 10  # Adjust the number of epochs here\n",
    "learning_rate = 3e-5  # Adjusted learning rate\n",
    "\n",
    "# Redefine the optimizer with the new learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variables to store loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "        # Assuming classification_labels is provided in the batch\n",
    "        classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "        total_loss = loss + classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(classification_logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == classification_labels)\n",
    "        total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss}, Training Accuracy: {accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            loss, logits, classification_logits = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "\n",
    "            classification_labels = torch.randint(0, 11, (input_ids.size(0),)).to(device)  # Random labels for example\n",
    "\n",
    "            classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)\n",
    "            total_loss = loss + classification_loss\n",
    "\n",
    "            val_loss += total_loss.item()\n",
    "            \n",
    "            _, preds = torch.max(classification_logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == classification_labels)\n",
    "            total_predictions += classification_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions.double() / total_predictions\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Detach the PrivacyEngine\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "    \n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences2 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    \n",
    "    # Alternate between \"This patient has\" and \"This patient is\"\n",
    "    prompt = \"This patient has \" if index % 2 == 0 else \"This patient is \"\n",
    "    \n",
    "    synthetic_sentence2 = generate_synthetic_sentence(model, tokenizer, specialty, prompt, max_length=50)\n",
    "    synthetic_sentences2.append(synthetic_sentence2)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_nodp1'] = synthetic_sentences2\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_bossep_nodp1_2014.csv', index=False)\n",
    "\n",
    "print(\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out gpt 2 (instead of distilgpt 2 and 8 epochs)\n",
    "* 22 min for a single epoch, 42 min for second epoch \n",
    "* x minutes for 8 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba520350c5e142dcbdbab7a4191445a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e450c1aa85c84733b2ac93d14d6ac0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b960ae21aab43b9a8de791e88d94712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0a94a1c8024b2f8c3ede802897f62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b985945c97e541bb8044f5b85f6c58dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9594b10ec7444521800f0b3730e88f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2eea3c1fb4f47228dc083cd4bcb1771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  149 ; Number of trainable layers:  76\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Epoch 6 completed.\n",
      "Epoch 7 completed.\n",
      "Epoch 8 completed.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_sentences4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1).ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m     prompt \u001b[39m=\u001b[39m extracted_text[\u001b[39mlen\u001b[39m(extracted_text)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m:]  \u001b[39m# Use the second half of the prompt\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m     synthetic_sentence_4 \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, prompt\u001b[39m=\u001b[39mprompt, max_length\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m     synthetic_sentences4\u001b[39m.\u001b[39mappend(synthetic_sentence4)\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-amag6vze734rfxxwm8nnn2bgpl.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msynthetic_sentence4\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m synthetic_sentences4\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synthetic_sentences4' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"Specialty: {self.labels[idx]} \"\n",
    "        text = specialty_label + self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=8,  # Increased epochs\n",
    "    target_epsilon=8,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(8):  # Increase epochs if needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences4 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence_4 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences4.append(synthetic_sentence4)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence4'] = synthetic_sentences4\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_gpt2.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences4 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence4 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences4.append(synthetic_sentence4)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence4'] = synthetic_sentences4\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_gpt2.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "Specialty: Cardiovascular / Pulmonary This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. I have had a very small heart rate, pulmonary hypertension or pulmonary bronchoscopy. My chest is low at this time and a slight loss in volume of air mass and breath rate. When breathing is normal, my upper body is at the moment\n",
      "Specialty: Cardiovascular / Pulmonary h resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. lung lungs pulmonary respiratory bron air and heart asthma.\n",
      " breathing- oxygen chest throat airflow breath, is or living body with breathe\n",
      "Specialty: Cardiovascular / Pulmonary h resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. lung lungs pulmonary respiratory bron air and heart asthma.\n",
      " breathing- oxygen chest throat airflow breath, is or living body with breathe\n",
      "Specialty: Cardiovascular / Pulmonary h resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past. Liver pulmonary liver cy l lung cy\n",
      " hepatic hepatatic\n",
      "\n",
      "blood liver\n",
      ", blood liver, liver liver cir\n",
      ". liver hepat\n",
      ":, hep, lung, heart, lymph,\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence2'])\n",
    "print(df.iloc[1]['synthetic_sentence3'])\n",
    "print(df.iloc[1]['synthetic_sentence4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_gpt2\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_gpt2\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out GPT-2 Medium with DP 4 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bae2f0a72a54c228802f44e115199cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5190babfc0b84d6f832484b8bd50a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd96edb6b884055807b6fc86bd17f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80696aa1483c4b1fbb5cf2458456efc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9660a46926034c569559da0a482d74ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc027eb951cf49648c33f74f44845215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c76ffa5ddb544edbbe5daa40f9a19c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  293 ; Number of trainable layers:  148\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"Specialty: {self.labels[idx]} \"\n",
    "        text = specialty_label + self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=4,  # Increased epochs\n",
    "    target_epsilon=3,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(4):  # Increase epochs if needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"\", max_length=50):\n",
    "    model.eval()\n",
    "    specialty_label = f\"Specialty: {label} \"\n",
    "    input_ids = tokenizer.encode(specialty_label + prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adding temperature to control randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences5 = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    extracted_text = row['extracted_text']\n",
    "    prompt = extracted_text[len(extracted_text)//2:]  # Use the second half of the prompt\n",
    "    synthetic_sentence5 = generate_synthetic_sentence(model, tokenizer, specialty, prompt=prompt, max_length=50)\n",
    "    synthetic_sentences5.append(synthetic_sentence5)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence5'] = synthetic_sentences5\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_gpt2_medium.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "Specialty: Cardiovascular / Pulmonary h resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "\"-,—'s is the \" and It to. \"The of' The I We D M;D/\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "#print(df.iloc[1]['synthetic_sentence'])\n",
    "#print(df.iloc[1]['synthetic_sentence2'])\n",
    "#print(df.iloc[1]['synthetic_sentence3'])\n",
    "#print(df.iloc[1]['synthetic_sentence4'])\n",
    "print(df.iloc[1]['synthetic_sentence5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_gpt2_medium\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_gpt2_medium\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence2'])\n",
    "print(df.iloc[1]['synthetic_sentence3'])\n",
    "print(df.iloc[1]['synthetic_sentence4'])\n",
    "print(df.iloc[1]['synthetic_sentence5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Out to have FastDP matched with LLama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc0cf8b4834474287a9200ffbd414d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "* Llama 3 seems to run with the Privacy Engine on 32GB CPU - 128GB\n",
    "* After running for 15 min it has not finished its first epoch yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadf17fa7b524f63b8e44a7622349177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd0d8c6b9c443e8a8c66109de5fc452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbca0192650d4b608c71307de80d6cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a9894d461043ae86f9834fa2e371e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6769a5499527413394e291122444d81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f8d479c5a74039a483de5da975bd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369f4f5da7f74de4bdae21d94a0b6c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083d0bcaab38441997ed0f457493eaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaabd747bc544599fcccefa46b62507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13987a57dbb84473afcc31b8bcea0c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3345a9e25bb42b4805702d0b7450733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e2b4a3dd9946ba888af7cc79e8f6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  226 ; Number of trainable layers:  226\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['model.embed_tokens']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight'] are not supported by privacy engine; these parameters are not requiring gradient nor updated.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining step completed successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/default/fastDP/fastDP/privacy_engine.py:324\u001b[0m, in \u001b[0;36mPrivacyEngine.attach.<locals>.dp_step\u001b[0;34m(_self, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m _self\u001b[39m.\u001b[39mzero_grad()         \u001b[39m# make sure no non-private grad remains\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39m_create_noisy_clipped_gradient(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 324\u001b[0m _self\u001b[39m.\u001b[39;49moriginal_step(closure\u001b[39m=\u001b[39;49mclosure)\n\u001b[1;32m    325\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39munlock()  \u001b[39m# Only enable creating new grads once parameters are updated.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39msteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     77\u001b[0m     d_p_list,\n\u001b[1;32m     78\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     79\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     80\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     81\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     83\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     85\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     86\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     88\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m func(params,\n\u001b[1;32m    223\u001b[0m      d_p_list,\n\u001b[1;32m    224\u001b[0m      momentum_buffer_list,\n\u001b[1;32m    225\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    226\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    229\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    230\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/sgd.py:265\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         d_p \u001b[39m=\u001b[39m buf\n\u001b[0;32m--> 265\u001b[0m param\u001b[39m.\u001b[39;49madd_(d_p, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from fastDP import PrivacyEngine\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = pipe.tokenizer\n",
    "model = pipe.model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=8,  # Adjusted batch size for memory constraints\n",
    "    sample_size=50000,\n",
    "    epochs=3,\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Example training pipeline\n",
    "# Assuming `batch` and `labels` are your input tensors and target labels respectively\n",
    "# For this example, we'll create dummy data\n",
    "batch = torch.randint(0, model.config.vocab_size, (8, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "labels = torch.randint(0, model.config.vocab_size, (8, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "\n",
    "# Standard training loop\n",
    "outputs = model(batch, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Training step completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aFter 2 hours of running the code below, has not finished first epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7971b7b62c2a4ff09db5b58abd0f3983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e13f09ad72c4572a56f56302d898560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b405500dfca44d2b97ad8262e32e5288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1175df751337407a8b97d48450526f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a68d45540504937a4bba104c40e7bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9c331537c8435a9e663549fd4dc5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ee703f98f5476d96691d03f5e02f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b28615e6f5143deb017f63f558777c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd589177e924f65b4aab1836e98ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb307830d914519925823c917abd91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a57d0ebaa348d496249cd7090b10ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad158e876a0143419e0e8cbd78b98f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  226 ; Number of trainable layers:  226\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['model.embed_tokens']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight'] are not supported by privacy engine; these parameters are not requiring gradient nor updated.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/default/fastDP/fastDP/privacy_engine.py:324\u001b[0m, in \u001b[0;36mPrivacyEngine.attach.<locals>.dp_step\u001b[0;34m(_self, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m _self\u001b[39m.\u001b[39mzero_grad()         \u001b[39m# make sure no non-private grad remains\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39m_create_noisy_clipped_gradient(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 324\u001b[0m _self\u001b[39m.\u001b[39;49moriginal_step(closure\u001b[39m=\u001b[39;49mclosure)\n\u001b[1;32m    325\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39munlock()  \u001b[39m# Only enable creating new grads once parameters are updated.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m _self\u001b[39m.\u001b[39mprivacy_engine\u001b[39m.\u001b[39msteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from fastDP import PrivacyEngine\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = pipe.tokenizer\n",
    "model = pipe.model.to(device)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=8,  # Adjusted batch size for memory constraints\n",
    "    sample_size=400,\n",
    "    epochs=3,  # Adjusted epochs\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Dummy dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepend the label to the sentence\n",
    "        sentiment_label = \"Positive: \" if self.labels[idx] == 1 else \"Negative: \"\n",
    "        text = sentiment_label + self.sentences[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Assume `data` is a list of sentences and `labels` is a list of sentiment labels\n",
    "# For demonstration purposes, we'll create a more diverse and realistic dataset\n",
    "data = [\n",
    "    \"I love this product because it is fantastic and works perfectly.\",\n",
    "    \"This is the worst purchase I have ever made, it is horrible.\",\n",
    "    \"I'm extremely happy with this item, it exceeded my expectations.\",\n",
    "    \"I regret buying this, it is completely useless.\",\n",
    "    \"This product is amazing! It has made my life so much easier.\",\n",
    "    \"Absolutely terrible, I will never buy this again.\",\n",
    "    \"This is the best thing I have ever purchased, highly recommend it!\",\n",
    "    \"It broke after one use, very disappointed.\",\n",
    "    \"I am so satisfied with this purchase, it is everything I wanted.\",\n",
    "    \"Not worth the money, very low quality.\"\n",
    "] * 100\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 100\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SentimentDataset(train_data, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_data, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Adjusted batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjusted epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Function to generate synthetic sentences with sentiment label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"Write a sentence that expresses a \", max_length=50):\n",
    "    model.eval()\n",
    "    sentiment_label = \"positive sentiment.\" if label == 1 else \"negative sentiment.\"\n",
    "    full_prompt = prompt + sentiment_label\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.7  # Adjust temperature for more randomness\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences\n",
    "synthetic_sentences = []\n",
    "for label in [0, 1]:\n",
    "    sentence = generate_synthetic_sentence(model, tokenizer, label)\n",
    "    synthetic_sentences.append((sentence, label))\n",
    "\n",
    "print(\"Synthetic sentences generated:\", synthetic_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Out with Mixtral 8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398cf726ef5e44baadbc49f6397d98b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a3450a27d14ff79c36dd78b30750dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2d0530d14a44a99d39a03f83b9f7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf7f1d2d0984211a5d2e361ddda287c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399d57701854c1d8848a3c9d6918536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31d799bd4c44c39aff106c235672a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a163648096449bcb0a9365dca7a4dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b40afa308469dad0549d5ac94f9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abc7cb2817e44379e44900f78a89974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmistralai/Mixtral-8x7B-v0.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:3264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3262\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3263\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3265\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3266\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3267\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   3268\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   3269\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   3270\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   3271\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   3272\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3273\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   3274\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   3275\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   3276\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   3277\u001b[0m     )\n\u001b[1;32m   3279\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3280\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3281\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(resolved_archive_file, \u001b[39mstr\u001b[39m)\n\u001b[1;32m   3282\u001b[0m     \u001b[39mand\u001b[39;00m resolved_archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3283\u001b[0m ):\n\u001b[1;32m   3284\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:1038\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1036\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1039\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1040\u001b[0m             shard_filename,\n\u001b[1;32m   1041\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1042\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1043\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1044\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1045\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1046\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1047\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1048\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1049\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1050\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m   1051\u001b[0m         )\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[39m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[39m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[39m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1222\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1224\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1226\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1227\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1228\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1229\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1231\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1232\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1233\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1234\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1236\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1237\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[39mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1368\u001b[0m         incomplete_path\u001b[39m=\u001b[39;49mPath(blob_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.incomplete\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1369\u001b[0m         destination_path\u001b[39m=\u001b[39;49mPath(blob_path),\n\u001b[1;32m   1370\u001b[0m         url_to_download\u001b[39m=\u001b[39;49murl_to_download,\n\u001b[1;32m   1371\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1372\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1373\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1374\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1375\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1376\u001b[0m     )\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[39mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[39m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[39m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     http_get(\n\u001b[1;32m   1885\u001b[0m         url_to_download,\n\u001b[1;32m   1886\u001b[0m         f,\n\u001b[1;32m   1887\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1888\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1889\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1890\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1891\u001b[0m     )\n\u001b[1;32m   1893\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownload complete. Moving file to \u001b[39m\u001b[39m{\u001b[39;00mdestination_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:542\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n\u001b[0;32m--> 542\u001b[0m     temp_file\u001b[39m.\u001b[39;49mwrite(chunk)\n\u001b[1;32m    543\u001b[0m     new_resume_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    544\u001b[0m     \u001b[39m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0d18ac1b774068bcaabbc061835014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac92c60e21b4f8b924a57fc38c5c705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2a3e84a9ea47f58ff98c18eeb4ad06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5981862b3b437999aa4bcd53a356ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6cc7bed0964a319f5cdb7cbde523fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97720bc423b4465cb14a755119fd5391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe39c25b3aad44eaae4a7f1f0fb88e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cece5c87b7694e9686026abe37a14d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244bc3d33b5e4faebbc0df410c49f1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e841038b7641a88e5993ed1c44660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1deb80e38741cd91d9b7672ed9f719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42daa0b52c742d9a7261dac291ff157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a901d05be844326adf2f0b87f2ec210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59494e5ef7944b18c2862d352977643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80e8675eab14547ba6dabc647e1ace7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447806bd6bb544c780229c2411e96f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b82cb897db44e383b7010dac0c4626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b1ad87c602476582ec7d9a1bfaf460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa003bddc8c847fcb86556704f6f9aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from fastDP import PrivacyEngine\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=16,  # Further adjusted batch size for memory constraints\n",
    "    sample_size=50000,\n",
    "    epochs=3,\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Example training pipeline\n",
    "# Assuming `batch` and `labels` are your input tensors and target labels respectively\n",
    "# For this example, we'll create dummy data\n",
    "batch = torch.randint(0, model.config.vocab_size, (16, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "labels = torch.randint(0, model.config.vocab_size, (16, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "\n",
    "# Standard training loop\n",
    "outputs = model(batch)\n",
    "loss = F.cross_entropy(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Training step completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Header and LLM combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ray/anaconda3/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2023.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ray/anaconda3/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_8stzu32z64efckgcwt2im26scn-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4219296b05e24c62bcaf0e0ec5479deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8da6d536d04400f9c321d9e023d54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b59b4634b424dd5a892490094e82731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e9bb67ab8842529d20feb26b1a47d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.6602942943572998\n",
      "Epoch 2 completed. Loss: 0.08503400534391403\n",
      "Epoch 3 completed. Loss: 0.0816885232925415\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_id = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the classifier head\n",
    "class ClassifierHead(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.dense = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.out_proj = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# Attach classifier head to the model\n",
    "class GPT2WithClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super(GPT2WithClassifier, self).__init__()\n",
    "        self.transformer = model.transformer\n",
    "        self.lm_head = model.lm_head\n",
    "        self.classifier = ClassifierHead(model.config.n_embd, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, cls_labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        cls_logits = self.classifier(hidden_states[:, 0, :])  # Use [CLS] token for classification\n",
    "        \n",
    "        outputs = (lm_logits, cls_logits)\n",
    "        \n",
    "        if labels is not None and cls_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            cls_loss = loss_fct(cls_logits.view(-1, cls_logits.size(-1)), cls_labels.view(-1))\n",
    "            loss = lm_loss + cls_loss\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.transformer.generate(*args, **kwargs)\n",
    "\n",
    "num_labels = 2  # For binary classification (positive/negative)\n",
    "model_with_classifier = GPT2WithClassifier(model, num_labels).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model_with_classifier.parameters(), lr=5e-5)\n",
    "\n",
    "# Dummy dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepend the label to the sentence\n",
    "        sentiment_label = \"Positive: \" if self.labels[idx] == 1 else \"Negative: \"\n",
    "        text = sentiment_label + self.sentences[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "        \n",
    "        return input_ids, attention_mask, target_ids, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"I love this product because it is fantastic and works perfectly.\",\n",
    "    \"This is the worst purchase I have ever made, it is horrible.\",\n",
    "    \"I'm extremely happy with this item, it exceeded my expectations.\",\n",
    "    \"I regret buying this, it is completely useless.\",\n",
    "    \"This product is amazing! It has made my life so much easier.\",\n",
    "    \"Absolutely terrible, I will never buy this again.\",\n",
    "    \"This is the best thing I have ever purchased, highly recommend it!\",\n",
    "    \"It broke after one use, very disappointed.\",\n",
    "    \"I am so satisfied with this purchase, it is everything I wanted.\",\n",
    "    \"Not worth the money, very low quality.\"\n",
    "] * 100\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 100\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SentimentDataset(train_data, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_data, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model_with_classifier.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids, cls_labels = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model_with_classifier(input_ids, attention_mask=attention_mask, labels=target_ids, cls_labels=cls_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.6073985695838928\n",
      "Epoch 2 completed. Loss: 0.08109154552221298\n",
      "Epoch 3 completed. Loss: 0.01457744836807251\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_id = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the classifier head\n",
    "class ClassifierHead(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.dense = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.out_proj = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# Attach classifier head to the model\n",
    "class GPT2WithClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super(GPT2WithClassifier, self).__init__()\n",
    "        self.transformer = model.transformer\n",
    "        self.lm_head = model.lm_head\n",
    "        self.classifier = ClassifierHead(model.config.n_embd, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, cls_labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        cls_logits = self.classifier(hidden_states[:, 0, :])  # Use [CLS] token for classification\n",
    "        \n",
    "        outputs = (lm_logits, cls_logits)\n",
    "        \n",
    "        if labels is not None and cls_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            cls_loss = loss_fct(cls_logits.view(-1, cls_logits.size(-1)), cls_labels.view(-1))\n",
    "            loss = lm_loss + cls_loss\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.transformer.generate(*args, **kwargs)\n",
    "\n",
    "num_labels = 2  # For binary classification (positive/negative)\n",
    "model_with_classifier = GPT2WithClassifier(model, num_labels).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model_with_classifier.parameters(), lr=5e-5)\n",
    "\n",
    "# Dummy dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepend the label to the sentence\n",
    "        sentiment_label = \"Positive: \" if self.labels[idx] == 1 else \"Negative: \"\n",
    "        text = sentiment_label + self.sentences[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "        \n",
    "        return input_ids, attention_mask, target_ids, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"I love this product because it is fantastic and works perfectly.\",\n",
    "    \"This is the worst purchase I have ever made, it is horrible.\",\n",
    "    \"I'm extremely happy with this item, it exceeded my expectations.\",\n",
    "    \"I regret buying this, it is completely useless.\",\n",
    "    \"This product is amazing! It has made my life so much easier.\",\n",
    "    \"Absolutely terrible, I will never buy this again.\",\n",
    "    \"This is the best thing I have ever purchased, highly recommend it!\",\n",
    "    \"It broke after one use, very disappointed.\",\n",
    "    \"I am so satisfied with this purchase, it is everything I wanted.\",\n",
    "    \"Not worth the money, very low quality.\"\n",
    "] * 100\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 100\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SentimentDataset(train_data, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_data, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model_with_classifier.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids, cls_labels = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model_with_classifier(input_ids, attention_mask=attention_mask, labels=target_ids, cls_labels=cls_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2WithClassifier' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(save_directory)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model_with_classifier\u001b[39m.\u001b[39;49msave_pretrained(save_directory)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Save the tokenizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(save_directory)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2WithClassifier' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model_with_classifier.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-02 13:29:26,080] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_id = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the classifier head\n",
    "class ClassifierHead(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.dense = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.out_proj = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# Attach classifier head to the model\n",
    "class GPT2WithClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super(GPT2WithClassifier, self).__init__()\n",
    "        self.transformer = model.transformer\n",
    "        self.lm_head = model.lm_head\n",
    "        self.classifier = ClassifierHead(model.config.n_embd, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, cls_labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        cls_logits = self.classifier(hidden_states[:, 0, :])  # Use [CLS] token for classification\n",
    "        \n",
    "        outputs = (lm_logits, cls_logits)\n",
    "        \n",
    "        if labels is not None and cls_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            cls_loss = loss_fct(cls_logits.view(-1, cls_logits.size(-1)), cls_labels.view(-1))\n",
    "            loss = lm_loss + cls_loss\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.transformer.generate(*args, **kwargs)\n",
    "    \n",
    "    def save_pretrained(self, save_directory):\n",
    "        # Save the transformer model\n",
    "        self.transformer.save_pretrained(save_directory)\n",
    "        # Save the classifier head separately\n",
    "        classifier_head_path = os.path.join(save_directory, 'classifier_head.pth')\n",
    "        torch.save(self.classifier.state_dict(), classifier_head_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, num_labels, *model_args, **kwargs):\n",
    "        # Load the transformer model\n",
    "        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "        # Initialize the custom model\n",
    "        model_with_classifier = cls(model, num_labels)\n",
    "        # Load the classifier head separately\n",
    "        classifier_head_path = os.path.join(pretrained_model_name_or_path, 'classifier_head.pth')\n",
    "        model_with_classifier.classifier.load_state_dict(torch.load(classifier_head_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "        return model_with_classifier\n",
    "\n",
    "num_labels = 2  # For binary classification (positive/negative)\n",
    "model_with_classifier = GPT2WithClassifier(model, num_labels).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model_with_classifier.parameters(), lr=5e-5)\n",
    "\n",
    "# Dummy dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepend the label to the sentence\n",
    "        sentiment_label = \"Positive: \" if self.labels[idx] == 1 else \"Negative: \"\n",
    "        text = sentiment_label + self.sentences[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "        \n",
    "        return input_ids, attention_mask, target_ids, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"I love this product because it is fantastic and works perfectly.\",\n",
    "    \"This is the worst purchase I have ever made, it is horrible.\",\n",
    "    \"I'm extremely happy with this item, it exceeded my expectations.\",\n",
    "    \"I regret buying this, it is completely useless.\",\n",
    "    \"This product is amazing! It has made my life so much easier.\",\n",
    "    \"Absolutely terrible, I will never buy this again.\",\n",
    "    \"This is the best thing I have ever purchased, highly recommend it!\",\n",
    "    \"It broke after one use, very disappointed.\",\n",
    "    \"I am so satisfied with this purchase, it is everything I wanted.\",\n",
    "    \"Not worth the money, very low quality.\"\n",
    "] * 100\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 100\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SentimentDataset(train_data, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_data, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model_with_classifier.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids, cls_labels = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model_with_classifier(input_ids, attention_mask=attention_mask, labels=target_ids, cls_labels=cls_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
    "\n",
    "# Saving the model\n",
    "save_directory = \"./trained_model_with_classifier\"\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "model_with_classifier.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2WithClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/ray/default/fastDP/trained_model_with_classifier\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m model_with_classifier, tokenizer \u001b[39m=\u001b[39m load_model_and_tokenizer(model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel and tokenizer have been loaded successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Load the custom model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model_with_classifier \u001b[39m=\u001b[39m GPT2WithClassifier\u001b[39m.\u001b[39mfrom_pretrained(model_path, num_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model_with_classifier\u001b[39m.\u001b[39meval()  \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_with_classifier, tokenizer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2WithClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    # Define the number of labels based on your classifier configuration\n",
    "    num_labels = 2  # Update this if your model uses a different number of labels\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Load the custom model\n",
    "    model_with_classifier = GPT2WithClassifier.from_pretrained(model_path, num_labels)\n",
    "    model_with_classifier.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    return model_with_classifier, tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"/home/ray/default/fastDP/trained_model_with_classifier\"\n",
    "model_with_classifier, tokenizer = load_model_and_tokenizer(model_path)\n",
    "print(\"Model and tokenizer have been loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2WithClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super(GPT2WithClassifier, self).__init__()\n",
    "        self.transformer = model.transformer\n",
    "        self.lm_head = model.lm_head\n",
    "        self.classifier = ClassifierHead(model.config.n_embd, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, cls_labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        cls_logits = self.classifier(hidden_states[:, 0, :])\n",
    "        return (lm_logits, cls_logits)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.lm_head.generate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m synthetic_sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model_with_classifier, tokenizer, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     synthetic_sentences\u001b[39m.\u001b[39mappend((sentence, label))\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSynthetic sentences generated:\u001b[39m\u001b[39m\"\u001b[39m, synthetic_sentences)\n",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m full_prompt \u001b[39m=\u001b[39m prompt \u001b[39m+\u001b[39m sentiment_label\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(full_prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m generated_sentence\n",
      "\u001b[1;32m/home/ray/default/fastDP/Kyra_Test.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-l3y6fg1rz9amflbeqthjba54ug.i.anyscaleuserdata.com/home/ray/default/fastDP/Kyra_Test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1323\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         synced_gpus \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_class()\n\u001b[1;32m   1325\u001b[0m \u001b[39m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[39mif\u001b[39;00m generation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[39m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[39m# three conditions must be met\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[39m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[39m# 2) the generation config must have seen no modification since its creation (the hash is the same);\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# 3) the user must have set generation parameters in the model config.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1110\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1109\u001b[0m     exception_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please use one of the following classes instead: \u001b[39m\u001b[39m{\u001b[39;00mgenerate_compatible_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1110\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'device' is already defined, typically as:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"Write a sentence that expresses a \", max_length=50):\n",
    "    model.eval()\n",
    "    sentiment_label = \"positive sentiment.\" if label == 1 else \"negative sentiment.\"\n",
    "    full_prompt = prompt + sentiment_label\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences\n",
    "synthetic_sentences = []\n",
    "for label in [0, 1]:\n",
    "    sentence = generate_synthetic_sentence(model_with_classifier, tokenizer, label)\n",
    "    synthetic_sentences.append((sentence, label))\n",
    "\n",
    "print(\"Synthetic sentences generated:\", synthetic_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic sentences with sentiment label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt=\"Write a sentence that expresses a \", max_length=50):\n",
    "    model.eval()\n",
    "    sentiment_label = \"positive sentiment.\" if label == 1 else \"negative sentiment.\"\n",
    "    full_prompt = prompt + sentiment_label\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences\n",
    "synthetic_sentences = []\n",
    "for label in [0, 1]:\n",
    "    sentence = generate_synthetic_sentence(model_with_classifier, tokenizer, label)\n",
    "    synthetic_sentences.append((sentence, label))\n",
    "\n",
    "print(\"Synthetic sentences generated:\", synthetic_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_OG</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>masked_transcriptions_OG</th>\n",
       "      <th>pii_masked_transcriptions_OG</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>synthetic_sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reason consultation mesothelioma history prese...</td>\n",
       "      <td>REASON FOR CONSULTATION: , Mesothelioma.,HISTO...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>reason consultation[DISEASE_DISORDER] history ...</td>\n",
       "      <td>reason consultation[DISEASE_DISORDER] history ...</td>\n",
       "      <td>REASON FOR CONSULTATION: , Mesothelioma.,HISTO...</td>\n",
       "      <td>REASON FOR CONSULTATION: , Mesothelioma.,HISTO...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , The patient is a...</td>\n",
       "      <td>The patient is a 73-year-old pleasant Caucasia...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiovas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>history present illness 91yearold male previou...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , This is a 91-yea...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>history present illness[AGE][SEX] previous his...</td>\n",
       "      <td>history present illness[AGE][SEX] previous his...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , This is a 91-yea...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , This is a 91-yea...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , This is a 91-yea...</td>\n",
       "      <td>This is a 91-year-old male with a previous his...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiac c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preoperative diagnosis left adrenal mass 55 cm...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>preoperative diagnosis left[DISEASE_DISORDER][...</td>\n",
       "      <td>preoperative diagnosis left[DISEASE_DISORDER][...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...</td>\n",
       "      <td>HISTORY:,  This is a 57-year-old female who wa...</td>\n",
       "      <td>This is a 57-year-old female who was found to ...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] pulmonary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>history present illness patient 68yearold man ...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>history present illness patient[AGE][SEX] retu...</td>\n",
       "      <td>history present illness patient[AGE][SEX] retu...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS:,  The patient is a...</td>\n",
       "      <td>The patient is a 68-year-old man who returns f...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cholester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subjective patient 78yearold female problem es...</td>\n",
       "      <td>SUBJECTIVE:,  The patient is a 78-year-old fem...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>subjective patient[AGE] female problem essenti...</td>\n",
       "      <td>subjective patient[AGE] female problem essenti...</td>\n",
       "      <td>SUBJECTIVE:,  The patient is a[AGE][SEX] with ...</td>\n",
       "      <td>SUBJECTIVE:,  The patient is a[AGE][SEX] with ...</td>\n",
       "      <td>SUBJECTIVE:,  The patient is a 78-year-old fem...</td>\n",
       "      <td>The patient is a 78-year-old female with the p...</td>\n",
       "      <td>[BOS]Cardiovascular / Pulmonary[SEP] cardiovas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       transcription  \\\n",
       "0  reason consultation mesothelioma history prese...   \n",
       "1  history present illness 91yearold male previou...   \n",
       "2  preoperative diagnosis left adrenal mass 55 cm...   \n",
       "3  history present illness patient 68yearold man ...   \n",
       "4  subjective patient 78yearold female problem es...   \n",
       "\n",
       "                                    transcription_OG  \\\n",
       "0  REASON FOR CONSULTATION: , Mesothelioma.,HISTO...   \n",
       "1  HISTORY OF PRESENT ILLNESS: , This is a 91-yea...   \n",
       "2  PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...   \n",
       "3  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "4  SUBJECTIVE:,  The patient is a 78-year-old fem...   \n",
       "\n",
       "            medical_specialty  \\\n",
       "0  Cardiovascular / Pulmonary   \n",
       "1  Cardiovascular / Pulmonary   \n",
       "2  Cardiovascular / Pulmonary   \n",
       "3  Cardiovascular / Pulmonary   \n",
       "4  Cardiovascular / Pulmonary   \n",
       "\n",
       "                               masked_transcriptions  \\\n",
       "0  reason consultation[DISEASE_DISORDER] history ...   \n",
       "1  history present illness[AGE][SEX] previous his...   \n",
       "2  preoperative diagnosis left[DISEASE_DISORDER][...   \n",
       "3  history present illness patient[AGE][SEX] retu...   \n",
       "4  subjective patient[AGE] female problem essenti...   \n",
       "\n",
       "                           pii_masked_transcriptions  \\\n",
       "0  reason consultation[DISEASE_DISORDER] history ...   \n",
       "1  history present illness[AGE][SEX] previous his...   \n",
       "2  preoperative diagnosis left[DISEASE_DISORDER][...   \n",
       "3  history present illness patient[AGE][SEX] retu...   \n",
       "4  subjective patient[AGE] female problem essenti...   \n",
       "\n",
       "                            masked_transcriptions_OG  \\\n",
       "0  REASON FOR CONSULTATION: , Mesothelioma.,HISTO...   \n",
       "1  HISTORY OF PRESENT ILLNESS: , This is a 91-yea...   \n",
       "2  PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...   \n",
       "3  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "4  SUBJECTIVE:,  The patient is a[AGE][SEX] with ...   \n",
       "\n",
       "                        pii_masked_transcriptions_OG  \\\n",
       "0  REASON FOR CONSULTATION: , Mesothelioma.,HISTO...   \n",
       "1  HISTORY OF PRESENT ILLNESS: , This is a 91-yea...   \n",
       "2  PREOPERATIVE DIAGNOSIS: , Left adrenal mass, 5...   \n",
       "3  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "4  SUBJECTIVE:,  The patient is a[AGE][SEX] with ...   \n",
       "\n",
       "                                age_related_sentence  \\\n",
       "0  HISTORY OF PRESENT ILLNESS: , The patient is a...   \n",
       "1  HISTORY OF PRESENT ILLNESS: , This is a 91-yea...   \n",
       "2  HISTORY:,  This is a 57-year-old female who wa...   \n",
       "3  HISTORY OF PRESENT ILLNESS:,  The patient is a...   \n",
       "4  SUBJECTIVE:,  The patient is a 78-year-old fem...   \n",
       "\n",
       "                                      extracted_text  \\\n",
       "0  The patient is a 73-year-old pleasant Caucasia...   \n",
       "1  This is a 91-year-old male with a previous his...   \n",
       "2  This is a 57-year-old female who was found to ...   \n",
       "3  The patient is a 68-year-old man who returns f...   \n",
       "4  The patient is a 78-year-old female with the p...   \n",
       "\n",
       "                                 synthetic_sentence2  \n",
       "0  [BOS]Cardiovascular / Pulmonary[SEP] cardiovas...  \n",
       "1  [BOS]Cardiovascular / Pulmonary[SEP] cardiac c...  \n",
       "2  [BOS]Cardiovascular / Pulmonary[SEP] pulmonary...  \n",
       "3  [BOS]Cardiovascular / Pulmonary[SEP] cholester...  \n",
       "4  [BOS]Cardiovascular / Pulmonary[SEP] cardiovas...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/ray/default/rss@vtti.com/synthetic_data_bossep_dp.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 91-year-old male with a previous history of working in the coalmine and significant exposure to silica with resultant pneumoconiosis and fibrosis of the lung. The patient also has a positive history of smoking in the past.\n",
      "[BOS]Cardiovascular / Pulmonary[SEP] cardiac coronary cardiovascular heart card blood arter fat cholesterol LDL HDL fatty adip triglycer insulin athe diabetes/\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['extracted_text'])\n",
    "#print(df.iloc[1]['synthetic_sentence'])\n",
    "print(df.iloc[1]['synthetic_sentence2'])\n",
    "#print(df.iloc[1]['synthetic_sentence3'])\n",
    "#print(df.iloc[1]['synthetic_sentence4'])\n",
    "#print(df.iloc[1]['synthetic_sentence5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
