{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "  Cloning https://github.com/awslabs/fast-differential-privacy.git to /private/var/folders/th/lhv5svr57tl9sd7p465ddg3h0000gn/T/pip-req-build-dnb74kyp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy.git /private/var/folders/th/lhv5svr57tl9sd7p465ddg3h0000gn/T/pip-req-build-dnb74kyp\n",
      "  Resolved https://github.com/awslabs/fast-differential-privacy.git to commit bd81a45ae1badcdc773c5306ebfcb1fafef43966\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/awslabs/fast-differential-privacy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (4.39.3)\n",
      "Requirement already satisfied: torch in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: filelock in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch scikit-learn codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/kyradresen/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from fastDP import PrivacyEngine\n",
    "import transformers, torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>med_masked_transcriptions</th>\n",
       "      <th>pii_masked_transcriptions</th>\n",
       "      <th>synthetic_sentence_nodp_temp0.6_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1_topk50</th>\n",
       "      <th>synthetic_sentence_eps8_temp1_topk100</th>\n",
       "      <th>synthetic_sentence_eps3_temp1_topk100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>(primarily obtained from boyfriend): This 31 y...</td>\n",
       "      <td>43</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>(primarily obtained from boyfriend): This[AGE]...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 55-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 38-year-ol...</td>\n",
       "      <td>[BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a 42-year-ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>The patient is a 50-year-old white male with h...</td>\n",
       "      <td>72</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>The patient is a[AGE] white[SEX] with history ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]This is a yo...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>43</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient was brought to th...</td>\n",
       "      <td>[BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...</td>\n",
       "      <td>[BOS]Urology[SEP]This is a 38-year-old with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>DESCRIPTION:, The patient was placed in the su...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>44</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 64-year-old ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient with a history of...</td>\n",
       "      <td>[BOS]Urology[SEP]Ophytology[SEPURE] The patien...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is referred to a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>63</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 1-year-old f...</td>\n",
       "      <td>[BOS]Urology[SEP]Neurology[SEPUE]Neuroptic[SEP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          medical_specialty  \\\n",
       "0                 Neurology   \n",
       "1  Cardiovascular/Pulmonary   \n",
       "2                   Urology   \n",
       "3                   Urology   \n",
       "4                   Urology   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1  PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:,  The patient was placed in the s...   \n",
       "4  PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "\n",
       "                                age_related_sentence  \\\n",
       "0  HX , (primarily obtained from boyfriend): This...   \n",
       "1  INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:, The patient was placed in the su...   \n",
       "4  INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "\n",
       "                                      extracted_text  word_count  \\\n",
       "0  (primarily obtained from boyfriend): This 31 y...          43   \n",
       "1  The patient is a 50-year-old white male with h...          72   \n",
       "2  Elective male sterilization via bilateral vase...          43   \n",
       "3  The patient was placed in the supine position ...          44   \n",
       "4  A gentleman who is here today requesting volun...          63   \n",
       "\n",
       "                           med_masked_transcriptions  \\\n",
       "0  (primarily obtained from boyfriend): This[AGE]...   \n",
       "1  The patient is a[AGE] white[SEX] with history ...   \n",
       "2  Elective male sterilization via bilateral vase...   \n",
       "3  The patient was placed in the supine position ...   \n",
       "4  A gentleman who is here today requesting volun...   \n",
       "\n",
       "                           pii_masked_transcriptions  \\\n",
       "0  (primarily obtained from boyfriend): This[AGE]...   \n",
       "1  The patient is a[AGE] white[SEX] with history ...   \n",
       "2  Elective male sterilization via bilateral vase...   \n",
       "3  The patient was placed in the supine position ...   \n",
       "4  A gentleman who is here today requesting volun...   \n",
       "\n",
       "              synthetic_sentence_nodp_temp0.6_topk50  \\\n",
       "0  [BOS]Neurology[SEP]The patient is a 55-year-ol...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]The patient was brought to th...   \n",
       "3  [BOS]Urology[SEP]The patient is a 64-year-old ...   \n",
       "4  [BOS]Urology[SEP]HISTORY OF PRESENT ILLNESS:, ...   \n",
       "\n",
       "               synthetic_sentence_eps16_temp1_topk50  \\\n",
       "0  [BOS]Neurology[SEP]The patient is a 38-year-ol...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]Cardiovascular/Pulmonary[SEP:...   \n",
       "3  [BOS]Urology[SEP]The patient with a history of...   \n",
       "4  [BOS]Urology[SEP]HISTORY OF THE PROCEDURE:, Th...   \n",
       "\n",
       "               synthetic_sentence_eps8_temp1_topk100  \\\n",
       "0  [BOS]Neurology[SEP]Shitetron[SEPSEP]The patien...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]This is a yo...   \n",
       "2  [BOS]Urology[SEP]HISTORY OF DISHISTORY AND BEE...   \n",
       "3  [BOS]Urology[SEP]Ophytology[SEPURE] The patien...   \n",
       "4  [BOS]Urology[SEP]The patient is a 1-year-old f...   \n",
       "\n",
       "               synthetic_sentence_eps3_temp1_topk100  \n",
       "0  [BOS]Neurology[SEP]The patient is a 42-year-ol...  \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2  [BOS]Urology[SEP]This is a 38-year-old with a ...  \n",
       "3  [BOS]Urology[SEP]The patient is referred to a ...  \n",
       "4  [BOS]Urology[SEP]Neurology[SEPUE]Neuroptic[SEP...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/Users/kyradresen/MasterThesis_MedTextPrivacy/data/2016r_mtsamples_final.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AWS FastDP Privacy Engine Runs with ChatGPT 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This gives a cuda out of memore error on the heavy GPUs \n",
    "* It runs succesfully on 32 vCPU, 128 GiB memory, takes around 5-10 min to run \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6444dd499f4f7e839ef9ae5e6ca989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989c34268c69438e98bd3230dbfa6269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43daab59bedb46ffb56fa273c5e92dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from transformers import GPT2LMHeadModel\n",
    "from fastDP import PrivacyEngine\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and move it to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=32,  # Adjusted batch size for memory constraints\n",
    "    sample_size=50000,\n",
    "    epochs=3,\n",
    "    target_epsilon=2,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Example training pipeline\n",
    "# Assuming `batch` and `labels` are your input tensors and target labels respectively\n",
    "# For this example, we'll create dummy data\n",
    "batch = torch.randint(0, model.config.vocab_size, (32, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "labels = torch.randint(0, model.config.vocab_size, (32, 512), dtype=torch.long).to(device)  # Adjusted batch size\n",
    "\n",
    "# Standard training loop\n",
    "outputs = model(batch)\n",
    "loss = F.cross_entropy(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Training step completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune DistilGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Load your dataframe here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['extracted_text'].tolist()\n",
    "labels = df['medical_specialty'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=16,\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "@track_emissions\n",
    "def training_loop():\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Run the training loop\n",
    "training_loop()\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_dpeps16_2016\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_dpeps16_2016\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_sentence_eps16_temp1_topk50.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 11:33:29] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 11:33:29] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 11:33:29] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 11:33:29] No GPU found.\n",
      "[codecarbon INFO @ 11:33:29] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 11:33:29] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 11:33:31] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 11:33:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 11:33:31]   Platform system: Linux-6.5.0-1020-aws-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 11:33:31]   Python version: 3.9.19\n",
      "[codecarbon INFO @ 11:33:31]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 11:33:31]   Available RAM : 124.434 GB\n",
      "[codecarbon INFO @ 11:33:31]   CPU count: 32\n",
      "[codecarbon INFO @ 11:33:31]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 11:33:31]   GPU count: None\n",
      "[codecarbon INFO @ 11:33:31]   GPU model: None\n",
      "[codecarbon INFO @ 11:33:46] Energy consumed for RAM : 0.000194 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:33:46] Energy consumed for all CPUs : 0.000438 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:33:46] 0.000632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:01] Energy consumed for RAM : 0.000389 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:01] Energy consumed for all CPUs : 0.000875 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:01] 0.001264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:16] Energy consumed for RAM : 0.000583 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:16] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:16] 0.001896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:31] Energy consumed for RAM : 0.000778 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:31] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:31] 0.002528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:34:46] Energy consumed for RAM : 0.000972 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:34:46] Energy consumed for all CPUs : 0.002187 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:34:46] 0.003159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:01] Energy consumed for RAM : 0.001166 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:01] Energy consumed for all CPUs : 0.002625 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:01] 0.003791 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:16] Energy consumed for RAM : 0.001361 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:16] Energy consumed for all CPUs : 0.003062 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:16] 0.004423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:31] Energy consumed for RAM : 0.001555 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:31] Energy consumed for all CPUs : 0.003500 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:31] 0.005055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:35:46] Energy consumed for RAM : 0.001750 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:35:46] Energy consumed for all CPUs : 0.003937 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:35:46] 0.005687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:01] Energy consumed for RAM : 0.001944 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:01] Energy consumed for all CPUs : 0.004375 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:01] 0.006319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:16] Energy consumed for RAM : 0.002138 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:16] Energy consumed for all CPUs : 0.004812 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:16] 0.006951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:31] Energy consumed for RAM : 0.002333 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:31] Energy consumed for all CPUs : 0.005250 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:31] 0.007582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:36:46] Energy consumed for RAM : 0.002527 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:36:46] Energy consumed for all CPUs : 0.005687 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:36:46] 0.008214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:01] Energy consumed for RAM : 0.002721 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:01] Energy consumed for all CPUs : 0.006125 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:01] 0.008846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:16] Energy consumed for RAM : 0.002916 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:16] Energy consumed for all CPUs : 0.006562 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:16] 0.009478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:31] Energy consumed for RAM : 0.003110 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:31] Energy consumed for all CPUs : 0.007000 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:31] 0.010110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:37:46] Energy consumed for RAM : 0.003305 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:37:46] Energy consumed for all CPUs : 0.007437 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:37:46] 0.010742 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:01] Energy consumed for RAM : 0.003499 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:01] Energy consumed for all CPUs : 0.007875 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:01] 0.011374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:16] Energy consumed for RAM : 0.003693 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:16] Energy consumed for all CPUs : 0.008312 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:16] 0.012005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:31] Energy consumed for RAM : 0.003888 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:31] Energy consumed for all CPUs : 0.008750 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:31] 0.012637 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:38:46] Energy consumed for RAM : 0.004082 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:38:46] Energy consumed for all CPUs : 0.009187 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:38:46] 0.013269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:01] Energy consumed for RAM : 0.004277 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:01] Energy consumed for all CPUs : 0.009624 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:01] 0.013901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:16] Energy consumed for RAM : 0.004471 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:16] Energy consumed for all CPUs : 0.010062 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:16] 0.014533 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:31] Energy consumed for RAM : 0.004665 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:31] Energy consumed for all CPUs : 0.010499 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:31] 0.015165 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:39:46] Energy consumed for RAM : 0.004860 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:39:46] Energy consumed for all CPUs : 0.010937 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:39:46] 0.015797 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:01] Energy consumed for RAM : 0.005054 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:01] Energy consumed for all CPUs : 0.011374 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:01] 0.016428 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:16] Energy consumed for RAM : 0.005249 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:16] Energy consumed for all CPUs : 0.011812 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:16] 0.017060 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:31] Energy consumed for RAM : 0.005443 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:31] Energy consumed for all CPUs : 0.012249 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:31] 0.017692 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:46] Energy consumed for RAM : 0.005637 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:40:46] Energy consumed for all CPUs : 0.012687 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:40:46] 0.018324 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:01] Energy consumed for RAM : 0.005832 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:01] Energy consumed for all CPUs : 0.013124 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:01] 0.018956 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:16] Energy consumed for RAM : 0.006026 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:16] Energy consumed for all CPUs : 0.013562 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:16] 0.019588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:24] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 11:41:24] Energy consumed for RAM : 0.006126 kWh. RAM Power : 46.66258478164673 W\n",
      "[codecarbon INFO @ 11:41:24] Energy consumed for all CPUs : 0.013787 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 11:41:24] 0.019913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:41:24] Done!\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSynthetic sentences generated and saved to \u001b[39m\u001b[39m{\u001b[39;00moutput_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     generate_and_save_synthetic_sentences(df, model, tokenizer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/codecarbon/emissions_tracker.py:1040\u001b[0m, in \u001b[0;36mtrack_emissions.<locals>._decorate.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m tracker\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     fn_result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1041\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1043\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGraceful stopping: collecting and writing information.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1044\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPlease wait a few seconds...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m     )\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     specialty \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mmedical_specialty\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     synthetic_sentence \u001b[39m=\u001b[39m generate_synthetic_sentence(model, tokenizer, specialty, max_length\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     synthetic_sentences\u001b[39m.\u001b[39mappend(synthetic_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Append the synthetic sentences to the dataframe\u001b[39;00m\n",
      "\u001b[1;32m/home/ray/default/Kyra_Test (1) (1) (2).ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m specialty_label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[BOS]\u001b[39m\u001b[39m{\u001b[39;00mspecialty_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,  \u001b[39m# Avoid repeating n-grams\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Enable sampling\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,  \u001b[39m# Top-k sampling\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,  \u001b[39m# Top-p sampling (nucleus sampling)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-rurxq15mg89lek3kwp43lamgxh.i.anyscaleuserdata.com/home/ray/default/Kyra_Test%20%281%29%20%281%29%20%282%29.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m generated_sentence\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1759\u001b[0m         input_ids,\n\u001b[1;32m   1760\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1761\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   1762\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1763\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1764\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1765\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config) \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2398\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2399\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2400\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2401\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2402\u001b[0m )\n\u001b[1;32m   2404\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1302\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1302\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1303\u001b[0m     input_ids,\n\u001b[1;32m   1304\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1305\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1306\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1307\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1308\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1309\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1310\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1311\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1312\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1313\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1314\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1315\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1317\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1319\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1116\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1105\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1106\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         output_attentions,\n\u001b[1;32m   1114\u001b[0m     )\n\u001b[1;32m   1115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m   1117\u001b[0m         hidden_states,\n\u001b[1;32m   1118\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m   1119\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1120\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m   1121\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1122\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1123\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1124\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1125\u001b[0m     )\n\u001b[1;32m   1127\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 614\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    615\u001b[0m     hidden_states,\n\u001b[1;32m    616\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    617\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    618\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    619\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    620\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:325\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    323\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    327\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    328\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/pytorch_utils.py:104\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    103\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 104\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "@track_emissions(project_name=\"synthetic_sentence_generation\")\n",
    "def generate_and_save_synthetic_sentences(df, model, tokenizer, output_file='synthetic_sentence_nodp_temp0.8_topk50.csv'):\n",
    "    # Generate synthetic sentences for each row in the dataframe\n",
    "    synthetic_sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        specialty = row['medical_specialty']\n",
    "        synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "        synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "    # Append the synthetic sentences to the dataframe\n",
    "    df['synthetic_sentence_nodp_temp0.8_topk50'] = synthetic_sentences\n",
    "\n",
    "    # Save the dataframe with the synthetic sentences\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Synthetic sentences generated and saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_and_save_synthetic_sentences(df, model, tokenizer)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Out to have FastDP matched with LLama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc0cf8b4834474287a9200ffbd414d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
