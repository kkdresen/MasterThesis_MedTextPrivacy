{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "  Cloning https://github.com/awslabs/fast-differential-privacy.git to /tmp/pip-req-build-yf7kdyd2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy.git /tmp/pip-req-build-yf7kdyd2\n",
      "  Resolved https://github.com/awslabs/fast-differential-privacy.git to commit 1647d274d120c1ccc73e369f2ffa811cfb994c76\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fastDP\n",
      "  Building wheel for fastDP (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastDP: filename=fastDP-2.0.0-py3-none-any.whl size=133223 sha256=d2765a85973d24a46276c769884f9ede291a77463518f1f2df9f4a51b1b5d6c1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cizt1nha/wheels/6a/32/f6/19f858522f7d03c4c8d5cabcf543389f86a227589eea066737\n",
      "Successfully built fastDP\n",
      "Installing collected packages: fastDP\n",
      "Successfully installed fastDP-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 35, in __init__\n",
      "    parsed = _parse_requirement(requirement_string)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 64, in parse_requirement\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 82, in _parse_requirement\n",
      "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 126, in _parse_requirement_details\n",
      "    marker = _parse_requirement_marker(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 147, in _parse_requirement_marker\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/bin/pip\", line 15, in <module>\n",
      "    snapshot_util.add_python_dependency(sys.argv[2:])\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2636, in add_python_dependency\n",
      "    new_packages = _parse_install_command(install_args)\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2584, in _parse_install_command\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2584, in <listcomp>\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 37, in __init__\n",
      "    raise InvalidRequirement(str(e)) from e\n",
      "packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/awslabs/fast-differential-privacy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl (779.1 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "Collecting codecarbon\n",
      "  Using cached codecarbon-2.5.0-py3-none-any.whl (496 kB)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (5.3.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2023.5.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: arrow in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (1.3.0)\n",
      "Requirement already satisfied: click in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (8.1.7)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (1.5.3)\n",
      "Requirement already satisfied: prometheus-client in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (0.19.0)\n",
      "Requirement already satisfied: psutil in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ray/anaconda3/lib/python3.9/site-packages (from codecarbon) (9.0.0)\n",
      "Collecting pynvml (from codecarbon)\n",
      "  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "Collecting rapidfuzz (from codecarbon)\n",
      "  Using cached rapidfuzz-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from arrow->codecarbon) (2.8.2)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ray/anaconda3/lib/python3.9/site-packages (from arrow->codecarbon) (2.9.0.20240316)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->codecarbon) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, sympy, safetensors, regex, rapidfuzz, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, codecarbon, transformers, torch\n",
      "Successfully installed codecarbon-2.5.0 huggingface-hub-0.23.4 joblib-1.4.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pynvml-11.5.0 rapidfuzz-3.9.3 regex-2024.5.15 safetensors-0.4.3 scikit-learn-1.5.0 sympy-1.12.1 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.1 transformers-4.42.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_d81e97c8fg46h1crfqpx4w37ei-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastDP import PrivacyEngine\n",
    "import transformers, torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>age_related_sentence</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>synthetic_sentence_eps16_temp0.6_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp0.8_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1.0_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1.2_topk50</th>\n",
       "      <th>synthetic_sentence_eps16_temp1.2_topk100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "      <td>HX , (primarily obtained from boyfriend): This...</td>\n",
       "      <td>(primarily obtained from boyfriend): This 31 y...</td>\n",
       "      <td>43</td>\n",
       "      <td>[BOS]Neurology[SEP]This is a male with a small...</td>\n",
       "      <td>[BOS]Neurology[SEP]The patient is a small-mold...</td>\n",
       "      <td>[BOS]Neurology[SEP]This is a healthy female wi...</td>\n",
       "      <td>[BOS]Neurology[SEP]A 38-year-old black girl wh...</td>\n",
       "      <td>[BOS]Neurology[SEP]Neurochrochurology etiology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiovascular/Pulmonary</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES,Airway obstruction seco...</td>\n",
       "      <td>INDICATIONS FOR SURGERY,The patient is a 50-ye...</td>\n",
       "      <td>The patient is a 50-year-old white male with h...</td>\n",
       "      <td>72</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]HISTORY OF T...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]After operat...</td>\n",
       "      <td>[BOS]Cardiovascular/Pulmonary[SEP]The patient ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "      <td>Elective male sterilization via bilateral vase...</td>\n",
       "      <td>43</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a patient with...</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENTNESS: This ...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a 3-year-old m...</td>\n",
       "      <td>[BOS]Urology[SEP]Obstetrics:,DURESEPERSON OR P...</td>\n",
       "      <td>[BOS]Urology[SEP]Physicians have been identifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "      <td>DESCRIPTION:, The patient was placed in the su...</td>\n",
       "      <td>The patient was placed in the supine position ...</td>\n",
       "      <td>44</td>\n",
       "      <td>[BOS]Urology[SEP]This is a 35-year-old female ...</td>\n",
       "      <td>[BOS]Urology[SEP]This is a 62-year-old African...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient is a very well-in...</td>\n",
       "      <td>[BOS]Urology[SEP]Obsole presentation of patien...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient was admitted to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "      <td>INDICATIONS FOR PROCEDURE:  ,A gentleman who i...</td>\n",
       "      <td>A gentleman who is here today requesting volun...</td>\n",
       "      <td>63</td>\n",
       "      <td>[BOS]Urology[SEP]HISTORY OF PRESENTNESS,, The ...</td>\n",
       "      <td>[BOS]Urology[SEP]This patient is a 30-year-old...</td>\n",
       "      <td>[BOS]Urology[SEP]Otetrics and evaluation of pa...</td>\n",
       "      <td>[BOS]Urology[SEP]The patient and Itho well des...</td>\n",
       "      <td>[BOS]Urology[SEP]Gastroenterology[SEP LVR]Ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          medical_specialty  \\\n",
       "0                 Neurology   \n",
       "1  Cardiovascular/Pulmonary   \n",
       "2                   Urology   \n",
       "3                   Urology   \n",
       "4                   Urology   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  CC:, Confusion and slurred speech.,HX , (prima...   \n",
       "1  PREOPERATIVE DIAGNOSES,Airway obstruction seco...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:,  The patient was placed in the s...   \n",
       "4  PREOPERATIVE DIAGNOSIS: , Voluntary sterility....   \n",
       "\n",
       "                                age_related_sentence  \\\n",
       "0  HX , (primarily obtained from boyfriend): This...   \n",
       "1  INDICATIONS FOR SURGERY,The patient is a 50-ye...   \n",
       "2  PROCEDURE: , Elective male sterilization via b...   \n",
       "3  DESCRIPTION:, The patient was placed in the su...   \n",
       "4  INDICATIONS FOR PROCEDURE:  ,A gentleman who i...   \n",
       "\n",
       "                                      extracted_text  word_count  \\\n",
       "0  (primarily obtained from boyfriend): This 31 y...          43   \n",
       "1  The patient is a 50-year-old white male with h...          72   \n",
       "2  Elective male sterilization via bilateral vase...          43   \n",
       "3  The patient was placed in the supine position ...          44   \n",
       "4  A gentleman who is here today requesting volun...          63   \n",
       "\n",
       "             synthetic_sentence_eps16_temp0.6_topk50  \\\n",
       "0  [BOS]Neurology[SEP]This is a male with a small...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]The patient is a patient with...   \n",
       "3  [BOS]Urology[SEP]This is a 35-year-old female ...   \n",
       "4  [BOS]Urology[SEP]HISTORY OF PRESENTNESS,, The ...   \n",
       "\n",
       "             synthetic_sentence_eps16_temp0.8_topk50  \\\n",
       "0  [BOS]Neurology[SEP]The patient is a small-mold...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]HISTORY OF T...   \n",
       "2  [BOS]Urology[SEP]HISTORY OF PRESENTNESS: This ...   \n",
       "3  [BOS]Urology[SEP]This is a 62-year-old African...   \n",
       "4  [BOS]Urology[SEP]This patient is a 30-year-old...   \n",
       "\n",
       "             synthetic_sentence_eps16_temp1.0_topk50  \\\n",
       "0  [BOS]Neurology[SEP]This is a healthy female wi...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...   \n",
       "2  [BOS]Urology[SEP]The patient is a 3-year-old m...   \n",
       "3  [BOS]Urology[SEP]The patient is a very well-in...   \n",
       "4  [BOS]Urology[SEP]Otetrics and evaluation of pa...   \n",
       "\n",
       "             synthetic_sentence_eps16_temp1.2_topk50  \\\n",
       "0  [BOS]Neurology[SEP]A 38-year-old black girl wh...   \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]After operat...   \n",
       "2  [BOS]Urology[SEP]Obstetrics:,DURESEPERSON OR P...   \n",
       "3  [BOS]Urology[SEP]Obsole presentation of patien...   \n",
       "4  [BOS]Urology[SEP]The patient and Itho well des...   \n",
       "\n",
       "            synthetic_sentence_eps16_temp1.2_topk100  \n",
       "0  [BOS]Neurology[SEP]Neurochrochurology etiology...  \n",
       "1  [BOS]Cardiovascular/Pulmonary[SEP]The patient ...  \n",
       "2  [BOS]Urology[SEP]Physicians have been identifi...  \n",
       "3  [BOS]Urology[SEP]The patient was admitted to t...  \n",
       "4  [BOS]Urology[SEP]Gastroenterology[SEP LVR]Ther...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/home/ray/default/eps_16.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from eps16_finetunedmodel_2016(rachidslive)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define the directory where the model and tokenizer are saved\n",
    "save_directory = \"eps16_finetunedmodel_2016(rachidslive)\" #set path to model\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# If necessary, move the model to the specified device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded from\", save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_16.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling, switch between 50 and 100\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.6 #temperature, switch between 0.6, 0.8, 1.0, 1.2\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp0.6_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS]Neurology[SEP]This is a male with a small history of abdominal pain, which is a small history and has a history of abdominal bleeding. The patient was admitted to the hospital in the last few days on the last week.,HISTORY OF THE PROCEDURE:,The patient was admitted with abdominal pain, but had difficulty with abdominal pain.,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['synthetic_sentence_eps16_temp0.6_topk50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp0.8_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1.0_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.2\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1.2_topk50'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.2\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1.2_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count',\n",
       "       'synthetic_sentence_eps16_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp0.8_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.0_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.2_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.2_topk100'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=1.0\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp1.0_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp0.8_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic sentences generated and saved to eps_8.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,  # Avoid repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=100,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p sampling (nucleus sampling)\n",
    "        temperature=0.6\n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['medical_specialty']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_eps16_temp0.6_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('eps_16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to eps_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_specialty', 'transcription', 'age_related_sentence',\n",
       "       'extracted_text', 'word_count',\n",
       "       'synthetic_sentence_eps16_temp0.6_topk50',\n",
       "       'synthetic_sentence_eps16_temp0.8_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.0_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.2_topk50',\n",
       "       'synthetic_sentence_eps16_temp1.2_topk100',\n",
       "       'synthetic_sentence_eps16_temp1.0_topk100',\n",
       "       'synthetic_sentence_eps16_temp0.8_topk100',\n",
       "       'synthetic_sentence_eps16_temp0.6_topk100'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
