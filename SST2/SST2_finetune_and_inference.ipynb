{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "  Cloning https://github.com/awslabs/fast-differential-privacy.git to /tmp/pip-req-build-zcl4ykao\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy.git /tmp/pip-req-build-zcl4ykao\n",
      "  Resolved https://github.com/awslabs/fast-differential-privacy.git to commit af783b348e82516f7565802cf1144a8be95c69a5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fastDP\n",
      "  Building wheel for fastDP (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastDP: filename=fastDP-2.0.0-py3-none-any.whl size=133222 sha256=5c999f8c0a8da5ffe7e87f7e6dc80335b9c3e8ed3733c3ed68655224a2bfe65a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zlq3mxs_/wheels/6a/32/f6/19f858522f7d03c4c8d5cabcf543389f86a227589eea066737\n",
      "Successfully built fastDP\n",
      "Installing collected packages: fastDP\n",
      "Successfully installed fastDP-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 35, in __init__\n",
      "    parsed = _parse_requirement(requirement_string)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 64, in parse_requirement\n",
      "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 82, in _parse_requirement\n",
      "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 126, in _parse_requirement_details\n",
      "    marker = _parse_requirement_marker(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_parser.py\", line 147, in _parse_requirement_marker\n",
      "    tokenizer.raise_syntax_error(\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "    raise ParserSyntaxError(\n",
      "packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/bin/pip\", line 15, in <module>\n",
      "    snapshot_util.add_python_dependency(sys.argv[2:])\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2579, in add_python_dependency\n",
      "    new_packages = _parse_install_command(install_args)\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2527, in _parse_install_command\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/tmp/anyscale/ray_container/snapshot_util.py\", line 2527, in <listcomp>\n",
      "    [Requirement(pkg) for pkg in namespace.package if pkg not in local_packages]\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/packaging/requirements.py\", line 37, in __init__\n",
      "    raise InvalidRequirement(str(e)) from e\n",
      "packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "    git+https://github.com/awslabs/fast-differential-privacy.git\n",
      "       ^\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/awslabs/fast-differential-privacy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ray/anaconda3/lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in /home/ray/anaconda3/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ray/anaconda3/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2023.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ray/anaconda3/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found credentials from IAM Role: cld_2xaislbg524y9urhb7nvm96bpl-cluster-node-role\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastDP import PrivacyEngine\n",
    "import transformers, torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2014, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55232</td>\n",
       "      <td>carried less by wow factors than by its funny ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1862</td>\n",
       "      <td>sorry use of aaliyah in her one and only starr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46429</td>\n",
       "      <td>the obnoxious special effects , the obligatory...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52913</td>\n",
       "      <td>like the best of godard 's movies ... it is vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58934</td>\n",
       "      <td>if you are willing to do this , then you so cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                           sentence  label\n",
       "0  55232  carried less by wow factors than by its funny ...      1\n",
       "1   1862  sorry use of aaliyah in her one and only starr...      0\n",
       "2  46429  the obnoxious special effects , the obligatory...      0\n",
       "3  52913  like the best of godard 's movies ... it is vi...      1\n",
       "4  58934  if you are willing to do this , then you so cr...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/home/ray/default/sst2_balanced_sentences.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0f415fd43742d598ed9f4fd7ef04fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7759f89a2c4165a49a5c6b39a932ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c43630f5194572a3c8cf5e63fc5b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6512d64217844e37957db6ccc39e78bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafd64ad6d5f498da7459f588f355dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb5c2eeaeab41d581f2be46190bfe22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9487d0eb5d4aff9c770c6ee5afbac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable components:  77 ; Number of trainable layers:  40\n",
      ">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",
      ">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['transformer.wte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Synthetic sentences generated and saved to synthetic_data_bossep.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from fastDP import PrivacyEngine  # Ensure fastDP is installed\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer, and move the model to the specified device\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Dataset class for multi-class classification\n",
    "class MedicalSpecialtyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specialty_label = f\"{self.labels[idx]}[SEP]\"\n",
    "        text = f\"[BOS]{specialty_label}{self.texts[idx]}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # The target is the same as input_ids but shifted by one position\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100  # Ignore padding token\n",
    "\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "\n",
    "# Extract texts and labels from the dataframe\n",
    "texts = df['sentence'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.05)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = MedicalSpecialtyDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = MedicalSpecialtyDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=64,\n",
    "    sample_size=len(train_dataset),\n",
    "    epochs=5,\n",
    "    target_epsilon=16, #change between 3, 8, 16\n",
    "    clipping_fn='automatic',\n",
    "    clipping_mode='MixOpt',\n",
    "    origin_params=None,\n",
    "    clipping_style='all-layer',\n",
    ")\n",
    "\n",
    "# Attach the PrivacyEngine to the optimizer\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, target_ids = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "# Detach and save the privacy engine state\n",
    "privacy_engine.detach()\n",
    "\n",
    "# Function to generate synthetic sentences with medical specialty label\n",
    "def generate_synthetic_sentence(model, tokenizer, label, prompt, max_length=80):\n",
    "    model.eval()\n",
    "    specialty_label = f\"{label}[SEP]\"\n",
    "    input_ids = tokenizer.encode(f\"[BOS]{specialty_label}{prompt}\", return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=True,\n",
    "        top_k=100,   #change between 50,100\n",
    "        top_p=0.95,\n",
    "        temperature=1.0 #change between 0.6, 0.8, 1.0, 1.2 \n",
    "    )\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_sentence\n",
    "\n",
    "# Generate synthetic sentences for each row in the dataframe\n",
    "synthetic_sentences = []\n",
    "for index, row in df.iterrows():\n",
    "    specialty = row['label']\n",
    "    synthetic_sentence = generate_synthetic_sentence(model, tokenizer, specialty, max_length=80)\n",
    "    synthetic_sentences.append(synthetic_sentence)\n",
    "\n",
    "# Append the synthetic sentences to the dataframe\n",
    "df['synthetic_sentence_dp_eps16_temp1_topk100'] = synthetic_sentences\n",
    "\n",
    "# Save the dataframe with the synthetic sentences\n",
    "df.to_csv('synthetic_data_eps16.csv', index=False)\n",
    "\n",
    "print(\"Synthetic sentences generated and saved to synthetic_data_bossep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model_with_classifier_distilgpt2_sst2_eps16\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"./trained_model_with_classifier_distilgpt2_sst2_eps16\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "import os\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
